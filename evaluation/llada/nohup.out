âž– Using existing negated importance: /home/qiheng/Projects/adaptive-dllm/configs/head_importance_llada_base_loss_gateIG_neg/head_importance.pt
========================================================
Quick Test Configuration
========================================================
Tasks: GSM8K, HumanEval
Model Types: standard, sparse, adaptive
Generation Length: 256 tokens
Block Size: 32
Test Samples: 50 per dataset
Importance tag: loss_gateIG_neg
Importance file: /home/qiheng/Projects/adaptive-dllm/configs/head_importance_llada_base_loss_gateIG_neg/head_importance.pt
========================================================

ðŸš€ Starting quick test evaluation...
Started at: Sunday, December 28, 2025 PM02:21:56 HKT


================================================
ðŸ“Š Task: GSM8K
================================================

Progress: [1/2]

========================================
Running: adaptive on gsm8k
========================================
Params: gen_length=256, steps=256, block_length=32, block_size=32, limit=100
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-28:14:22:05 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-28:14:22:05 INFO     [__main__:446] Selected Tasks: ['gsm8k']
2025-12-28:14:22:05 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-28:14:22:05 INFO     [evaluator:240] Initializing llada_eval model, with arguments: {'model_path': 'GSAI-ML/LLaDA-8B-Base', 'model_type': 'adaptive', 'gen_length': 256, 'steps':
        256, 'block_length': 32, 'skip': 0.2, 'select': 0.3, 'block_size': 32, 'importance_source': 'precomputed',
        'precomputed_importance_path':
        '/home/qiheng/Projects/adaptive-dllm/configs/head_importance_llada_base_loss_gateIG_neg/head_importance.pt'}
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
âœ“ Loading pre-computed importance scores from: /home/qiheng/Projects/adaptive-dllm/configs/head_importance_llada_base_loss_gateIG_neg/head_importance.pt
âœ“ Created adaptive config using PRE-COMPUTED importance scores

================================================================================
ðŸ“Š LLaDA Adaptive Sparsity Configuration Statistics
================================================================================
Mode: Relative Weights (mean=1.0, multiply by select at inference)
Target select: 0.300 (30.0%)
Layers: 32, KV Heads per layer: 32
Total heads: 1024

--------------------------------------------------------------------------------
Per-Layer Statistics:
--------------------------------------------------------------------------------
tensor([2.1412, 0.8629, 2.1412, 2.1412, 2.1412, 2.1412, 1.8891, 0.4282, 2.1412,
        2.1412, 2.1412, 2.1412, 2.1412, 0.4282, 1.7261, 2.1412, 2.1412, 2.1412,
        2.1412, 2.1412, 2.1412, 2.1412, 1.6004, 1.9233, 2.1412, 1.9459, 2.1412,
        1.6463, 2.1412, 2.1412, 2.1412, 2.1412])
Layer  0: weight_mean=1.9281 (range=[0.428, 2.141]) â†’ keep_ratio_mean=0.5784 (57.8%) (range=[0.128, 0.642])
tensor([0.6301, 0.6611, 0.7218, 0.8702, 0.6885, 0.6294, 0.7899, 0.6925, 0.6519,
        0.4282, 0.6556, 0.4420, 0.6210, 0.6253, 0.6545, 0.5732, 0.6292, 0.5812,
        0.4282, 0.6419, 0.7638, 1.2216, 0.6525, 0.6930, 0.7061, 1.4387, 0.7347,
        1.6489, 0.8620, 0.5214, 0.9295, 0.6327])
Layer  1: weight_mean=0.7319 (range=[0.428, 1.649]) â†’ keep_ratio_mean=0.2196 (22.0%) (range=[0.128, 0.495])
tensor([0.7388, 0.4282, 0.5825, 0.6851, 0.6636, 0.5996, 0.7338, 0.5592, 0.6389,
        0.6941, 0.7842, 0.6311, 0.6439, 0.5947, 0.6128, 0.6115, 0.7487, 0.6530,
        0.7183, 1.4537, 0.6099, 0.7020, 0.6364, 0.6304, 0.6276, 0.6255, 0.6574,
        0.6560, 0.6886, 0.5034, 0.6158, 1.5371])
Layer  2: weight_mean=0.6958 (range=[0.428, 1.537]) â†’ keep_ratio_mean=0.2087 (20.9%) (range=[0.128, 0.461])
tensor([0.6781, 0.5251, 0.6429, 0.7491, 0.6455, 0.4552, 0.9904, 0.6595, 0.6382,
        0.5879, 0.4282, 0.6543, 0.4486, 0.6167, 0.6239, 0.6152, 0.7763, 0.7419,
        0.6321, 0.5587, 0.6691, 0.6500, 0.6985, 0.5811, 0.6896, 0.6652, 0.6381,
        0.5461, 0.5885, 0.8543, 0.6465, 0.6494])
Layer  3: weight_mean=0.6420 (range=[0.428, 0.990]) â†’ keep_ratio_mean=0.1926 (19.3%) (range=[0.128, 0.297])
tensor([0.8129, 0.5251, 0.5920, 0.6388, 0.6419, 0.8803, 0.9532, 0.9065, 0.8709,
        0.7369, 0.6774, 0.5846, 0.5832, 0.7233, 0.6694, 0.6579, 0.6769, 0.6464,
        0.7494, 0.5283, 0.7726, 0.6136, 0.8425, 0.6678, 0.5695, 0.6650, 0.6872,
        0.7252, 0.9005, 0.6497, 0.6389, 0.6986])
Layer  4: weight_mean=0.7027 (range=[0.525, 0.953]) â†’ keep_ratio_mean=0.2108 (21.1%) (range=[0.158, 0.286])
tensor([0.5799, 0.6314, 0.4775, 0.5325, 0.5348, 1.1435, 0.6262, 0.6353, 0.9558,
        0.4974, 0.6273, 0.7008, 0.6021, 0.6997, 0.6642, 0.7143, 1.3063, 0.6228,
        0.7612, 0.6080, 0.5457, 0.7134, 0.9463, 0.6113, 0.7582, 0.5606, 0.7550,
        0.6720, 0.7542, 0.6252, 0.5969, 0.9860])
Layer  5: weight_mean=0.7014 (range=[0.478, 1.306]) â†’ keep_ratio_mean=0.2104 (21.0%) (range=[0.143, 0.392])
tensor([0.7995, 0.6904, 0.8299, 1.0892, 0.9459, 0.7305, 0.7391, 0.5486, 0.6784,
        0.8375, 0.6051, 0.7051, 0.6214, 0.6723, 0.6713, 0.7014, 0.6233, 1.3090,
        0.7401, 0.6893, 0.7251, 0.8938, 0.6541, 1.0454, 0.6771, 0.7166, 0.6162,
        0.7690, 0.7373, 0.6827, 0.7115, 0.5655])
Layer  6: weight_mean=0.7507 (range=[0.549, 1.309]) â†’ keep_ratio_mean=0.2252 (22.5%) (range=[0.165, 0.393])
tensor([0.8128, 0.6926, 0.9067, 0.5776, 0.6462, 0.7927, 0.6887, 0.8137, 0.4282,
        0.8258, 0.4549, 0.7246, 0.5100, 0.7208, 0.6342, 0.7469, 0.5666, 0.7537,
        1.0502, 0.6530, 0.5540, 0.7374, 0.7545, 1.2752, 0.5785, 0.8901, 0.6755,
        0.6119, 0.7915, 0.7270, 0.8004, 0.7677])
Layer  7: weight_mean=0.7239 (range=[0.428, 1.275]) â†’ keep_ratio_mean=0.2172 (21.7%) (range=[0.128, 0.383])
tensor([0.6450, 0.6199, 0.5831, 0.6215, 0.6520, 0.8245, 0.7831, 1.0120, 0.8119,
        1.0434, 0.7380, 0.7774, 0.6762, 0.6144, 0.6698, 0.7286, 0.8525, 0.8352,
        0.6369, 0.6780, 0.6691, 0.6756, 0.5914, 0.7401, 0.7748, 0.7099, 0.4282,
        0.6053, 0.6544, 1.0208, 0.8878, 0.7931])
Layer  8: weight_mean=0.7298 (range=[0.428, 1.043]) â†’ keep_ratio_mean=0.2189 (21.9%) (range=[0.128, 0.313])
tensor([0.5975, 0.7200, 0.7645, 0.7419, 0.5793, 0.5756, 0.7317, 0.8529, 0.6673,
        0.6781, 0.5331, 0.7734, 0.6267, 0.5997, 0.5974, 0.7150, 0.6283, 0.7162,
        0.6261, 0.9330, 0.8709, 0.7212, 0.5703, 0.6752, 0.5696, 0.8590, 0.7151,
        0.9655, 0.5541, 0.8045, 0.6746, 0.7336])
Layer  9: weight_mean=0.6991 (range=[0.533, 0.966]) â†’ keep_ratio_mean=0.2097 (21.0%) (range=[0.160, 0.290])
tensor([0.4282, 0.6086, 0.6234, 0.7469, 1.0216, 0.8136, 1.6608, 0.4938, 0.7139,
        1.0919, 0.4282, 1.1549, 0.5718, 0.8776, 0.7370, 0.6689, 0.7162, 0.6909,
        0.9193, 0.8045, 0.8063, 0.6248, 0.6651, 0.6422, 0.7993, 0.7698, 1.5159,
        0.7109, 1.7045, 0.6439, 0.8299, 0.5175])
Layer 10: weight_mean=0.8126 (range=[0.428, 1.705]) â†’ keep_ratio_mean=0.2438 (24.4%) (range=[0.128, 0.511])
tensor([0.6165, 0.7628, 0.7851, 0.6534, 0.9526, 0.5101, 0.8373, 0.6561, 0.7284,
        0.8800, 0.5694, 0.4282, 0.8286, 0.4903, 0.8245, 0.8413, 0.6870, 0.7893,
        0.9276, 0.7742, 0.7003, 0.8624, 0.7172, 0.9832, 0.7594, 0.7029, 0.6638,
        0.6001, 0.6807, 0.5933, 0.8089, 0.7268])
Layer 11: weight_mean=0.7294 (range=[0.428, 0.983]) â†’ keep_ratio_mean=0.2188 (21.9%) (range=[0.128, 0.295])
tensor([0.8581, 0.6850, 0.9184, 1.3307, 0.6209, 0.6865, 0.9157, 0.8368, 0.8095,
        0.6236, 0.8076, 0.7341, 0.6608, 0.7002, 1.3123, 0.8084, 0.7841, 0.6766,
        0.7011, 0.4830, 0.7639, 0.8102, 1.0396, 0.7748, 1.2109, 0.5958, 0.8357,
        0.6850, 0.8743, 0.7576, 0.8118, 0.9781])
Layer 12: weight_mean=0.8154 (range=[0.483, 1.331]) â†’ keep_ratio_mean=0.2446 (24.5%) (range=[0.145, 0.399])
tensor([0.4282, 1.0987, 0.7752, 0.4282, 0.8917, 0.8059, 0.8805, 1.4660, 0.8678,
        0.8864, 0.5020, 1.7950, 0.7183, 1.3753, 1.5656, 0.6154, 1.5616, 0.6364,
        0.8774, 1.3376, 0.4282, 0.8842, 0.6539, 1.3719, 2.1412, 0.8216, 1.3698,
        0.6322, 1.1576, 1.5013, 0.9370, 2.1412])
Layer 13: weight_mean=1.0485 (range=[0.428, 2.141]) â†’ keep_ratio_mean=0.3146 (31.5%) (range=[0.128, 0.642])
tensor([0.7988, 1.2315, 0.8841, 0.8416, 0.7346, 0.4282, 0.6940, 0.7987, 0.8164,
        0.9938, 0.7753, 0.6926, 0.6808, 0.8787, 0.7234, 0.8928, 0.7623, 0.8856,
        0.6250, 0.6974, 1.2372, 0.9370, 0.6474, 0.6509, 0.6145, 1.1269, 0.8206,
        0.5020, 0.6485, 1.2555, 0.6377, 0.8013])
Layer 14: weight_mean=0.8036 (range=[0.428, 1.255]) â†’ keep_ratio_mean=0.2411 (24.1%) (range=[0.128, 0.377])
tensor([0.6524, 0.8669, 0.6821, 1.0204, 1.3263, 0.7889, 1.3098, 0.7128, 1.1286,
        0.6174, 0.5899, 0.9892, 0.7218, 0.8611, 0.9479, 1.1881, 1.0002, 0.4282,
        2.1389, 0.4282, 0.9524, 0.7972, 0.9312, 0.5101, 0.7432, 0.8612, 0.8340,
        1.1707, 1.1928, 0.7452, 1.2529, 0.9188])
Layer 15: weight_mean=0.9159 (range=[0.428, 2.139]) â†’ keep_ratio_mean=0.2748 (27.5%) (range=[0.128, 0.642])
tensor([0.6532, 1.0490, 0.7702, 0.7844, 1.1122, 0.8598, 1.1580, 0.9554, 0.9377,
        0.9275, 0.7913, 0.4282, 1.3684, 0.4282, 1.1078, 1.5307, 1.3440, 0.6387,
        0.8063, 0.5768, 1.0246, 1.2721, 0.9958, 1.1865, 1.5167, 1.8625, 1.0425,
        0.4282, 0.7167, 1.4044, 1.4887, 1.2484])
Layer 16: weight_mean=1.0130 (range=[0.428, 1.863]) â†’ keep_ratio_mean=0.3039 (30.4%) (range=[0.128, 0.559])
tensor([0.6790, 0.8897, 0.9750, 0.8589, 0.9098, 0.9433, 0.4282, 1.5900, 0.4282,
        0.8466, 1.4073, 0.6619, 0.6797, 1.2645, 0.7342, 1.1212, 0.4282, 0.9266,
        1.2228, 2.0979, 0.7067, 0.9194, 2.0610, 1.2025, 0.9986, 0.7124, 1.3489,
        0.8726, 0.9545, 1.1290, 0.8232, 0.8362])
Layer 17: weight_mean=0.9893 (range=[0.428, 2.098]) â†’ keep_ratio_mean=0.2968 (29.7%) (range=[0.128, 0.629])
tensor([1.0559, 2.1412, 0.9288, 1.2217, 1.1092, 0.9353, 1.5632, 0.4282, 0.6163,
        0.8720, 0.7721, 1.7504, 0.8189, 0.4453, 0.4282, 0.8784, 0.7175, 1.2813,
        0.8715, 1.2889, 1.5101, 1.7574, 0.9894, 0.5517, 1.1029, 0.8401, 0.8106,
        0.6583, 0.9681, 0.6907, 1.6945, 0.7812])
Layer 18: weight_mean=1.0150 (range=[0.428, 2.141]) â†’ keep_ratio_mean=0.3045 (30.4%) (range=[0.128, 0.642])
tensor([0.4325, 2.1412, 0.9855, 0.5891, 0.6648, 1.7875, 1.0239, 0.8637, 0.8360,
        0.4401, 1.0015, 0.5522, 1.6273, 1.4191, 0.7512, 0.8763, 1.2332, 0.4282,
        1.3770, 1.0155, 0.9018, 1.5687, 0.9966, 0.4405, 1.3178, 0.9810, 0.8438,
        0.5964, 1.0173, 1.5686, 0.5860, 1.3025])
Layer 19: weight_mean=1.0052 (range=[0.428, 2.141]) â†’ keep_ratio_mean=0.3016 (30.2%) (range=[0.128, 0.642])
tensor([0.8228, 0.4282, 0.6860, 1.9827, 1.7577, 1.3892, 0.8121, 0.7666, 1.5413,
        0.8013, 2.0415, 0.4282, 1.0023, 1.1435, 0.9422, 1.7488, 0.6988, 1.0782,
        0.7023, 1.1069, 0.8700, 0.8683, 1.2104, 1.1097, 0.5136, 0.9070, 0.8912,
        1.0394, 1.1830, 1.5286, 0.7275, 1.4571])
Layer 20: weight_mean=1.0683 (range=[0.428, 2.041]) â†’ keep_ratio_mean=0.3205 (32.0%) (range=[0.128, 0.612])
tensor([1.2176, 0.5660, 0.8154, 0.8156, 1.2200, 0.9118, 1.2770, 1.8466, 2.1412,
        2.1412, 2.1412, 0.7152, 2.1412, 0.4282, 1.4505, 0.4282, 2.1412, 2.1353,
        1.8802, 2.1019, 1.4223, 2.1412, 0.4282, 2.1142, 2.1412, 0.4282, 1.5955,
        0.4282, 1.6831, 0.8054, 1.3530, 2.1412])
Layer 21: weight_mean=1.4124 (range=[0.428, 2.141]) â†’ keep_ratio_mean=0.4237 (42.4%) (range=[0.128, 0.642])
tensor([1.2828, 1.2140, 1.3876, 1.4556, 2.1412, 0.7387, 1.0005, 0.4951, 1.3982,
        0.8965, 1.1988, 1.0579, 0.9966, 0.9423, 0.4282, 1.2582, 0.5118, 1.4589,
        1.6226, 0.4282, 0.5741, 0.7657, 1.0461, 2.1412, 1.0958, 1.3456, 1.0953,
        0.4282, 0.7534, 1.0596, 1.2131, 1.0333])
Layer 22: weight_mean=1.0770 (range=[0.428, 2.141]) â†’ keep_ratio_mean=0.3231 (32.3%) (range=[0.128, 0.642])
tensor([0.6311, 1.9081, 2.1412, 0.6333, 0.4282, 1.2609, 0.9327, 0.9393, 1.1668,
        1.9174, 1.5072, 1.3898, 0.4282, 1.4104, 1.5591, 2.1412, 1.9618, 0.4282,
        1.4487, 1.4899, 1.0379, 0.9054, 1.7937, 1.2465, 1.2351, 0.4282, 1.6665,
        0.4282, 1.2858, 0.6936, 2.0859, 0.4282])
Layer 23: weight_mean=1.2175 (range=[0.428, 2.141]) â†’ keep_ratio_mean=0.3652 (36.5%) (range=[0.128, 0.642])
tensor([1.0498, 1.2389, 0.8868, 1.4674, 1.3077, 1.1082, 1.7436, 0.5177, 1.1158,
        0.4282, 1.1102, 1.3421, 1.2671, 1.1818, 1.3034, 1.4272, 1.1933, 1.2854,
        1.9449, 1.1452, 0.4282, 0.4282, 0.9899, 1.0426, 1.0489, 1.0999, 1.0123,
        1.5457, 1.2482, 1.5155, 1.0811, 0.9490])
Layer 24: weight_mean=1.1392 (range=[0.428, 1.945]) â†’ keep_ratio_mean=0.3418 (34.2%) (range=[0.128, 0.583])
tensor([2.1412, 0.4282, 2.1412, 1.0441, 2.1412, 1.6837, 1.5616, 2.0283, 1.2736,
        1.4021, 1.2068, 2.1412, 2.0509, 1.2779, 1.6218, 2.1412, 2.1014, 1.3468,
        0.9538, 1.6901, 1.9742, 1.6467, 0.4282, 0.4282, 0.5764, 1.8272, 1.8139,
        2.1412, 1.5933, 2.1412, 0.4282, 1.8776])
Layer 25: weight_mean=1.5392 (range=[0.428, 2.141]) â†’ keep_ratio_mean=0.4618 (46.2%) (range=[0.128, 0.642])
tensor([1.2442, 0.9532, 0.8583, 1.3166, 0.8991, 0.7878, 0.9032, 0.9005, 1.5983,
        0.9821, 1.0768, 1.4066, 1.8216, 1.5238, 0.8529, 1.0590, 1.4166, 1.3106,
        0.4282, 1.3283, 0.6707, 1.3711, 1.0709, 1.2548, 0.4282, 1.3756, 1.1970,
        0.7651, 0.8574, 1.4958, 1.1907, 0.9229])
Layer 26: weight_mean=1.1021 (range=[0.428, 1.822]) â†’ keep_ratio_mean=0.3306 (33.1%) (range=[0.128, 0.546])
tensor([0.7660, 0.7570, 1.0196, 1.0532, 1.0455, 1.2373, 0.9021, 1.2813, 1.1011,
        0.7803, 1.2160, 0.9339, 1.4426, 1.1330, 0.9784, 1.0316, 1.0341, 1.4328,
        1.3348, 0.9787, 0.4282, 0.9881, 0.7814, 1.4505, 0.9813, 1.1360, 1.4439,
        1.1859, 0.9587, 1.2460, 0.9915, 1.0904])
Layer 27: weight_mean=1.0669 (range=[0.428, 1.451]) â†’ keep_ratio_mean=0.3201 (32.0%) (range=[0.128, 0.435])
tensor([1.4880, 1.6242, 0.8637, 1.0099, 1.0289, 0.9355, 1.1709, 1.5293, 1.1997,
        1.4846, 1.0393, 1.2544, 1.1057, 0.8743, 0.8742, 1.0190, 0.7213, 0.8520,
        1.5323, 0.7798, 1.1608, 2.0609, 1.1889, 0.8048, 0.6467, 0.8043, 1.1335,
        1.4667, 0.8644, 1.0093, 0.8483, 2.1312])
Layer 28: weight_mean=1.1408 (range=[0.647, 2.131]) â†’ keep_ratio_mean=0.3423 (34.2%) (range=[0.194, 0.639])
tensor([1.0326, 0.8663, 1.5068, 0.7172, 0.7266, 0.9876, 0.5619, 0.9247, 0.7460,
        1.0327, 0.9147, 0.8097, 0.7967, 0.8898, 0.8138, 1.2482, 1.1150, 1.2056,
        1.5754, 0.7212, 1.7681, 0.8687, 0.8000, 2.1154, 1.1349, 1.3728, 0.8046,
        1.0443, 1.1696, 0.7756, 0.8277, 0.9033])
Layer 29: weight_mean=1.0243 (range=[0.562, 2.115]) â†’ keep_ratio_mean=0.3073 (30.7%) (range=[0.169, 0.635])
tensor([1.3075, 0.9550, 0.9606, 1.3248, 0.8049, 2.0039, 1.2704, 2.1412, 1.5137,
        0.6886, 0.9572, 0.9778, 1.5676, 1.1380, 1.1524, 1.5252, 1.1440, 1.1084,
        1.0213, 1.0238, 1.7880, 1.0813, 0.7515, 1.6378, 1.2911, 0.8318, 1.2670,
        0.9981, 0.9988, 1.3563, 1.0656, 1.2509])
Layer 30: weight_mean=1.2158 (range=[0.689, 2.141]) â†’ keep_ratio_mean=0.3647 (36.5%) (range=[0.207, 0.642])
tensor([1.9811, 2.1412, 2.1412, 1.1556, 1.9410, 1.8090, 2.1372, 1.5636, 0.8901,
        1.0857, 1.7943, 2.1149, 1.8762, 2.1412, 0.8362, 2.1412, 1.5179, 1.8636,
        1.3198, 1.2584, 0.7329, 0.8497, 1.9792, 1.2320, 1.0157, 0.7637, 1.1698,
        1.3683, 2.1412, 1.4214, 1.3121, 1.6899])
Layer 31: weight_mean=1.5433 (range=[0.733, 2.141]) â†’ keep_ratio_mean=0.4630 (46.3%) (range=[0.220, 0.642])

--------------------------------------------------------------------------------
Global Statistics:
--------------------------------------------------------------------------------
Relative weights:
  Mean:   1.0000 (should be â‰ˆ1.0)
  Std:    0.4549
  Range:  [0.428, 2.141]

Actual keep_ratios (weights Ã— select=0.3):
  Mean:   0.3000 (30.0%)
  Target: 0.3000 (30.0%)
  Deviation: 0.0000 (0.00%)
  Range:  [0.128, 0.642]
  Heads hitting upper limit (>1.0): 0/1024 (0.0%)
  âœ… Mean matches target (deviation < 1%)

--------------------------------------------------------------------------------
Layer-wise Variation:
--------------------------------------------------------------------------------
Layer means range: [0.1926, 0.5784]
Layer means std:   0.0892
Actual keep_ratio range across layers: [0.193, 0.578]
Variation: 38.6% spread
================================================================================

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 184.94it/s]
2025-12-28:14:23:13 INFO     [evaluator:305] gsm8k: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}
2025-12-28:14:23:13 INFO     [api.task:434] Building contexts for gsm8k on rank 0...
Converted 32 blocks to adaptive blocks

======================================================================
LLaDA Evaluation Setup
======================================================================
Model type: adaptive
Model path: GSAI-ML/LLaDA-8B-Base
Steps: 256, Gen length: 256, Block length: 32
Sparse params: skip=0.2, select=0.3, block_size=32
======================================================================

  0%|          | 0/100 [00:00<?, ?it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:00<00:00, 330.74it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:00<00:00, 341.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 344.91it/s]
2025-12-28:14:23:13 INFO     [evaluator:574] Running generate_until requests
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:00<00:00, 526.81 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 472.69 examples/s]
Generating...:   0%|          | 0/100 [00:00<?, ?it/s]Generating...:   1%|          | 1/100 [00:35<58:02, 35.18s/it]Generating...:   2%|â–         | 2/100 [01:10<57:11, 35.02s/it]Generating...:   3%|â–Ž         | 3/100 [01:42<54:47, 33.89s/it]Generating...:   4%|â–         | 4/100 [02:12<51:23, 32.12s/it]Generating...:   5%|â–Œ         | 5/100 [02:45<51:50, 32.74s/it]Generating...:   6%|â–Œ         | 6/100 [03:10<46:44, 29.83s/it]Generating...:   7%|â–‹         | 7/100 [03:39<45:54, 29.62s/it]Generating...:   8%|â–Š         | 8/100 [04:10<46:09, 30.10s/it]Generating...:   9%|â–‰         | 9/100 [04:44<47:20, 31.21s/it]Generating...:  10%|â–ˆ         | 10/100 [05:12<45:21, 30.24s/it]Generating...:  11%|â–ˆ         | 11/100 [05:41<44:35, 30.07s/it]Generating...:  12%|â–ˆâ–        | 12/100 [06:22<48:44, 33.24s/it]Generating...:  13%|â–ˆâ–Ž        | 13/100 [07:02<51:07, 35.26s/it]Generating...:  14%|â–ˆâ–        | 14/100 [07:37<50:46, 35.43s/it]Generating...:  15%|â–ˆâ–Œ        | 15/100 [08:05<46:56, 33.14s/it]Generating...:  16%|â–ˆâ–Œ        | 16/100 [08:35<44:52, 32.05s/it]Generating...:  17%|â–ˆâ–‹        | 17/100 [09:11<46:14, 33.43s/it]Generating...:  18%|â–ˆâ–Š        | 18/100 [09:39<43:16, 31.66s/it]Generating...:  19%|â–ˆâ–‰        | 19/100 [10:07<41:08, 30.48s/it]Generating...:  20%|â–ˆâ–ˆ        | 20/100 [10:35<39:55, 29.95s/it]Generating...:  21%|â–ˆâ–ˆ        | 21/100 [11:11<41:33, 31.57s/it]Generating...:  22%|â–ˆâ–ˆâ–       | 22/100 [11:47<42:52, 32.98s/it]Generating...:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [12:23<43:22, 33.80s/it]Generating...:  24%|â–ˆâ–ˆâ–       | 24/100 [12:53<41:27, 32.73s/it]Generating...:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [13:27<41:33, 33.25s/it]Generating...:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [14:01<40:59, 33.23s/it]Generating...:  27%|â–ˆâ–ˆâ–‹       | 27/100 [14:31<39:30, 32.47s/it]Generating...:  28%|â–ˆâ–ˆâ–Š       | 28/100 [14:59<37:12, 31.01s/it]Generating...:  29%|â–ˆâ–ˆâ–‰       | 29/100 [15:28<35:52, 30.32s/it]Generating...:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [15:56<34:49, 29.86s/it]Generating...:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [16:26<34:04, 29.63s/it]Generating...:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [17:00<35:14, 31.10s/it]Generating...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [17:34<35:32, 31.82s/it]Generating...:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [18:02<33:44, 30.67s/it]Generating...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [18:34<33:57, 31.34s/it]Generating...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [19:04<32:44, 30.70s/it]Generating...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [19:32<31:26, 29.95s/it]Generating...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [20:08<32:54, 31.85s/it]Generating...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [20:36<31:02, 30.54s/it]Generating...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [21:07<30:40, 30.67s/it]Generating...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [21:40<30:49, 31.35s/it]Generating...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [22:14<31:05, 32.16s/it]Generating...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [22:42<29:24, 30.95s/it]Generating...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [23:19<30:33, 32.73s/it]Generating...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [23:48<29:06, 31.75s/it]Generating...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [24:23<29:22, 32.64s/it]Generating...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [24:54<28:21, 32.09s/it]Generating...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [25:30<28:54, 33.36s/it]Generating...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [25:58<26:59, 31.76s/it]Generating...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [26:28<25:59, 31.18s/it]Generating...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [26:59<25:24, 31.12s/it]Generating...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [27:26<24:01, 30.04s/it]Generating...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [27:56<23:20, 29.80s/it]Generating...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [28:37<25:29, 33.26s/it]Generating...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [29:07<24:20, 32.47s/it]Generating...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [29:40<23:51, 32.53s/it]Generating...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [30:14<23:29, 32.77s/it]Generating...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [30:47<23:09, 33.09s/it]Generating...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [31:25<23:26, 34.32s/it]Generating...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [31:56<22:15, 33.39s/it]Generating...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [32:26<21:09, 32.56s/it]Generating...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [32:55<19:52, 31.38s/it]Generating...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [33:31<20:14, 32.83s/it]Generating...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [34:00<18:56, 31.57s/it]Generating...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [34:33<18:38, 31.96s/it]Generating...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [35:03<17:47, 31.39s/it]Generating...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [35:36<17:32, 31.89s/it]Generating...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [36:04<16:29, 30.92s/it]Generating...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [36:38<16:24, 31.75s/it]Generating...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [37:07<15:26, 30.90s/it]Generating...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [37:34<14:19, 29.63s/it]Generating...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [38:08<14:26, 30.94s/it]Generating...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [38:48<15:07, 33.63s/it]Generating...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [39:20<14:24, 33.26s/it]Generating...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [39:57<14:17, 34.30s/it]Generating...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [40:25<13:00, 32.52s/it]Generating...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [40:59<12:36, 32.90s/it]Generating...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [41:29<11:47, 32.15s/it]Generating...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [41:58<10:52, 31.09s/it]Generating...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [42:34<10:54, 32.71s/it]Generating...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [43:12<10:50, 34.23s/it]Generating...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [43:42<09:54, 33.05s/it]Generating...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [44:10<08:52, 31.33s/it]Generating...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [44:43<08:30, 31.91s/it]Generating...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [45:13<07:49, 31.28s/it]Generating...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [45:48<07:33, 32.43s/it]Generating...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [46:18<06:53, 31.79s/it]Generating...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [46:49<06:16, 31.35s/it]Generating...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [47:22<05:50, 31.89s/it]Generating...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [47:59<05:33, 33.35s/it]Generating...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [48:29<04:51, 32.37s/it]Generating...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [48:58<04:10, 31.37s/it]Generating...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [49:29<03:38, 31.26s/it]Generating...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [50:02<03:11, 31.93s/it]Generating...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [50:36<02:42, 32.53s/it]Generating...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [51:08<02:08, 32.20s/it]Generating...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [51:38<01:34, 31.60s/it]Generating...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [52:13<01:05, 32.81s/it]Generating...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [52:46<00:32, 32.88s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [53:16<00:00, 32.00s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [53:16<00:00, 31.97s/it]
2025-12-28:15:17:57 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-28:15:17:57 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: gsm8k
llada_eval (model_path=GSAI-ML/LLaDA-8B-Base,model_type=adaptive,gen_length=256,steps=256,block_length=32,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,precomputed_importance_path=/home/qiheng/Projects/adaptive-dllm/configs/head_importance_llada_base_loss_gateIG_neg/head_importance.pt), gen_kwargs: (None), limit: 100.0, num_fewshot: None, batch_size: 1
|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|
|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|
|gsm8k|      3|flexible-extract|     5|exact_match|â†‘  | 0.68|Â±  |0.0469|
|     |       |strict-match    |     5|exact_match|â†‘  | 0.69|Â±  |0.0465|

âœ… Completed adaptive on gsm8k
â±ï¸  Running time: 56m 4s (3364s total)


================================================
ðŸ“Š Task: HUMANEVAL
================================================

Progress: [2/2]

========================================
Running: adaptive on humaneval
========================================
Params: gen_length=256, steps=256, block_length=32, block_size=32, limit=100
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-28:15:18:09 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-28:15:18:09 INFO     [__main__:446] Selected Tasks: ['humaneval']
2025-12-28:15:18:09 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-28:15:18:09 INFO     [evaluator:240] Initializing llada_eval model, with arguments: {'model_path': 'GSAI-ML/LLaDA-8B-Base', 'model_type': 'adaptive', 'gen_length': 256, 'steps':
        256, 'block_length': 32, 'skip': 0.2, 'select': 0.3, 'block_size': 32, 'importance_source': 'precomputed',
        'precomputed_importance_path':
        '/home/qiheng/Projects/adaptive-dllm/configs/head_importance_llada_base_loss_gateIG_neg/head_importance.pt'}
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
âœ“ Loading pre-computed importance scores from: /home/qiheng/Projects/adaptive-dllm/configs/head_importance_llada_base_loss_gateIG_neg/head_importance.pt
âœ“ Created adaptive config using PRE-COMPUTED importance scores

================================================================================
ðŸ“Š LLaDA Adaptive Sparsity Configuration Statistics
================================================================================
Mode: Relative Weights (mean=1.0, multiply by select at inference)
Target select: 0.300 (30.0%)
Layers: 32, KV Heads per layer: 32
Total heads: 1024

--------------------------------------------------------------------------------
Per-Layer Statistics:
--------------------------------------------------------------------------------
tensor([2.1412, 0.8629, 2.1412, 2.1412, 2.1412, 2.1412, 1.8891, 0.4282, 2.1412,
        2.1412, 2.1412, 2.1412, 2.1412, 0.4282, 1.7261, 2.1412, 2.1412, 2.1412,
        2.1412, 2.1412, 2.1412, 2.1412, 1.6004, 1.9233, 2.1412, 1.9459, 2.1412,
        1.6463, 2.1412, 2.1412, 2.1412, 2.1412])
Layer  0: weight_mean=1.9281 (range=[0.428, 2.141]) â†’ keep_ratio_mean=0.5784 (57.8%) (range=[0.128, 0.642])
tensor([0.6301, 0.6611, 0.7218, 0.8702, 0.6885, 0.6294, 0.7899, 0.6925, 0.6519,
        0.4282, 0.6556, 0.4420, 0.6210, 0.6253, 0.6545, 0.5732, 0.6292, 0.5812,
        0.4282, 0.6419, 0.7638, 1.2216, 0.6525, 0.6930, 0.7061, 1.4387, 0.7347,
        1.6489, 0.8620, 0.5214, 0.9295, 0.6327])
Layer  1: weight_mean=0.7319 (range=[0.428, 1.649]) â†’ keep_ratio_mean=0.2196 (22.0%) (range=[0.128, 0.495])
tensor([0.7388, 0.4282, 0.5825, 0.6851, 0.6636, 0.5996, 0.7338, 0.5592, 0.6389,
        0.6941, 0.7842, 0.6311, 0.6439, 0.5947, 0.6128, 0.6115, 0.7487, 0.6530,
        0.7183, 1.4537, 0.6099, 0.7020, 0.6364, 0.6304, 0.6276, 0.6255, 0.6574,
        0.6560, 0.6886, 0.5034, 0.6158, 1.5371])
Layer  2: weight_mean=0.6958 (range=[0.428, 1.537]) â†’ keep_ratio_mean=0.2087 (20.9%) (range=[0.128, 0.461])
tensor([0.6781, 0.5251, 0.6429, 0.7491, 0.6455, 0.4552, 0.9904, 0.6595, 0.6382,
        0.5879, 0.4282, 0.6543, 0.4486, 0.6167, 0.6239, 0.6152, 0.7763, 0.7419,
        0.6321, 0.5587, 0.6691, 0.6500, 0.6985, 0.5811, 0.6896, 0.6652, 0.6381,
        0.5461, 0.5885, 0.8543, 0.6465, 0.6494])
Layer  3: weight_mean=0.6420 (range=[0.428, 0.990]) â†’ keep_ratio_mean=0.1926 (19.3%) (range=[0.128, 0.297])
tensor([0.8129, 0.5251, 0.5920, 0.6388, 0.6419, 0.8803, 0.9532, 0.9065, 0.8709,
        0.7369, 0.6774, 0.5846, 0.5832, 0.7233, 0.6694, 0.6579, 0.6769, 0.6464,
        0.7494, 0.5283, 0.7726, 0.6136, 0.8425, 0.6678, 0.5695, 0.6650, 0.6872,
        0.7252, 0.9005, 0.6497, 0.6389, 0.6986])
Layer  4: weight_mean=0.7027 (range=[0.525, 0.953]) â†’ keep_ratio_mean=0.2108 (21.1%) (range=[0.158, 0.286])
tensor([0.5799, 0.6314, 0.4775, 0.5325, 0.5348, 1.1435, 0.6262, 0.6353, 0.9558,
        0.4974, 0.6273, 0.7008, 0.6021, 0.6997, 0.6642, 0.7143, 1.3063, 0.6228,
        0.7612, 0.6080, 0.5457, 0.7134, 0.9463, 0.6113, 0.7582, 0.5606, 0.7550,
        0.6720, 0.7542, 0.6252, 0.5969, 0.9860])
Layer  5: weight_mean=0.7014 (range=[0.478, 1.306]) â†’ keep_ratio_mean=0.2104 (21.0%) (range=[0.143, 0.392])
tensor([0.7995, 0.6904, 0.8299, 1.0892, 0.9459, 0.7305, 0.7391, 0.5486, 0.6784,
        0.8375, 0.6051, 0.7051, 0.6214, 0.6723, 0.6713, 0.7014, 0.6233, 1.3090,
        0.7401, 0.6893, 0.7251, 0.8938, 0.6541, 1.0454, 0.6771, 0.7166, 0.6162,
        0.7690, 0.7373, 0.6827, 0.7115, 0.5655])
Layer  6: weight_mean=0.7507 (range=[0.549, 1.309]) â†’ keep_ratio_mean=0.2252 (22.5%) (range=[0.165, 0.393])
tensor([0.8128, 0.6926, 0.9067, 0.5776, 0.6462, 0.7927, 0.6887, 0.8137, 0.4282,
        0.8258, 0.4549, 0.7246, 0.5100, 0.7208, 0.6342, 0.7469, 0.5666, 0.7537,
        1.0502, 0.6530, 0.5540, 0.7374, 0.7545, 1.2752, 0.5785, 0.8901, 0.6755,
        0.6119, 0.7915, 0.7270, 0.8004, 0.7677])
Layer  7: weight_mean=0.7239 (range=[0.428, 1.275]) â†’ keep_ratio_mean=0.2172 (21.7%) (range=[0.128, 0.383])
tensor([0.6450, 0.6199, 0.5831, 0.6215, 0.6520, 0.8245, 0.7831, 1.0120, 0.8119,
        1.0434, 0.7380, 0.7774, 0.6762, 0.6144, 0.6698, 0.7286, 0.8525, 0.8352,
        0.6369, 0.6780, 0.6691, 0.6756, 0.5914, 0.7401, 0.7748, 0.7099, 0.4282,
        0.6053, 0.6544, 1.0208, 0.8878, 0.7931])
Layer  8: weight_mean=0.7298 (range=[0.428, 1.043]) â†’ keep_ratio_mean=0.2189 (21.9%) (range=[0.128, 0.313])
tensor([0.5975, 0.7200, 0.7645, 0.7419, 0.5793, 0.5756, 0.7317, 0.8529, 0.6673,
        0.6781, 0.5331, 0.7734, 0.6267, 0.5997, 0.5974, 0.7150, 0.6283, 0.7162,
        0.6261, 0.9330, 0.8709, 0.7212, 0.5703, 0.6752, 0.5696, 0.8590, 0.7151,
        0.9655, 0.5541, 0.8045, 0.6746, 0.7336])
Layer  9: weight_mean=0.6991 (range=[0.533, 0.966]) â†’ keep_ratio_mean=0.2097 (21.0%) (range=[0.160, 0.290])
tensor([0.4282, 0.6086, 0.6234, 0.7469, 1.0216, 0.8136, 1.6608, 0.4938, 0.7139,
        1.0919, 0.4282, 1.1549, 0.5718, 0.8776, 0.7370, 0.6689, 0.7162, 0.6909,
        0.9193, 0.8045, 0.8063, 0.6248, 0.6651, 0.6422, 0.7993, 0.7698, 1.5159,
        0.7109, 1.7045, 0.6439, 0.8299, 0.5175])
Layer 10: weight_mean=0.8126 (range=[0.428, 1.705]) â†’ keep_ratio_mean=0.2438 (24.4%) (range=[0.128, 0.511])
tensor([0.6165, 0.7628, 0.7851, 0.6534, 0.9526, 0.5101, 0.8373, 0.6561, 0.7284,
        0.8800, 0.5694, 0.4282, 0.8286, 0.4903, 0.8245, 0.8413, 0.6870, 0.7893,
        0.9276, 0.7742, 0.7003, 0.8624, 0.7172, 0.9832, 0.7594, 0.7029, 0.6638,
        0.6001, 0.6807, 0.5933, 0.8089, 0.7268])
Layer 11: weight_mean=0.7294 (range=[0.428, 0.983]) â†’ keep_ratio_mean=0.2188 (21.9%) (range=[0.128, 0.295])
tensor([0.8581, 0.6850, 0.9184, 1.3307, 0.6209, 0.6865, 0.9157, 0.8368, 0.8095,
        0.6236, 0.8076, 0.7341, 0.6608, 0.7002, 1.3123, 0.8084, 0.7841, 0.6766,
        0.7011, 0.4830, 0.7639, 0.8102, 1.0396, 0.7748, 1.2109, 0.5958, 0.8357,
        0.6850, 0.8743, 0.7576, 0.8118, 0.9781])
Layer 12: weight_mean=0.8154 (range=[0.483, 1.331]) â†’ keep_ratio_mean=0.2446 (24.5%) (range=[0.145, 0.399])
tensor([0.4282, 1.0987, 0.7752, 0.4282, 0.8917, 0.8059, 0.8805, 1.4660, 0.8678,
        0.8864, 0.5020, 1.7950, 0.7183, 1.3753, 1.5656, 0.6154, 1.5616, 0.6364,
        0.8774, 1.3376, 0.4282, 0.8842, 0.6539, 1.3719, 2.1412, 0.8216, 1.3698,
        0.6322, 1.1576, 1.5013, 0.9370, 2.1412])
Layer 13: weight_mean=1.0485 (range=[0.428, 2.141]) â†’ keep_ratio_mean=0.3146 (31.5%) (range=[0.128, 0.642])
tensor([0.7988, 1.2315, 0.8841, 0.8416, 0.7346, 0.4282, 0.6940, 0.7987, 0.8164,
        0.9938, 0.7753, 0.6926, 0.6808, 0.8787, 0.7234, 0.8928, 0.7623, 0.8856,
        0.6250, 0.6974, 1.2372, 0.9370, 0.6474, 0.6509, 0.6145, 1.1269, 0.8206,
        0.5020, 0.6485, 1.2555, 0.6377, 0.8013])
Layer 14: weight_mean=0.8036 (range=[0.428, 1.255]) â†’ keep_ratio_mean=0.2411 (24.1%) (range=[0.128, 0.377])
tensor([0.6524, 0.8669, 0.6821, 1.0204, 1.3263, 0.7889, 1.3098, 0.7128, 1.1286,
        0.6174, 0.5899, 0.9892, 0.7218, 0.8611, 0.9479, 1.1881, 1.0002, 0.4282,
        2.1389, 0.4282, 0.9524, 0.7972, 0.9312, 0.5101, 0.7432, 0.8612, 0.8340,
        1.1707, 1.1928, 0.7452, 1.2529, 0.9188])
Layer 15: weight_mean=0.9159 (range=[0.428, 2.139]) â†’ keep_ratio_mean=0.2748 (27.5%) (range=[0.128, 0.642])
tensor([0.6532, 1.0490, 0.7702, 0.7844, 1.1122, 0.8598, 1.1580, 0.9554, 0.9377,
        0.9275, 0.7913, 0.4282, 1.3684, 0.4282, 1.1078, 1.5307, 1.3440, 0.6387,
        0.8063, 0.5768, 1.0246, 1.2721, 0.9958, 1.1865, 1.5167, 1.8625, 1.0425,
        0.4282, 0.7167, 1.4044, 1.4887, 1.2484])
Layer 16: weight_mean=1.0130 (range=[0.428, 1.863]) â†’ keep_ratio_mean=0.3039 (30.4%) (range=[0.128, 0.559])
tensor([0.6790, 0.8897, 0.9750, 0.8589, 0.9098, 0.9433, 0.4282, 1.5900, 0.4282,
        0.8466, 1.4073, 0.6619, 0.6797, 1.2645, 0.7342, 1.1212, 0.4282, 0.9266,
        1.2228, 2.0979, 0.7067, 0.9194, 2.0610, 1.2025, 0.9986, 0.7124, 1.3489,
        0.8726, 0.9545, 1.1290, 0.8232, 0.8362])
Layer 17: weight_mean=0.9893 (range=[0.428, 2.098]) â†’ keep_ratio_mean=0.2968 (29.7%) (range=[0.128, 0.629])
tensor([1.0559, 2.1412, 0.9288, 1.2217, 1.1092, 0.9353, 1.5632, 0.4282, 0.6163,
        0.8720, 0.7721, 1.7504, 0.8189, 0.4453, 0.4282, 0.8784, 0.7175, 1.2813,
        0.8715, 1.2889, 1.5101, 1.7574, 0.9894, 0.5517, 1.1029, 0.8401, 0.8106,
        0.6583, 0.9681, 0.6907, 1.6945, 0.7812])
Layer 18: weight_mean=1.0150 (range=[0.428, 2.141]) â†’ keep_ratio_mean=0.3045 (30.4%) (range=[0.128, 0.642])
tensor([0.4325, 2.1412, 0.9855, 0.5891, 0.6648, 1.7875, 1.0239, 0.8637, 0.8360,
        0.4401, 1.0015, 0.5522, 1.6273, 1.4191, 0.7512, 0.8763, 1.2332, 0.4282,
        1.3770, 1.0155, 0.9018, 1.5687, 0.9966, 0.4405, 1.3178, 0.9810, 0.8438,
        0.5964, 1.0173, 1.5686, 0.5860, 1.3025])
Layer 19: weight_mean=1.0052 (range=[0.428, 2.141]) â†’ keep_ratio_mean=0.3016 (30.2%) (range=[0.128, 0.642])
tensor([0.8228, 0.4282, 0.6860, 1.9827, 1.7577, 1.3892, 0.8121, 0.7666, 1.5413,
        0.8013, 2.0415, 0.4282, 1.0023, 1.1435, 0.9422, 1.7488, 0.6988, 1.0782,
        0.7023, 1.1069, 0.8700, 0.8683, 1.2104, 1.1097, 0.5136, 0.9070, 0.8912,
        1.0394, 1.1830, 1.5286, 0.7275, 1.4571])
Layer 20: weight_mean=1.0683 (range=[0.428, 2.041]) â†’ keep_ratio_mean=0.3205 (32.0%) (range=[0.128, 0.612])
tensor([1.2176, 0.5660, 0.8154, 0.8156, 1.2200, 0.9118, 1.2770, 1.8466, 2.1412,
        2.1412, 2.1412, 0.7152, 2.1412, 0.4282, 1.4505, 0.4282, 2.1412, 2.1353,
        1.8802, 2.1019, 1.4223, 2.1412, 0.4282, 2.1142, 2.1412, 0.4282, 1.5955,
        0.4282, 1.6831, 0.8054, 1.3530, 2.1412])
Layer 21: weight_mean=1.4124 (range=[0.428, 2.141]) â†’ keep_ratio_mean=0.4237 (42.4%) (range=[0.128, 0.642])
tensor([1.2828, 1.2140, 1.3876, 1.4556, 2.1412, 0.7387, 1.0005, 0.4951, 1.3982,
        0.8965, 1.1988, 1.0579, 0.9966, 0.9423, 0.4282, 1.2582, 0.5118, 1.4589,
        1.6226, 0.4282, 0.5741, 0.7657, 1.0461, 2.1412, 1.0958, 1.3456, 1.0953,
        0.4282, 0.7534, 1.0596, 1.2131, 1.0333])
Layer 22: weight_mean=1.0770 (range=[0.428, 2.141]) â†’ keep_ratio_mean=0.3231 (32.3%) (range=[0.128, 0.642])
tensor([0.6311, 1.9081, 2.1412, 0.6333, 0.4282, 1.2609, 0.9327, 0.9393, 1.1668,
        1.9174, 1.5072, 1.3898, 0.4282, 1.4104, 1.5591, 2.1412, 1.9618, 0.4282,
        1.4487, 1.4899, 1.0379, 0.9054, 1.7937, 1.2465, 1.2351, 0.4282, 1.6665,
        0.4282, 1.2858, 0.6936, 2.0859, 0.4282])
Layer 23: weight_mean=1.2175 (range=[0.428, 2.141]) â†’ keep_ratio_mean=0.3652 (36.5%) (range=[0.128, 0.642])
tensor([1.0498, 1.2389, 0.8868, 1.4674, 1.3077, 1.1082, 1.7436, 0.5177, 1.1158,
        0.4282, 1.1102, 1.3421, 1.2671, 1.1818, 1.3034, 1.4272, 1.1933, 1.2854,
        1.9449, 1.1452, 0.4282, 0.4282, 0.9899, 1.0426, 1.0489, 1.0999, 1.0123,
        1.5457, 1.2482, 1.5155, 1.0811, 0.9490])
Layer 24: weight_mean=1.1392 (range=[0.428, 1.945]) â†’ keep_ratio_mean=0.3418 (34.2%) (range=[0.128, 0.583])
tensor([2.1412, 0.4282, 2.1412, 1.0441, 2.1412, 1.6837, 1.5616, 2.0283, 1.2736,
        1.4021, 1.2068, 2.1412, 2.0509, 1.2779, 1.6218, 2.1412, 2.1014, 1.3468,
        0.9538, 1.6901, 1.9742, 1.6467, 0.4282, 0.4282, 0.5764, 1.8272, 1.8139,
        2.1412, 1.5933, 2.1412, 0.4282, 1.8776])
Layer 25: weight_mean=1.5392 (range=[0.428, 2.141]) â†’ keep_ratio_mean=0.4618 (46.2%) (range=[0.128, 0.642])
tensor([1.2442, 0.9532, 0.8583, 1.3166, 0.8991, 0.7878, 0.9032, 0.9005, 1.5983,
        0.9821, 1.0768, 1.4066, 1.8216, 1.5238, 0.8529, 1.0590, 1.4166, 1.3106,
        0.4282, 1.3283, 0.6707, 1.3711, 1.0709, 1.2548, 0.4282, 1.3756, 1.1970,
        0.7651, 0.8574, 1.4958, 1.1907, 0.9229])
Layer 26: weight_mean=1.1021 (range=[0.428, 1.822]) â†’ keep_ratio_mean=0.3306 (33.1%) (range=[0.128, 0.546])
tensor([0.7660, 0.7570, 1.0196, 1.0532, 1.0455, 1.2373, 0.9021, 1.2813, 1.1011,
        0.7803, 1.2160, 0.9339, 1.4426, 1.1330, 0.9784, 1.0316, 1.0341, 1.4328,
        1.3348, 0.9787, 0.4282, 0.9881, 0.7814, 1.4505, 0.9813, 1.1360, 1.4439,
        1.1859, 0.9587, 1.2460, 0.9915, 1.0904])
Layer 27: weight_mean=1.0669 (range=[0.428, 1.451]) â†’ keep_ratio_mean=0.3201 (32.0%) (range=[0.128, 0.435])
tensor([1.4880, 1.6242, 0.8637, 1.0099, 1.0289, 0.9355, 1.1709, 1.5293, 1.1997,
        1.4846, 1.0393, 1.2544, 1.1057, 0.8743, 0.8742, 1.0190, 0.7213, 0.8520,
        1.5323, 0.7798, 1.1608, 2.0609, 1.1889, 0.8048, 0.6467, 0.8043, 1.1335,
        1.4667, 0.8644, 1.0093, 0.8483, 2.1312])
Layer 28: weight_mean=1.1408 (range=[0.647, 2.131]) â†’ keep_ratio_mean=0.3423 (34.2%) (range=[0.194, 0.639])
tensor([1.0326, 0.8663, 1.5068, 0.7172, 0.7266, 0.9876, 0.5619, 0.9247, 0.7460,
        1.0327, 0.9147, 0.8097, 0.7967, 0.8898, 0.8138, 1.2482, 1.1150, 1.2056,
        1.5754, 0.7212, 1.7681, 0.8687, 0.8000, 2.1154, 1.1349, 1.3728, 0.8046,
        1.0443, 1.1696, 0.7756, 0.8277, 0.9033])
Layer 29: weight_mean=1.0243 (range=[0.562, 2.115]) â†’ keep_ratio_mean=0.3073 (30.7%) (range=[0.169, 0.635])
tensor([1.3075, 0.9550, 0.9606, 1.3248, 0.8049, 2.0039, 1.2704, 2.1412, 1.5137,
        0.6886, 0.9572, 0.9778, 1.5676, 1.1380, 1.1524, 1.5252, 1.1440, 1.1084,
        1.0213, 1.0238, 1.7880, 1.0813, 0.7515, 1.6378, 1.2911, 0.8318, 1.2670,
        0.9981, 0.9988, 1.3563, 1.0656, 1.2509])
Layer 30: weight_mean=1.2158 (range=[0.689, 2.141]) â†’ keep_ratio_mean=0.3647 (36.5%) (range=[0.207, 0.642])
tensor([1.9811, 2.1412, 2.1412, 1.1556, 1.9410, 1.8090, 2.1372, 1.5636, 0.8901,
        1.0857, 1.7943, 2.1149, 1.8762, 2.1412, 0.8362, 2.1412, 1.5179, 1.8636,
        1.3198, 1.2584, 0.7329, 0.8497, 1.9792, 1.2320, 1.0157, 0.7637, 1.1698,
        1.3683, 2.1412, 1.4214, 1.3121, 1.6899])
Layer 31: weight_mean=1.5433 (range=[0.733, 2.141]) â†’ keep_ratio_mean=0.4630 (46.3%) (range=[0.220, 0.642])

--------------------------------------------------------------------------------
Global Statistics:
--------------------------------------------------------------------------------
Relative weights:
  Mean:   1.0000 (should be â‰ˆ1.0)
  Std:    0.4549
  Range:  [0.428, 2.141]

Actual keep_ratios (weights Ã— select=0.3):
  Mean:   0.3000 (30.0%)
  Target: 0.3000 (30.0%)
  Deviation: 0.0000 (0.00%)
  Range:  [0.128, 0.642]
  Heads hitting upper limit (>1.0): 0/1024 (0.0%)
  âœ… Mean matches target (deviation < 1%)

--------------------------------------------------------------------------------
Layer-wise Variation:
--------------------------------------------------------------------------------
Layer means range: [0.1926, 0.5784]
Layer means std:   0.0892
Actual keep_ratio range across layers: [0.193, 0.578]
Variation: 38.6% spread
================================================================================

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 189.70it/s]
Converted 32 blocks to adaptive blocks

======================================================================
LLaDA Evaluation Setup
======================================================================
Model type: adaptive
Model path: GSAI-ML/LLaDA-8B-Base
Steps: 256, Gen length: 256, Block length: 32
Sparse params: skip=0.2, select=0.3, block_size=32
======================================================================

2025-12-28:15:19:17 INFO     [evaluator:305] humaneval: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-12-28:15:19:17 INFO     [api.task:434] Building contexts for humaneval on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2632.38it/s]
2025-12-28:15:19:17 INFO     [evaluator:574] Running generate_until requests
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2693.59 examples/s]
Generating...:   0%|          | 0/100 [00:00<?, ?it/s]Generating...:   1%|          | 1/100 [00:13<21:58, 13.32s/it]Generating...:   2%|â–         | 2/100 [00:27<22:49, 13.97s/it]Generating...:   3%|â–Ž         | 3/100 [00:40<21:57, 13.58s/it]Generating...:   4%|â–         | 4/100 [00:52<20:37, 12.89s/it]Generating...:   5%|â–Œ         | 5/100 [01:04<19:53, 12.56s/it]Generating...:   6%|â–Œ         | 6/100 [01:17<19:37, 12.53s/it]Generating...:   7%|â–‹         | 7/100 [01:28<18:57, 12.23s/it]Generating...:   8%|â–Š         | 8/100 [01:40<18:42, 12.21s/it]Generating...:   9%|â–‰         | 9/100 [01:52<18:24, 12.14s/it]Generating...:  10%|â–ˆ         | 10/100 [02:04<18:09, 12.10s/it]Generating...:  11%|â–ˆ         | 11/100 [02:18<18:33, 12.51s/it]Generating...:  12%|â–ˆâ–        | 12/100 [02:29<17:54, 12.20s/it]Generating...:  13%|â–ˆâ–Ž        | 13/100 [02:41<17:35, 12.13s/it]Generating...:  14%|â–ˆâ–        | 14/100 [02:53<17:02, 11.89s/it]Generating...:  15%|â–ˆâ–Œ        | 15/100 [03:04<16:39, 11.76s/it]Generating...:  16%|â–ˆâ–Œ        | 16/100 [03:16<16:27, 11.76s/it]Generating...:  17%|â–ˆâ–‹        | 17/100 [03:27<16:10, 11.69s/it]Generating...:  18%|â–ˆâ–Š        | 18/100 [03:40<16:24, 12.01s/it]Generating...:  19%|â–ˆâ–‰        | 19/100 [03:52<16:06, 11.93s/it]Generating...:  20%|â–ˆâ–ˆ        | 20/100 [04:04<15:59, 11.99s/it]Generating...:  21%|â–ˆâ–ˆ        | 21/100 [04:16<15:58, 12.13s/it]Generating...:  22%|â–ˆâ–ˆâ–       | 22/100 [04:29<15:55, 12.25s/it]Generating...:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [04:41<15:32, 12.10s/it]Generating...:  24%|â–ˆâ–ˆâ–       | 24/100 [04:52<15:02, 11.87s/it]Generating...:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [05:04<14:41, 11.76s/it]Generating...:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [05:16<14:45, 11.97s/it]Generating...:  27%|â–ˆâ–ˆâ–‹       | 27/100 [05:28<14:26, 11.87s/it]Generating...:  28%|â–ˆâ–ˆâ–Š       | 28/100 [05:40<14:15, 11.88s/it]Generating...:  29%|â–ˆâ–ˆâ–‰       | 29/100 [05:51<13:51, 11.71s/it]Generating...:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [06:02<13:36, 11.67s/it]Generating...:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [06:15<13:36, 11.83s/it]Generating...:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [06:27<13:32, 11.95s/it]Generating...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [06:43<14:33, 13.04s/it]Generating...:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [06:55<14:05, 12.80s/it]Generating...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [07:07<13:31, 12.48s/it]Generating...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [07:18<13:06, 12.30s/it]Generating...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [07:29<12:30, 11.91s/it]Generating...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [07:41<12:21, 11.96s/it]Generating...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [07:54<12:18, 12.11s/it]Generating...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [08:06<12:02, 12.04s/it]Generating...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [08:18<12:01, 12.23s/it]Generating...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [08:31<11:50, 12.24s/it]Generating...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [08:42<11:24, 12.01s/it]Generating...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [08:55<11:27, 12.28s/it]Generating...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [09:07<11:04, 12.08s/it]Generating...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [09:18<10:37, 11.81s/it]Generating...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [09:31<10:43, 12.15s/it]Generating...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [09:42<10:21, 11.95s/it]Generating...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [09:54<10:10, 11.97s/it]Generating...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [10:06<09:50, 11.80s/it]Generating...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [10:17<09:32, 11.68s/it]Generating...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [10:30<09:30, 11.88s/it]Generating...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [10:41<09:15, 11.82s/it]Generating...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [10:52<08:52, 11.57s/it]Generating...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [11:05<08:52, 11.83s/it]Generating...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [11:16<08:32, 11.64s/it]Generating...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [11:27<08:19, 11.61s/it]Generating...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [11:39<08:05, 11.55s/it]Generating...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [11:50<07:44, 11.34s/it]Generating...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [12:01<07:36, 11.42s/it]Generating...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [12:13<07:25, 11.42s/it]Generating...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [12:24<07:16, 11.49s/it]Generating...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [12:36<07:08, 11.58s/it]Generating...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [12:48<07:02, 11.73s/it]Generating...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [13:00<06:53, 11.81s/it]Generating...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [13:11<06:32, 11.56s/it]Generating...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [13:23<06:21, 11.55s/it]Generating...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [13:36<06:26, 12.08s/it]Generating...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [13:52<06:54, 13.39s/it]Generating...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [14:05<06:35, 13.19s/it]Generating...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [14:17<06:14, 12.92s/it]Generating...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [14:29<05:51, 12.57s/it]Generating...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [14:43<05:46, 12.84s/it]Generating...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [14:55<05:29, 12.68s/it]Generating...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [15:08<05:22, 12.90s/it]Generating...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [15:20<04:57, 12.39s/it]Generating...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [15:32<04:42, 12.30s/it]Generating...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [15:43<04:24, 12.02s/it]Generating...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [16:00<04:41, 13.40s/it]Generating...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [16:12<04:21, 13.07s/it]Generating...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [16:24<04:00, 12.66s/it]Generating...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [16:39<04:04, 13.57s/it]Generating...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [16:51<03:39, 12.92s/it]Generating...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [17:02<03:16, 12.31s/it]Generating...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [17:14<03:03, 12.24s/it]Generating...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [17:25<02:45, 11.84s/it]Generating...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [17:37<02:34, 11.91s/it]Generating...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [17:52<02:36, 13.07s/it]Generating...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [18:05<02:22, 12.98s/it]Generating...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [18:17<02:05, 12.50s/it]Generating...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [18:27<01:47, 11.93s/it]Generating...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [18:39<01:33, 11.74s/it]Generating...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [18:50<01:22, 11.72s/it]Generating...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [19:01<01:08, 11.49s/it]Generating...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [19:17<01:03, 12.80s/it]Generating...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [19:29<00:50, 12.58s/it]Generating...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [19:41<00:36, 12.33s/it]Generating...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [19:52<00:24, 12.07s/it]Generating...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [20:04<00:11, 11.97s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [20:16<00:00, 12.04s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [20:16<00:00, 12.17s/it]
2025-12-28:15:41:21 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-28:15:41:21 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: humaneval
llada_eval (model_path=GSAI-ML/LLaDA-8B-Base,model_type=adaptive,gen_length=256,steps=256,block_length=32,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,precomputed_importance_path=/home/qiheng/Projects/adaptive-dllm/configs/head_importance_llada_base_loss_gateIG_neg/head_importance.pt), gen_kwargs: (None), limit: 100.0, num_fewshot: None, batch_size: 1
|  Tasks  |Version|  Filter   |n-shot|Metric|   |Value|   |Stderr|
|---------|------:|-----------|-----:|------|---|----:|---|-----:|
|humaneval|      1|create_test|     0|pass@1|   | 0.46|Â±  |0.0501|

âœ… Completed adaptive on humaneval
â±ï¸  Running time: 23m 24s (1404s total)


================================================
âœ¨ All evaluations completed!
Finished at: Sunday, December 28, 2025 PM03:41:24 HKT
================================================

ðŸ“ Results saved in: results/
ðŸ“Š Timing log: results/timing_log.txt

ðŸ“ˆ Summary:

Task: gsm8k
  âŒ adaptive: FAILED

Task: humaneval
  âŒ adaptive: FAILED

