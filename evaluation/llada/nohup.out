nohup: ignoring input
========================================================
Quick Test Configuration
========================================================
Tasks: GSM8K, HumanEval
Model Types: standard, sparse, adaptive
Generation Length: 256 tokens
Block Size: 32
Test Samples: 50 per dataset
========================================================

ðŸš€ Starting quick test evaluation...
Started at: Friday, December 19, 2025 AM12:53:28 HKT


================================================
ðŸ“Š Task: HUMANEVAL
================================================

Progress: [1/1]

========================================
Running: adaptive on humaneval
========================================
Params: gen_length=256, steps=256, block_length=32, block_size=32, limit=50
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-19:00:53:36 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-19:00:53:36 INFO     [__main__:446] Selected Tasks: ['humaneval']
2025-12-19:00:53:36 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-19:00:53:36 INFO     [evaluator:240] Initializing llada_eval model, with arguments: {'model_path': 'GSAI-ML/LLaDA-8B-Base', 'model_type': 'adaptive', 'gen_length': 256, 'steps':
        256, 'block_length': 32, 'skip': 0.2, 'select': 0.3, 'block_size': 32, 'base_sparsity': 0.5, 'importance_source': 'precomputed'}
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
âœ“ Loading pre-computed importance scores from: /home/qiheng/Projects/adaptive-dllm/configs/head_importance_llada_base/head_importance.pt
âœ“ Created adaptive config using PRE-COMPUTED importance scores
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 200.18it/s]
[Layer 0] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.9000, 0.1174, 0.1039, 0.1011, 0.1101, 0.1396, 0.1175, 0.1381, 0.1175,
        0.1221, 0.1069, 0.1022, 0.1020, 0.1238, 0.1026, 0.1404, 0.1042, 0.1115,
        0.1686, 0.1089, 0.1096, 0.1018, 0.1031, 0.1000, 0.1456, 0.1224, 0.1020,
        0.1040, 0.1070, 0.1098, 0.2253, 0.1004])
[Layer 1] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.1017, 0.1056, 0.1020, 0.1077, 0.1424, 0.1029, 0.1568, 0.1267, 0.1004,
        0.1109, 0.1002, 0.1009, 0.1060, 0.1143, 0.1000, 0.1024, 0.1007, 0.1055,
        0.1010, 0.1001, 0.1120, 0.1141, 0.1032, 0.1029, 0.1075, 0.9000, 0.1284,
        0.1264, 0.1020, 0.1120, 0.1019, 0.1008])
[Layer 2] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.1817, 0.5818, 0.1104, 0.1107, 0.1057, 0.2754, 0.2173, 0.1594, 0.1043,
        0.2186, 0.1085, 0.1199, 0.1075, 0.1773, 0.1370, 0.1447, 0.2243, 0.7005,
        0.1189, 0.4588, 0.1348, 0.9000, 0.2569, 0.1202, 0.1000, 0.1082, 0.2491,
        0.1230, 0.3876, 0.1307, 0.1180, 0.2451])
[Layer 3] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.3037, 0.4269, 0.1942, 0.2772, 0.1130, 0.1703, 0.7897, 0.2131, 0.1092,
        0.1895, 0.4157, 0.1002, 0.9000, 0.1599, 0.1337, 0.1585, 0.2810, 0.1607,
        0.1123, 0.4055, 0.1645, 0.1511, 0.1631, 0.2102, 0.1365, 0.2614, 0.1022,
        0.1202, 0.1174, 0.2642, 0.1000, 0.1033])
[Layer 4] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.2840, 0.3776, 0.1974, 0.1366, 0.1424, 0.1877, 0.8652, 0.2189, 0.3500,
        0.1545, 0.1466, 0.1390, 0.2008, 0.3234, 0.1431, 0.1185, 0.1250, 0.2133,
        0.3291, 0.1594, 0.2563, 0.7324, 0.2533, 0.8078, 0.1307, 0.1837, 0.2691,
        0.2833, 0.1782, 0.1000, 0.6909, 0.9000])
[Layer 5] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.1000, 0.2767, 0.3688, 0.5968, 0.2019, 0.1907, 0.1019, 0.1716, 0.1688,
        0.3142, 0.1575, 0.1045, 0.1495, 0.1956, 0.1407, 0.1452, 0.1002, 0.1266,
        0.1503, 0.4883, 0.1133, 0.1941, 0.3419, 0.2844, 0.3165, 0.9000, 0.1443,
        0.1087, 0.4689, 0.1258, 0.1547, 0.6024])
[Layer 6] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.2293, 0.1149, 0.1572, 0.4363, 0.1081, 0.1347, 0.1636, 0.2504, 0.1000,
        0.1632, 0.2221, 0.2296, 0.2914, 0.1181, 0.1787, 0.1146, 0.1636, 0.9000,
        0.1058, 0.1378, 0.1380, 0.3711, 0.1536, 0.2936, 0.1110, 0.1093, 0.1795,
        0.1105, 0.2041, 0.1289, 0.1071, 0.1150])
[Layer 7] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.1008, 0.1580, 0.3062, 0.1133, 0.1518, 0.8754, 0.1368, 0.1238, 0.2366,
        0.1423, 0.1285, 0.2847, 0.1943, 0.1895, 0.2137, 0.1909, 0.3186, 0.1000,
        0.9000, 0.1848, 0.5087, 0.1949, 0.1162, 0.3507, 0.2308, 0.2342, 0.1083,
        0.1843, 0.1516, 0.3463, 0.2097, 0.1165])
[Layer 8] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.1359, 0.4715, 0.1053, 0.1001, 0.1654, 0.1942, 0.1357, 0.2639, 0.1668,
        0.2556, 0.1151, 0.1000, 0.1547, 0.1124, 0.2533, 0.1859, 0.2403, 0.3352,
        0.1155, 0.9000, 0.1196, 0.2198, 0.1094, 0.2076, 0.2989, 0.3131, 0.2987,
        0.1761, 0.1108, 0.7017, 0.1108, 0.1002])
[Layer 9] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.1416, 0.1078, 0.9000, 0.5541, 0.2748, 0.1101, 0.3095, 0.1087, 0.7616,
        0.1560, 0.1683, 0.1884, 0.8479, 0.5650, 0.3465, 0.1000, 0.2270, 0.2105,
        0.1450, 0.1748, 0.1811, 0.1919, 0.4166, 0.1808, 0.1015, 0.2556, 0.1861,
        0.5298, 0.3659, 0.1816, 0.4474, 0.7817])
[Layer 10] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.4904, 0.1271, 0.1449, 0.1271, 0.1908, 0.1717, 0.4661, 0.9000, 0.1110,
        0.1161, 0.2389, 0.8349, 0.1139, 0.1063, 0.1000, 0.1980, 0.1086, 0.1461,
        0.2752, 0.1017, 0.1988, 0.1505, 0.1319, 0.1408, 0.2073, 0.1808, 0.6511,
        0.1113, 0.7291, 0.2611, 0.2163, 0.1654])
[Layer 11] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.4392, 0.1008, 0.1983, 0.2521, 0.1439, 0.9000, 0.1457, 0.1879, 0.1084,
        0.1645, 0.1491, 0.3363, 0.1173, 0.2538, 0.1355, 0.1278, 0.1638, 0.1700,
        0.2235, 0.1790, 0.1715, 0.1818, 0.2604, 0.2758, 0.1207, 0.1000, 0.1774,
        0.2085, 0.2158, 0.4203, 0.1762, 0.1217])
[Layer 12] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.5145, 0.1909, 0.2706, 0.1550, 0.2887, 0.1137, 0.1475, 0.3726, 0.1135,
        0.2415, 0.2174, 0.1753, 0.3139, 0.3068, 0.9000, 0.1137, 0.1000, 0.1160,
        0.7744, 0.1457, 0.2199, 0.2224, 0.1280, 0.1812, 0.5171, 0.2119, 0.1602,
        0.1909, 0.2122, 0.7462, 0.3047, 0.2419])
[Layer 13] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.8363, 0.4472, 0.2046, 0.5229, 0.1845, 0.2413, 0.1030, 0.9000, 0.2948,
        0.6367, 0.4482, 0.6521, 0.3302, 0.2598, 0.5243, 0.2677, 0.3145, 0.1993,
        0.1289, 0.2683, 0.1999, 0.4788, 0.1147, 0.1564, 0.5097, 0.2512, 0.1972,
        0.1000, 0.1290, 0.2550, 0.1024, 0.5703])
[Layer 14] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.1000, 0.4694, 0.1332, 0.2364, 0.3586, 0.1137, 0.2086, 0.4158, 0.5320,
        0.2813, 0.3424, 0.3492, 0.1877, 0.9000, 0.3176, 0.1194, 0.2234, 0.1139,
        0.1886, 0.8752, 0.3618, 0.1422, 0.2136, 0.4451, 0.1663, 0.1508, 0.4105,
        0.4211, 0.1897, 0.4194, 0.4835, 0.2526])
[Layer 15] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.7677, 0.1021, 0.3598, 0.2836, 0.1337, 0.6831, 0.3053, 0.6434, 0.3166,
        0.5244, 0.2734, 0.6711, 0.4346, 0.2961, 0.2864, 0.2994, 0.1100, 0.6858,
        0.3151, 0.1880, 0.1000, 0.2285, 0.5234, 0.2873, 0.1036, 0.2963, 0.3976,
        0.5199, 0.1919, 0.1941, 0.9000, 0.2818])
[Layer 16] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.1003, 0.1863, 0.3966, 0.1999, 0.9000, 0.1512, 0.2056, 0.1886, 0.1481,
        0.4688, 0.2038, 0.3963, 0.2395, 0.1666, 0.1827, 0.2318, 0.1370, 0.1872,
        0.1000, 0.2664, 0.3330, 0.1695, 0.1063, 0.2250, 0.1172, 0.3295, 0.2405,
        0.2894, 0.1330, 0.1320, 0.1449, 0.1207])
[Layer 17] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.2444, 0.1000, 0.1118, 0.3572, 0.2026, 0.3682, 0.9000, 0.2055, 0.2273,
        0.1592, 0.4078, 0.1444, 0.2156, 0.3089, 0.5114, 0.3722, 0.2359, 0.3085,
        0.3083, 0.1433, 0.3999, 0.2568, 0.1941, 0.3372, 0.1648, 0.2594, 0.1076,
        0.3499, 0.1992, 0.5044, 0.2715, 0.4003])
[Layer 18] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.3152, 0.4362, 0.2758, 0.3256, 0.2307, 0.4304, 0.4310, 0.3237, 0.8142,
        0.4385, 0.8459, 0.4178, 0.2692, 0.5785, 0.4576, 0.4922, 0.4937, 0.3549,
        0.1503, 0.2957, 0.3261, 0.7273, 0.6470, 0.9000, 0.2887, 0.1000, 0.1503,
        0.2898, 0.2357, 0.4459, 0.2637, 0.3249])
[Layer 19] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.4022, 0.4737, 0.1000, 0.3196, 0.2771, 0.6395, 0.1214, 0.1318, 0.2833,
        0.2808, 0.1992, 0.1386, 0.4244, 0.1149, 0.5291, 0.4524, 0.2520, 0.9000,
        0.2907, 0.1785, 0.1130, 0.6857, 0.1906, 0.6962, 0.4906, 0.1143, 0.2577,
        0.3373, 0.1536, 0.2758, 0.2162, 0.4195])
[Layer 20] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.6730, 0.1240, 0.1730, 0.9000, 0.1466, 0.1699, 0.2009, 0.2006, 0.3638,
        0.1345, 0.1503, 0.3651, 0.4627, 0.1034, 0.1512, 0.1790, 0.1959, 0.2064,
        0.1524, 0.2120, 0.2700, 0.1053, 0.1000, 0.2289, 0.1198, 0.3616, 0.3426,
        0.1568, 0.1575, 0.1398, 0.1002, 0.1979])
[Layer 21] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.1792, 0.2959, 0.1980, 0.6985, 0.3060, 0.6149, 0.2166, 0.4963, 0.1373,
        0.2251, 0.6275, 0.1700, 0.4589, 0.3241, 0.1000, 0.1861, 0.6721, 0.3013,
        0.6670, 0.1825, 0.2877, 0.4924, 0.2261, 0.3614, 0.4553, 0.9000, 0.3626,
        0.5855, 0.2078, 0.3776, 0.1660, 0.4050])
[Layer 22] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.1444, 0.3303, 0.2219, 0.2513, 0.2838, 0.2426, 0.1433, 0.2439, 0.1227,
        0.2028, 0.1477, 0.1067, 0.2056, 0.1975, 0.3192, 0.1000, 0.1773, 0.1869,
        0.2404, 0.3118, 0.1588, 0.3890, 0.1359, 0.9000, 0.1386, 0.1178, 0.1967,
        0.1219, 0.2062, 0.2357, 0.1545, 0.1662])
[Layer 23] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.2618, 0.1970, 0.4772, 0.3037, 0.9000, 0.2050, 0.1441, 0.2260, 0.4297,
        0.4795, 0.1913, 0.1864, 0.5435, 0.5437, 0.2011, 0.5705, 0.1906, 0.4884,
        0.6931, 0.1021, 0.2593, 0.2243, 0.4400, 0.1000, 0.1817, 0.1451, 0.4782,
        0.2189, 0.1676, 0.1480, 0.2214, 0.2949])
[Layer 24] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.2138, 0.1774, 0.1272, 0.2926, 0.5503, 0.1806, 0.5731, 0.2578, 0.1997,
        0.2529, 0.1232, 0.1173, 0.3445, 0.2159, 0.2712, 0.4239, 0.1209, 0.3522,
        0.2328, 0.1017, 0.2084, 0.9000, 0.1977, 0.1068, 0.1904, 0.1996, 0.1000,
        0.3541, 0.1221, 0.2062, 0.2497, 0.2944])
[Layer 25] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.3805, 0.1811, 0.2674, 0.1222, 0.1969, 0.2175, 0.1213, 0.3796, 0.1000,
        0.1266, 0.1269, 0.1367, 0.1684, 0.2382, 0.2435, 0.4118, 0.1038, 0.1750,
        0.1231, 0.1539, 0.1876, 0.2040, 0.1313, 0.4672, 0.1021, 0.6610, 0.8298,
        0.1950, 0.1883, 0.9000, 0.1029, 0.1918])
[Layer 26] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.2702, 0.1158, 0.1000, 0.2154, 0.1478, 0.1624, 0.5439, 0.1080, 0.4732,
        0.2934, 0.1247, 0.2242, 0.3986, 0.2455, 0.2996, 0.1873, 0.5930, 0.5423,
        0.9000, 0.2098, 0.1386, 0.2475, 0.1619, 0.3376, 0.4309, 0.1485, 0.3996,
        0.2887, 0.1312, 0.5338, 0.1318, 0.1263])
[Layer 27] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.1983, 0.1262, 0.5872, 0.2920, 0.2776, 0.5474, 0.1646, 0.1960, 0.1053,
        0.1224, 0.1509, 0.1648, 0.2468, 0.1762, 0.1660, 0.2138, 0.2363, 0.2017,
        0.1000, 0.1709, 0.9000, 0.1625, 0.1194, 0.1883, 0.2147, 0.5532, 0.1215,
        0.1659, 0.1508, 0.1466, 0.1231, 0.2225])
[Layer 28] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.3972, 0.2301, 0.1618, 0.5332, 0.1593, 0.1338, 0.1000, 0.3185, 0.3369,
        0.5031, 0.3019, 0.4265, 0.3037, 0.1693, 0.4182, 0.3473, 0.1236, 0.2966,
        0.9000, 0.1508, 0.5394, 0.8678, 0.1610, 0.2531, 0.6814, 0.1306, 0.8598,
        0.4013, 0.2804, 0.1132, 0.1666, 0.8712])
[Layer 29] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.6873, 0.2638, 0.3759, 0.4391, 0.5644, 0.2951, 0.2632, 0.1000, 0.3721,
        0.7320, 0.1425, 0.1341, 0.3423, 0.1174, 0.2128, 0.9000, 0.2734, 0.2375,
        0.3019, 0.1494, 0.3301, 0.1908, 0.2027, 0.4875, 0.5543, 0.7467, 0.2990,
        0.1457, 0.2753, 0.1835, 0.1658, 0.4002])
[Layer 30] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.3264, 0.1189, 0.1616, 0.4480, 0.1554, 0.2621, 0.4945, 0.3089, 0.9000,
        0.1282, 0.2675, 0.2462, 0.4297, 0.1361, 0.1038, 0.1411, 0.1130, 0.1095,
        0.1275, 0.1000, 0.6214, 0.1453, 0.1269, 0.4457, 0.1631, 0.1194, 0.2206,
        0.1517, 0.2364, 0.1375, 0.1160, 0.3872])
[Layer 31] Set adaptive sparsity levels:
  KV heads: 32, Query heads: 32
  Keep ratios per group: tensor([0.5801, 0.4588, 0.1072, 0.1750, 0.1105, 0.2590, 0.1267, 0.1769, 0.2514,
        0.1183, 0.1466, 0.5723, 0.2653, 0.1744, 0.1031, 0.9000, 0.1480, 0.1373,
        0.1616, 0.2824, 0.1304, 0.2263, 0.1568, 0.1509, 0.3393, 0.1000, 0.2228,
        0.1829, 0.2156, 0.1023, 0.1175, 0.2294])
Converted 32 blocks to adaptive blocks

======================================================================
LLaDA Evaluation Setup
======================================================================
Model type: adaptive
Model path: GSAI-ML/LLaDA-8B-Base
Steps: 256, Gen length: 256, Block length: 32
Sparse params: skip=0.2, select=0.3, block_size=32
======================================================================

2025-12-19:00:54:40 INFO     [evaluator:305] humaneval: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-12-19:00:54:40 INFO     [api.task:434] Building contexts for humaneval on rank 0...
  0%|          | 0/50 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 4181.09it/s]
2025-12-19:00:54:40 INFO     [evaluator:574] Running generate_until requests
Map:   0%|          | 0/50 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 2270.06 examples/s]
Generating...:   0%|          | 0/50 [00:00<?, ?it/s]Generating...:   2%|â–         | 1/50 [00:12<10:11, 12.48s/it]Generating...:   4%|â–         | 2/50 [00:25<10:12, 12.77s/it]Generating...:   6%|â–Œ         | 3/50 [00:38<09:57, 12.72s/it]Generating...:   8%|â–Š         | 4/50 [00:48<09:07, 11.91s/it]Generating...:  10%|â–ˆ         | 5/50 [00:59<08:42, 11.60s/it]Generating...:  12%|â–ˆâ–        | 6/50 [01:10<08:22, 11.41s/it]Generating...:  14%|â–ˆâ–        | 7/50 [01:21<08:00, 11.17s/it]Generating...:  16%|â–ˆâ–Œ        | 8/50 [01:32<07:50, 11.20s/it]Generating...:  18%|â–ˆâ–Š        | 9/50 [01:44<07:39, 11.22s/it]Generating...:  20%|â–ˆâ–ˆ        | 10/50 [01:55<07:29, 11.24s/it]Generating...:  22%|â–ˆâ–ˆâ–       | 11/50 [02:07<07:33, 11.63s/it]Generating...:  24%|â–ˆâ–ˆâ–       | 12/50 [02:18<07:10, 11.33s/it]Generating...:  26%|â–ˆâ–ˆâ–Œ       | 13/50 [02:29<06:59, 11.34s/it]Generating...:  28%|â–ˆâ–ˆâ–Š       | 14/50 [02:40<06:44, 11.23s/it]Generating...:  30%|â–ˆâ–ˆâ–ˆ       | 15/50 [02:51<06:26, 11.04s/it]Generating...:  32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [03:02<06:14, 11.02s/it]Generating...:  34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [03:13<06:04, 11.03s/it]Generating...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [03:25<06:03, 11.36s/it]Generating...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [03:36<05:51, 11.33s/it]Generating...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [03:48<05:42, 11.43s/it]Generating...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [04:00<05:34, 11.54s/it]Generating...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [04:11<05:23, 11.57s/it]Generating...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [04:22<05:07, 11.38s/it]Generating...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [04:33<04:49, 11.13s/it]Generating...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [04:44<04:34, 10.97s/it]Generating...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [04:55<04:28, 11.19s/it]Generating...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [05:06<04:15, 11.11s/it]Generating...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [05:17<04:03, 11.06s/it]Generating...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [05:28<03:49, 10.91s/it]Generating...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [05:39<03:37, 10.89s/it]Generating...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [05:50<03:27, 10.94s/it]Generating...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [06:01<03:17, 10.95s/it]Generating...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [06:16<03:27, 12.21s/it]Generating...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [06:27<03:12, 12.01s/it]Generating...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [06:38<02:54, 11.66s/it]Generating...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [06:49<02:39, 11.41s/it]Generating...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [06:59<02:24, 11.15s/it]Generating...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [07:11<02:13, 11.16s/it]Generating...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [07:22<02:04, 11.29s/it]Generating...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [07:33<01:52, 11.25s/it]Generating...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [07:45<01:42, 11.43s/it]Generating...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [07:57<01:32, 11.51s/it]Generating...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [08:08<01:19, 11.36s/it]Generating...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [08:20<01:09, 11.54s/it]Generating...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [08:31<00:56, 11.30s/it]Generating...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [08:41<00:44, 11.05s/it]Generating...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [08:53<00:33, 11.26s/it]Generating...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [09:04<00:22, 11.13s/it]Generating...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [09:15<00:11, 11.17s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [09:26<00:00, 11.07s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [09:26<00:00, 11.33s/it]
2025-12-19:01:05:25 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-19:01:05:25 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: humaneval
llada_eval (model_path=GSAI-ML/LLaDA-8B-Base,model_type=adaptive,gen_length=256,steps=256,block_length=32,skip=0.2,select=0.3,block_size=32,base_sparsity=0.5,importance_source=precomputed), gen_kwargs: (None), limit: 50.0, num_fewshot: None, batch_size: 1
|  Tasks  |Version|  Filter   |n-shot|Metric|   |Value|   |Stderr|
|---------|------:|-----------|-----:|------|---|----:|---|-----:|
|humaneval|      1|create_test|     0|pass@1|   | 0.58|Â±  |0.0705|

âœ… Completed adaptive on humaneval
â±ï¸  Running time: 11m 59s (719s total)


================================================
âœ¨ All evaluations completed!
Finished at: Friday, December 19, 2025 AM01:05:27 HKT
================================================

ðŸ“ Results saved in: results/
ðŸ“Š Timing log: results/timing_log.txt

ðŸ“ˆ Summary:

Task: humaneval
  âŒ adaptive: FAILED

