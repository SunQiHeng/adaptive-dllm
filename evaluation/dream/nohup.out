========================================================
Dream Quick Test Configuration
========================================================
Tasks: GSM8K, HumanEval
Model Types: standard, sparse, adaptive
Max New Tokens: 256
Block Size: 32
Test Samples: 50 per dataset
========================================================

ğŸš€ Starting Dream quick test evaluation...
Started at: Monday, December 22, 2025 AM01:02:43 HKT


================================================
ğŸ“Š Task: GSM8K
================================================

Progress: [1/6]

========================================
Running: standard on gsm8k
========================================
Params: max_new_tokens=256, steps=256, block_size=32, limit=100
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-22:01:02:52 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-22:01:02:52 INFO     [__main__:446] Selected Tasks: ['gsm8k']
2025-12-22:01:02:52 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-22:01:02:52 INFO     [evaluator:240] Initializing dream_eval model, with arguments: {'model_path': '/data/qh_models/Dream-v0-Instruct-7B', 'model_type': 'standard',
        'max_new_tokens': 256, 'steps': 256, 'temperature': 0.8, 'top_p': 0.9, 'alg': 'entropy', 'alg_temp': 1.5, 'skip': 0.2, 'select':
        0.3, 'block_size': 32, 'importance_source': 'precomputed', 'min_sparsity': 0.15, 'max_sparsity': 0.85}

Loading Dream model from /data/qh_models/Dream-v0-Instruct-7B...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 109.22it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
2025-12-22:01:03:03 INFO     [evaluator:305] gsm8k: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}
2025-12-22:01:03:03 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k from 5 to 0
2025-12-22:01:03:03 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-12-22:01:03:03 INFO     [api.task:434] Building contexts for gsm8k on rank 0...

======================================================================
Dream Evaluation Setup
======================================================================
Model type: standard
Model path: /data/qh_models/Dream-v0-Instruct-7B
Steps: 256, Max new tokens: 256
Temperature: 0.8, Top-p: 0.9, Top-k: None
======================================================================

  0%|          | 0/100 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2341.45it/s]
2025-12-22:01:03:03 INFO     [evaluator:574] Running generate_until requests
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1206.23 examples/s]
Generating...:   0%|          | 0/100 [00:00<?, ?it/s]Generating...:   1%|          | 1/100 [00:07<12:09,  7.37s/it]Generating...:   2%|â–         | 2/100 [00:13<10:38,  6.52s/it]Generating...:   3%|â–         | 3/100 [00:20<10:50,  6.71s/it]Generating...:   4%|â–         | 4/100 [00:26<10:15,  6.41s/it]Generating...:   5%|â–Œ         | 5/100 [00:33<10:25,  6.59s/it]Generating...:   6%|â–Œ         | 6/100 [00:40<10:31,  6.72s/it]Generating...:   7%|â–‹         | 7/100 [00:47<10:33,  6.81s/it]Generating...:   8%|â–Š         | 8/100 [00:54<10:36,  6.92s/it]Generating...:   9%|â–‰         | 9/100 [01:01<10:39,  7.02s/it]Generating...:  10%|â–ˆ         | 10/100 [01:08<10:34,  7.05s/it]Generating...:  11%|â–ˆ         | 11/100 [01:15<10:28,  7.06s/it]Generating...:  12%|â–ˆâ–        | 12/100 [01:22<10:22,  7.08s/it]Generating...:  13%|â–ˆâ–        | 13/100 [01:29<10:17,  7.10s/it]Generating...:  14%|â–ˆâ–        | 14/100 [01:37<10:11,  7.11s/it]Generating...:  15%|â–ˆâ–Œ        | 15/100 [01:44<10:04,  7.11s/it]Generating...:  16%|â–ˆâ–Œ        | 16/100 [01:51<10:02,  7.17s/it]Generating...:  17%|â–ˆâ–‹        | 17/100 [01:58<09:53,  7.15s/it]Generating...:  18%|â–ˆâ–Š        | 18/100 [02:05<09:45,  7.14s/it]Generating...:  19%|â–ˆâ–‰        | 19/100 [02:11<09:12,  6.82s/it]Generating...:  20%|â–ˆâ–ˆ        | 20/100 [02:18<09:13,  6.92s/it]Generating...:  21%|â–ˆâ–ˆ        | 21/100 [02:26<09:11,  6.98s/it]Generating...:  22%|â–ˆâ–ˆâ–       | 22/100 [02:33<09:05,  6.99s/it]Generating...:  23%|â–ˆâ–ˆâ–       | 23/100 [02:40<09:01,  7.03s/it]Generating...:  24%|â–ˆâ–ˆâ–       | 24/100 [02:46<08:31,  6.73s/it]Generating...:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [02:52<08:09,  6.52s/it]Generating...:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [02:59<08:17,  6.73s/it]Generating...:  27%|â–ˆâ–ˆâ–‹       | 27/100 [03:06<08:20,  6.86s/it]Generating...:  28%|â–ˆâ–ˆâ–Š       | 28/100 [03:13<08:19,  6.94s/it]Generating...:  29%|â–ˆâ–ˆâ–‰       | 29/100 [03:20<08:15,  6.99s/it]Generating...:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [03:28<08:12,  7.04s/it]Generating...:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [03:34<07:52,  6.84s/it]Generating...:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [03:41<07:52,  6.94s/it]Generating...:  33%|â–ˆâ–ˆâ–ˆâ–      | 33/100 [03:47<07:28,  6.69s/it]Generating...:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [03:53<07:08,  6.50s/it]Generating...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [03:59<06:54,  6.37s/it]Generating...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [04:06<07:02,  6.60s/it]Generating...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [04:14<07:06,  6.76s/it]Generating...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [04:21<07:08,  6.91s/it]Generating...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [04:28<07:05,  6.98s/it]Generating...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [04:35<07:03,  7.07s/it]Generating...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [04:42<06:58,  7.08s/it]Generating...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [04:50<06:52,  7.12s/it]Generating...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/100 [04:57<06:48,  7.17s/it]Generating...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [05:04<06:42,  7.19s/it]Generating...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [05:11<06:36,  7.21s/it]Generating...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [05:19<06:31,  7.25s/it]Generating...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [05:26<06:24,  7.26s/it]Generating...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [05:33<06:14,  7.21s/it]Generating...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [05:39<05:51,  6.89s/it]Generating...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [05:46<05:46,  6.94s/it]Generating...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [05:52<05:27,  6.68s/it]Generating...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [05:59<05:26,  6.80s/it]Generating...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/100 [06:07<05:24,  6.91s/it]Generating...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [06:14<05:23,  7.03s/it]Generating...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [06:21<05:19,  7.10s/it]Generating...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [06:28<05:13,  7.12s/it]Generating...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [06:35<05:05,  7.10s/it]Generating...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [06:43<05:00,  7.16s/it]Generating...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [06:50<04:54,  7.19s/it]Generating...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [06:56<04:33,  6.83s/it]Generating...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [07:02<04:17,  6.60s/it]Generating...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [07:09<04:17,  6.77s/it]Generating...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 63/100 [07:16<04:15,  6.90s/it]Generating...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [07:24<04:12,  7.00s/it]Generating...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [07:31<04:08,  7.10s/it]Generating...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [07:38<04:01,  7.10s/it]Generating...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [07:45<03:54,  7.11s/it]Generating...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [07:52<03:48,  7.13s/it]Generating...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [07:59<03:33,  6.88s/it]Generating...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [08:06<03:28,  6.95s/it]Generating...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [08:13<03:23,  7.00s/it]Generating...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [08:20<03:16,  7.03s/it]Generating...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 73/100 [08:27<03:10,  7.06s/it]Generating...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [08:34<03:04,  7.09s/it]Generating...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [08:41<02:57,  7.12s/it]Generating...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [08:49<02:51,  7.14s/it]Generating...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [08:56<02:45,  7.18s/it]Generating...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [09:03<02:37,  7.15s/it]Generating...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [09:10<02:29,  7.12s/it]Generating...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [09:17<02:22,  7.13s/it]Generating...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [09:23<02:10,  6.86s/it]Generating...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [09:31<02:05,  6.96s/it]Generating...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83/100 [09:37<01:54,  6.71s/it]Generating...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [09:43<01:43,  6.50s/it]Generating...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [09:49<01:35,  6.38s/it]Generating...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [09:56<01:33,  6.65s/it]Generating...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [10:03<01:29,  6.85s/it]Generating...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [10:11<01:23,  6.96s/it]Generating...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [10:17<01:13,  6.71s/it]Generating...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [10:24<01:08,  6.83s/it]Generating...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [10:31<01:02,  6.96s/it]Generating...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [10:37<00:53,  6.68s/it]Generating...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 93/100 [10:44<00:47,  6.80s/it]Generating...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [10:52<00:41,  6.95s/it]Generating...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [10:59<00:34,  7.00s/it]Generating...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [11:06<00:28,  7.02s/it]Generating...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [11:12<00:20,  6.72s/it]Generating...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [11:19<00:13,  6.83s/it]Generating...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [11:26<00:06,  6.97s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [11:33<00:00,  7.05s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [11:33<00:00,  6.94s/it]
2025-12-22:01:15:46 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-22:01:15:46 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: gsm8k
dream_eval (model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=standard,max_new_tokens=256,steps=256,temperature=0.8,top_p=0.9,alg=entropy,alg_temp=1.5,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85), gen_kwargs: (None), limit: 100.0, num_fewshot: 0, batch_size: 1
|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|
|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|
|gsm8k|      3|flexible-extract|     0|exact_match|â†‘  | 0.45|Â±  |0.0500|
|     |       |strict-match    |     0|exact_match|â†‘  | 0.15|Â±  |0.0359|

âœ… Completed standard on gsm8k
â±ï¸  Running time: 13m 5s (785s total)


Progress: [2/6]

========================================
Running: sparse on gsm8k
========================================
Params: max_new_tokens=256, steps=256, block_size=32, limit=100
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-22:01:15:57 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-22:01:15:57 INFO     [__main__:446] Selected Tasks: ['gsm8k']
2025-12-22:01:15:57 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-22:01:15:57 INFO     [evaluator:240] Initializing dream_eval model, with arguments: {'model_path': '/data/qh_models/Dream-v0-Instruct-7B', 'model_type': 'sparse',
        'max_new_tokens': 256, 'steps': 256, 'temperature': 0.8, 'top_p': 0.9, 'alg': 'entropy', 'alg_temp': 1.5, 'skip': 0.2, 'select':
        0.3, 'block_size': 32, 'importance_source': 'precomputed', 'min_sparsity': 0.15, 'max_sparsity': 0.85}
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]

Loading Dream model from /data/qh_models/Dream-v0-Instruct-7B...
Using the default sparsity block size: 128
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 116.24it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
2025-12-22:01:16:08 INFO     [evaluator:305] gsm8k: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}
2025-12-22:01:16:08 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k from 5 to 0
2025-12-22:01:16:08 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-12-22:01:16:08 INFO     [api.task:434] Building contexts for gsm8k on rank 0...

======================================================================
Dream Evaluation Setup
======================================================================
Model type: sparse
Model path: /data/qh_models/Dream-v0-Instruct-7B
Steps: 256, Max new tokens: 256
Temperature: 0.8, Top-p: 0.9, Top-k: None
Sparse params: skip=0.2, select=0.3, block_size=32
======================================================================

  0%|          | 0/100 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2366.75it/s]
2025-12-22:01:16:08 INFO     [evaluator:574] Running generate_until requests
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1241.54 examples/s]
Generating...:   0%|          | 0/100 [00:00<?, ?it/s]2025-12-22:01:17:14 WARNING  [models.Dream.core.sparsed_modeling_dream:328] The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
Generating...:   1%|          | 1/100 [00:10<16:57, 10.28s/it]Generating...:   2%|â–         | 2/100 [00:21<17:22, 10.64s/it]Generating...:   3%|â–         | 3/100 [00:30<15:59,  9.89s/it]Generating...:   4%|â–         | 4/100 [00:39<15:11,  9.49s/it]Generating...:   5%|â–Œ         | 5/100 [00:48<14:48,  9.35s/it]Generating...:   6%|â–Œ         | 6/100 [00:57<14:26,  9.22s/it]Generating...:   7%|â–‹         | 7/100 [01:06<14:08,  9.12s/it]Generating...:   8%|â–Š         | 8/100 [01:14<13:50,  9.03s/it]Generating...:   9%|â–‰         | 9/100 [01:23<13:37,  8.98s/it]Generating...:  10%|â–ˆ         | 10/100 [01:32<13:24,  8.94s/it]Generating...:  11%|â–ˆ         | 11/100 [01:41<13:12,  8.90s/it]Generating...:  12%|â–ˆâ–        | 12/100 [01:49<12:34,  8.57s/it]Generating...:  13%|â–ˆâ–        | 13/100 [01:58<12:34,  8.67s/it]Generating...:  14%|â–ˆâ–        | 14/100 [02:06<12:25,  8.67s/it]Generating...:  15%|â–ˆâ–Œ        | 15/100 [02:15<12:28,  8.80s/it]Generating...:  16%|â–ˆâ–Œ        | 16/100 [02:24<12:25,  8.87s/it]Generating...:  17%|â–ˆâ–‹        | 17/100 [02:33<12:17,  8.89s/it]Generating...:  18%|â–ˆâ–Š        | 18/100 [02:42<11:59,  8.78s/it]Generating...:  19%|â–ˆâ–‰        | 19/100 [02:51<11:51,  8.78s/it]Generating...:  20%|â–ˆâ–ˆ        | 20/100 [03:00<11:59,  9.00s/it]Generating...:  21%|â–ˆâ–ˆ        | 21/100 [03:10<12:01,  9.13s/it]Generating...:  22%|â–ˆâ–ˆâ–       | 22/100 [03:19<11:47,  9.07s/it]Generating...:  23%|â–ˆâ–ˆâ–       | 23/100 [03:28<11:40,  9.10s/it]Generating...:  24%|â–ˆâ–ˆâ–       | 24/100 [03:37<11:25,  9.02s/it]Generating...:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [03:45<11:08,  8.91s/it]Generating...:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [03:54<10:57,  8.88s/it]Generating...:  27%|â–ˆâ–ˆâ–‹       | 27/100 [04:03<10:50,  8.91s/it]Generating...:  28%|â–ˆâ–ˆâ–Š       | 28/100 [04:12<10:44,  8.95s/it]Generating...:  29%|â–ˆâ–ˆâ–‰       | 29/100 [04:21<10:35,  8.96s/it]Generating...:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [04:30<10:21,  8.88s/it]Generating...:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [04:38<10:06,  8.78s/it]Generating...:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [04:47<09:56,  8.77s/it]Generating...:  33%|â–ˆâ–ˆâ–ˆâ–      | 33/100 [04:56<09:48,  8.78s/it]Generating...:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [05:05<09:46,  8.89s/it]Generating...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [05:14<09:35,  8.86s/it]Generating...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [05:23<09:25,  8.83s/it]Generating...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [05:32<09:20,  8.89s/it]Generating...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [05:40<09:08,  8.85s/it]Generating...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [05:50<09:07,  8.97s/it]Generating...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [05:59<09:03,  9.05s/it]Generating...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [06:08<08:48,  8.96s/it]Generating...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [06:17<08:40,  8.98s/it]Generating...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/100 [06:26<08:33,  9.01s/it]Generating...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [06:35<08:24,  9.00s/it]Generating...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [06:44<08:15,  9.01s/it]Generating...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [06:53<08:07,  9.03s/it]Generating...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [07:02<07:57,  9.00s/it]Generating...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [07:10<07:42,  8.90s/it]Generating...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [07:19<07:32,  8.88s/it]Generating...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [07:28<07:22,  8.85s/it]Generating...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [07:37<07:09,  8.77s/it]Generating...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [07:46<07:05,  8.87s/it]Generating...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/100 [07:55<07:00,  8.94s/it]Generating...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [08:04<06:53,  9.00s/it]Generating...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [08:13<06:49,  9.09s/it]Generating...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [08:22<06:38,  9.07s/it]Generating...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [08:31<06:28,  9.05s/it]Generating...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [08:40<06:15,  8.93s/it]Generating...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [08:49<06:07,  8.97s/it]Generating...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [08:58<06:00,  9.00s/it]Generating...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [09:07<05:51,  9.00s/it]Generating...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [09:16<05:39,  8.95s/it]Generating...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 63/100 [09:25<05:28,  8.89s/it]Generating...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [09:33<05:18,  8.85s/it]Generating...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [09:42<05:08,  8.82s/it]Generating...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [09:51<05:02,  8.90s/it]Generating...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [10:00<04:54,  8.92s/it]Generating...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [10:09<04:45,  8.91s/it]Generating...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [10:18<04:33,  8.83s/it]Generating...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [10:27<04:26,  8.87s/it]Generating...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [10:35<04:14,  8.78s/it]Generating...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [10:44<04:04,  8.71s/it]Generating...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 73/100 [10:52<03:54,  8.69s/it]Generating...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [11:01<03:46,  8.72s/it]Generating...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [11:10<03:40,  8.84s/it]Generating...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [11:19<03:32,  8.87s/it]Generating...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [11:28<03:25,  8.93s/it]Generating...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [11:37<03:17,  8.96s/it]Generating...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [11:46<03:06,  8.86s/it]Generating...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [11:55<02:57,  8.89s/it]Generating...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [12:03<02:46,  8.78s/it]Generating...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [12:13<02:40,  8.91s/it]Generating...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83/100 [12:21<02:28,  8.76s/it]Generating...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [12:30<02:19,  8.69s/it]Generating...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [12:38<02:11,  8.74s/it]Generating...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [12:47<02:03,  8.82s/it]Generating...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [12:56<01:55,  8.86s/it]Generating...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [13:06<01:47,  8.95s/it]Generating...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [13:14<01:37,  8.83s/it]Generating...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [13:23<01:28,  8.89s/it]Generating...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [13:32<01:19,  8.84s/it]Generating...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [13:40<01:09,  8.74s/it]Generating...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 93/100 [13:49<01:01,  8.75s/it]Generating...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [13:58<00:52,  8.78s/it]Generating...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [14:07<00:43,  8.73s/it]Generating...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [14:15<00:34,  8.73s/it]Generating...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [14:24<00:25,  8.65s/it]Generating...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [14:32<00:17,  8.65s/it]Generating...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [14:42<00:08,  8.78s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [14:50<00:00,  8.75s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [14:50<00:00,  8.91s/it]
2025-12-22:01:32:10 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-22:01:32:10 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: gsm8k
dream_eval (model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=sparse,max_new_tokens=256,steps=256,temperature=0.8,top_p=0.9,alg=entropy,alg_temp=1.5,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85), gen_kwargs: (None), limit: 100.0, num_fewshot: 0, batch_size: 1
|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|
|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|
|gsm8k|      3|flexible-extract|     0|exact_match|â†‘  | 0.37|Â±  |0.0485|
|     |       |strict-match    |     0|exact_match|â†‘  | 0.11|Â±  |0.0314|

âœ… Completed sparse on gsm8k
â±ï¸  Running time: 16m 24s (984s total)


Progress: [3/6]

========================================
Running: adaptive on gsm8k
========================================
Params: max_new_tokens=256, steps=256, block_size=32, limit=100
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-22:01:32:20 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-22:01:32:20 INFO     [__main__:446] Selected Tasks: ['gsm8k']
2025-12-22:01:32:20 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-22:01:32:20 INFO     [evaluator:240] Initializing dream_eval model, with arguments: {'model_path': '/data/qh_models/Dream-v0-Instruct-7B', 'model_type': 'adaptive',
        'max_new_tokens': 256, 'steps': 256, 'temperature': 0.8, 'top_p': 0.9, 'alg': 'entropy', 'alg_temp': 1.5, 'skip': 0.2, 'select':
        0.3, 'block_size': 32, 'importance_source': 'precomputed', 'min_sparsity': 0.15, 'max_sparsity': 0.85}
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]

Loading Dream model from /data/qh_models/Dream-v0-Instruct-7B...
Using the default sparsity block size: 128
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 97.38it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
2025-12-22:01:33:36 INFO     [evaluator:305] gsm8k: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}
2025-12-22:01:33:36 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k from 5 to 0
2025-12-22:01:33:36 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-12-22:01:33:36 INFO     [api.task:434] Building contexts for gsm8k on rank 0...
âœ“ Loading pre-computed importance scores from: /home/qiheng/Projects/adaptive-dllm/configs/head_importance_dream/head_importance.pt
âœ“ Created adaptive config using PRE-COMPUTED importance scores

================================================================================
ğŸ“Š Dream Adaptive Sparsity Configuration Statistics
================================================================================
Mode: Relative Weights (mean=1.0, multiply by select at inference)
Target select: 0.300 (30.0%)
Layers: 28, KV Heads per layer: 4
Total heads: 112

--------------------------------------------------------------------------------
Per-Layer Statistics:
--------------------------------------------------------------------------------
Layer  0: weight_mean=2.1101 â†’ keep_ratio=0.6330 (63.3%), range=[0.024, 1.000]
Layer  1: weight_mean=0.3457 â†’ keep_ratio=0.1037 (10.4%), range=[0.000, 0.389]
Layer  2: weight_mean=0.4433 â†’ keep_ratio=0.1330 (13.3%), range=[0.000, 1.000]
Layer  3: weight_mean=1.1076 â†’ keep_ratio=0.3323 (33.2%), range=[0.011, 1.000]
Layer  4: weight_mean=1.2691 â†’ keep_ratio=0.3807 (38.1%), range=[0.002, 1.000]
Layer  5: weight_mean=0.6808 â†’ keep_ratio=0.2042 (20.4%), range=[0.000, 1.000]
Layer  6: weight_mean=1.2413 â†’ keep_ratio=0.3724 (37.2%), range=[0.031, 1.000]
Layer  7: weight_mean=0.5171 â†’ keep_ratio=0.1551 (15.5%), range=[0.000, 1.000]
Layer  8: weight_mean=0.9650 â†’ keep_ratio=0.2895 (29.0%), range=[0.000, 1.000]
Layer  9: weight_mean=0.3463 â†’ keep_ratio=0.1039 (10.4%), range=[0.000, 0.568]
Layer 10: weight_mean=0.4940 â†’ keep_ratio=0.1482 (14.8%), range=[0.000, 1.000]
Layer 11: weight_mean=0.5980 â†’ keep_ratio=0.1794 (17.9%), range=[0.026, 0.633]
Layer 12: weight_mean=0.6318 â†’ keep_ratio=0.1896 (19.0%), range=[0.000, 0.934]
Layer 13: weight_mean=0.5337 â†’ keep_ratio=0.1601 (16.0%), range=[0.000, 1.000]
Layer 14: weight_mean=1.0248 â†’ keep_ratio=0.3074 (30.7%), range=[0.000, 1.000]
Layer 15: weight_mean=1.3154 â†’ keep_ratio=0.3946 (39.5%), range=[0.019, 1.000]
Layer 16: weight_mean=0.7365 â†’ keep_ratio=0.2210 (22.1%), range=[0.000, 1.000]
Layer 17: weight_mean=1.4104 â†’ keep_ratio=0.4231 (42.3%), range=[0.000, 1.000]
Layer 18: weight_mean=0.8546 â†’ keep_ratio=0.2564 (25.6%), range=[0.000, 1.000]
Layer 19: weight_mean=1.7783 â†’ keep_ratio=0.5335 (53.3%), range=[0.000, 1.000]
Layer 20: weight_mean=0.7570 â†’ keep_ratio=0.2271 (22.7%), range=[0.000, 1.000]
Layer 21: weight_mean=0.8359 â†’ keep_ratio=0.2508 (25.1%), range=[0.004, 1.000]
Layer 22: weight_mean=0.7756 â†’ keep_ratio=0.2327 (23.3%), range=[0.000, 1.000]
Layer 23: weight_mean=1.1250 â†’ keep_ratio=0.3375 (33.7%), range=[0.000, 1.000]
Layer 24: weight_mean=1.2877 â†’ keep_ratio=0.3863 (38.6%), range=[0.021, 1.000]
Layer 25: weight_mean=2.0572 â†’ keep_ratio=0.6172 (61.7%), range=[0.000, 1.000]
Layer 26: weight_mean=1.6868 â†’ keep_ratio=0.5060 (50.6%), range=[0.016, 1.000]
Layer 27: weight_mean=1.0709 â†’ keep_ratio=0.3213 (32.1%), range=[0.000, 1.000]
select: 0.3

--------------------------------------------------------------------------------
Global Statistics:
--------------------------------------------------------------------------------
Relative weights:
  Mean:   0.2680 (should be â‰ˆ1.0)
  Std:    0.3087
  Range:  [0.000, 1.000]

Actual keep_ratios (weights Ã— select=0.3):
  Mean:   0.2680 (26.8%)
  Target: 0.3000 (30.0%)
  Deviation: 0.0320 (3.20%)
  Range:  [0.000, 1.000]
  Heads hitting upper limit (>1.0): 0/112 (0.0%)
  âš ï¸  Mean has slight deviation (1-5%)

--------------------------------------------------------------------------------
Layer-wise Variation:
--------------------------------------------------------------------------------
Layer means range: [0.3457, 2.1101]
Layer means std:   0.4884
Actual keep_ratio range across layers: [0.104, 0.633]
Variation: 52.9% spread
================================================================================


!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
ğŸ“Œ é‡è¦è¯´æ˜ï¼šAdaptive Sparsity å·¥ä½œåŸç†
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
adaptive_config ä¸­å­˜å‚¨çš„æ˜¯ç›¸å¯¹æƒé‡ (relative weights)ï¼Œå…¨å±€å¹³å‡=1.0
åœ¨æ¨ç†æ—¶ï¼Œå®é™…çš„ keep_ratio è®¡ç®—æ–¹å¼ä¸ºï¼š
  keep_ratio = relative_weight Ã— select = relative_weight Ã— 0.3

ä¾‹å¦‚ï¼š
  å¦‚æœæŸä¸ªheadçš„ relative_weight = 1.2
  é‚£ä¹ˆå®é™… keep_ratio = 1.2 Ã— 0.3 = 0.360 (36.0%)
  å¦‚æœæŸä¸ªheadçš„ relative_weight = 0.8
  é‚£ä¹ˆå®é™… keep_ratio = 0.8 Ã— 0.3 = 0.240 (24.0%)

æ‰€æœ‰headsçš„å®é™…keep_ratioçš„å¹³å‡å€¼ = 0.300 (30.0%)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


======================================================================
Dream Evaluation Setup
======================================================================
Model type: adaptive
Model path: /data/qh_models/Dream-v0-Instruct-7B
Steps: 256, Max new tokens: 256
Temperature: 0.8, Top-p: 0.9, Top-k: None
Sparse params: skip=0.2, select=0.3, block_size=32
Adaptive params: importance_source=precomputed, min=0.15, max=0.85
======================================================================

  0%|          | 0/100 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2254.15it/s]
2025-12-22:01:33:36 INFO     [evaluator:574] Running generate_until requests
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1053.68 examples/s]
Generating...:   0%|          | 0/100 [00:00<?, ?it/s]Generating...:   1%|          | 1/100 [00:13<21:42, 13.16s/it]Generating...:   2%|â–         | 2/100 [00:26<21:11, 12.97s/it]Generating...:   3%|â–         | 3/100 [00:37<19:47, 12.24s/it]Generating...:   4%|â–         | 4/100 [00:47<18:28, 11.55s/it]Generating...:   5%|â–Œ         | 5/100 [00:59<18:24, 11.63s/it]Generating...:   6%|â–Œ         | 6/100 [01:10<17:56, 11.45s/it]Generating...:   7%|â–‹         | 7/100 [01:21<17:33, 11.32s/it]Generating...:   8%|â–Š         | 8/100 [01:33<17:19, 11.30s/it]Generating...:   9%|â–‰         | 9/100 [01:44<17:19, 11.42s/it]Generating...:  10%|â–ˆ         | 10/100 [01:56<17:04, 11.39s/it]Generating...:  11%|â–ˆ         | 11/100 [02:07<16:46, 11.31s/it]Generating...:  12%|â–ˆâ–        | 12/100 [02:17<16:17, 11.10s/it]Generating...:  13%|â–ˆâ–        | 13/100 [02:29<16:22, 11.29s/it]Generating...:  14%|â–ˆâ–        | 14/100 [02:40<15:58, 11.15s/it]Generating...:  15%|â–ˆâ–Œ        | 15/100 [02:51<15:51, 11.19s/it]Generating...:  16%|â–ˆâ–Œ        | 16/100 [03:03<15:58, 11.41s/it]Generating...:  17%|â–ˆâ–‹        | 17/100 [03:14<15:40, 11.33s/it]Generating...:  18%|â–ˆâ–Š        | 18/100 [03:25<15:15, 11.17s/it]Generating...:  19%|â–ˆâ–‰        | 19/100 [03:36<14:52, 11.02s/it]Generating...:  20%|â–ˆâ–ˆ        | 20/100 [03:47<14:47, 11.10s/it]Generating...:  21%|â–ˆâ–ˆ        | 21/100 [03:58<14:40, 11.14s/it]Generating...:  22%|â–ˆâ–ˆâ–       | 22/100 [04:10<14:34, 11.21s/it]Generating...:  23%|â–ˆâ–ˆâ–       | 23/100 [04:21<14:23, 11.22s/it]Generating...:  24%|â–ˆâ–ˆâ–       | 24/100 [04:31<13:56, 11.01s/it]Generating...:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [04:42<13:36, 10.88s/it]Generating...:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [04:53<13:25, 10.88s/it]Generating...:  27%|â–ˆâ–ˆâ–‹       | 27/100 [05:04<13:24, 11.01s/it]Generating...:  28%|â–ˆâ–ˆâ–Š       | 28/100 [05:15<13:16, 11.06s/it]Generating...:  29%|â–ˆâ–ˆâ–‰       | 29/100 [05:27<13:10, 11.13s/it]Generating...:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [05:38<13:02, 11.17s/it]Generating...:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [05:48<12:30, 10.88s/it]Generating...:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [05:59<12:19, 10.87s/it]Generating...:  33%|â–ˆâ–ˆâ–ˆâ–      | 33/100 [06:09<11:59, 10.74s/it]Generating...:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [06:20<11:52, 10.79s/it]Generating...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [06:31<11:32, 10.66s/it]Generating...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [06:41<11:26, 10.73s/it]Generating...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [06:53<11:30, 10.97s/it]Generating...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [07:04<11:17, 10.93s/it]Generating...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [07:15<11:11, 11.00s/it]Generating...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [07:26<11:03, 11.06s/it]Generating...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [07:37<10:49, 11.01s/it]Generating...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [07:49<10:55, 11.31s/it]Generating...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/100 [08:01<10:49, 11.40s/it]Generating...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [08:13<10:45, 11.53s/it]Generating...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [08:24<10:34, 11.53s/it]Generating...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [08:36<10:24, 11.57s/it]Generating...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [08:47<10:11, 11.55s/it]Generating...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [08:58<09:49, 11.33s/it]Generating...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [09:09<09:29, 11.16s/it]Generating...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [09:20<09:11, 11.04s/it]Generating...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [09:30<08:52, 10.87s/it]Generating...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [09:41<08:47, 10.99s/it]Generating...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/100 [09:53<08:42, 11.12s/it]Generating...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [10:05<08:41, 11.34s/it]Generating...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [10:16<08:33, 11.42s/it]Generating...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [10:27<08:18, 11.34s/it]Generating...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [10:38<08:01, 11.19s/it]Generating...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [10:49<07:49, 11.19s/it]Generating...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [11:01<07:48, 11.42s/it]Generating...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [11:12<07:27, 11.19s/it]Generating...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [11:23<07:10, 11.05s/it]Generating...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [11:34<06:57, 11.00s/it]Generating...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 63/100 [11:45<06:51, 11.11s/it]Generating...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [11:56<06:36, 11.01s/it]Generating...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [12:07<06:27, 11.07s/it]Generating...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [12:18<06:19, 11.15s/it]Generating...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [12:29<06:08, 11.17s/it]Generating...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [12:41<05:56, 11.15s/it]Generating...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [12:51<05:41, 11.02s/it]Generating...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [13:02<05:31, 11.05s/it]Generating...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [13:13<05:18, 10.99s/it][Layer 0] Step 0 RESET: q_len=343
[Layer 0] Step 52 BUILT MASK: q_len=343, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=305
[Layer 0] Step 52 BUILT MASK: q_len=305, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=336
[Layer 0] Step 52 BUILT MASK: q_len=336, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=314
[Layer 0] Step 52 BUILT MASK: q_len=314, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=390
[Layer 0] Step 52 BUILT MASK: q_len=390, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=335
[Layer 0] Step 52 BUILT MASK: q_len=335, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=321
[Layer 0] Step 52 BUILT MASK: q_len=321, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=348
[Layer 0] Step 52 BUILT MASK: q_len=348, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=383
[Layer 0] Step 52 BUILT MASK: q_len=383, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=339
[Layer 0] Step 52 BUILT MASK: q_len=339, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=339
[Layer 0] Step 52 BUILT MASK: q_len=339, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=343
[Layer 0] Step 52 BUILT MASK: q_len=343, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=346
[Layer 0] Step 52 BUILT MASK: q_len=346, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=336
[Layer 0] Step 52 BUILT MASK: q_len=336, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=331
[Layer 0] Step 52 BUILT MASK: q_len=331, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=373
[Layer 0] Step 52 BUILT MASK: q_len=373, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=332
[Layer 0] Step 52 BUILT MASK: q_len=332, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=336
[Layer 0] Step 52 BUILT MASK: q_len=336, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=307
[Layer 0] Step 52 BUILT MASK: q_len=307, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=344
[Layer 0] Step 52 BUILT MASK: q_len=344, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=342
[Layer 0] Step 52 BUILT MASK: q_len=342, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=324
[Layer 0] Step 52 BUILT MASK: q_len=324, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=334
[Layer 0] Step 52 BUILT MASK: q_len=334, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=319
[Layer 0] Step 52 BUILT MASK: q_len=319, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=318
[Layer 0] Step 52 BUILT MASK: q_len=318, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=346
[Layer 0] Step 52 BUILT MASK: q_len=346, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=347
[Layer 0] Step 52 BUILT MASK: q_len=347, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=337
[Layer 0] Step 52 BUILT MASK: q_len=337, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=329
[Layer 0] Step 52 BUILT MASK: q_len=329, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=346
[Layer 0] Step 52 BUILT MASK: q_len=346, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=318
[Layer 0] Step 52 BUILT MASK: q_len=318, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=342
[Layer 0] Step 52 BUILT MASK: q_len=342, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=320
[Layer 0] Step 52 BUILT MASK: q_len=320, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=309
[Layer 0] Step 52 BUILT MASK: q_len=309, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=320
[Layer 0] Step 52 BUILT MASK: q_len=320, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=331
[Layer 0] Step 52 BUILT MASK: q_len=331, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=322
[Layer 0] Step 52 BUILT MASK: q_len=322, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=342
[Layer 0] Step 52 BUILT MASK: q_len=342, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=323
[Layer 0] Step 52 BUILT MASK: q_len=323, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=351
[Layer 0] Step 52 BUILT MASK: q_len=351, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=323
[Layer 0] Step 52 BUILT MASK: q_len=323, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=407
[Layer 0] Step 52 BUILT MASK: q_len=407, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=360
[Layer 0] Step 52 BUILT MASK: q_len=360, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=340
[Layer 0] Step 52 BUILT MASK: q_len=340, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=356
[Layer 0] Step 52 BUILT MASK: q_len=356, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=371
[Layer 0] Step 52 BUILT MASK: q_len=371, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=372
[Layer 0] Step 52 BUILT MASK: q_len=372, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=329
[Layer 0] Step 52 BUILT MASK: q_len=329, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=315
[Layer 0] Step 52 BUILT MASK: q_len=315, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=322
[Layer 0] Step 52 BUILT MASK: q_len=322, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=315
[Layer 0] Step 52 BUILT MASK: q_len=315, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=328
[Layer 0] Step 52 BUILT MASK: q_len=328, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=341
[Layer 0] Step 52 BUILT MASK: q_len=341, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=379
[Layer 0] Step 52 BUILT MASK: q_len=379, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=361
[Layer 0] Step 52 BUILT MASK: q_len=361, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=329
[Layer 0] Step 52 BUILT MASK: q_len=329, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=324
[Layer 0] Step 52 BUILT MASK: q_len=324, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=356
[Layer 0] Step 52 BUILT MASK: q_len=356, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=363
[Layer 0] Step 52 BUILT MASK: q_len=363, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=311
[Layer 0] Step 52 BUILT MASK: q_len=311, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=317
[Layer 0] Step 52 BUILT MASK: q_len=317, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=340
[Layer 0] Step 52 BUILT MASK: q_len=340, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=347
[Layer 0] Step 52 BUILT MASK: q_len=347, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=348
[Layer 0] Step 52 BUILT MASK: q_len=348, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=371
[Layer 0] Step 52 BUILT MASK: q_len=371, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=326
[Layer 0] Step 52 BUILT MASK: q_len=326, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=337
[Layer 0] Step 52 BUILT MASK: q_len=337, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=333
[Layer 0] Step 52 BUILT MASK: q_len=333, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=316
[Layer 0] Step 52 BUILT MASK: q_len=316, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=330
[Layer 0] Step 52 BUILT MASK: q_len=330, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=331
[Layer 0] Step 52 BUILT MASK: q_len=331, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=323
Generating...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [13:24<05:05, 10.93s/it]Generating...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 73/100 [13:35<04:53, 10.86s/it]Generating...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [13:46<04:41, 10.84s/it]Generating...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [13:57<04:38, 11.14s/it]Generating...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [14:09<04:30, 11.27s/it]Generating...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [14:21<04:21, 11.35s/it]Generating...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [14:31<04:05, 11.18s/it]Generating...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [14:42<03:52, 11.06s/it]Generating...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [14:53<03:43, 11.16s/it]Generating...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [15:04<03:26, 10.86s/it]Generating...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [15:15<03:16, 10.92s/it]Generating...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83/100 [15:25<03:03, 10.82s/it]Generating...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [15:36<02:52, 10.81s/it]Generating...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [15:47<02:41, 10.75s/it]Generating...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [15:58<02:33, 10.97s/it]Generating...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [16:10<02:24, 11.14s/it]Generating...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [16:21<02:15, 11.25s/it]Generating...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [16:31<02:00, 10.94s/it]Generating...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [16:42<01:49, 10.90s/it]Generating...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [16:54<01:39, 11.04s/it]Generating...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [17:04<01:26, 10.77s/it]Generating...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 93/100 [17:15<01:15, 10.79s/it]Generating...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [17:26<01:05, 10.94s/it]Generating...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [17:37<00:54, 10.96s/it]Generating...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [17:48<00:43, 10.96s/it]Generating...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [17:58<00:32, 10.73s/it]Generating...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [18:09<00:21, 10.85s/it]Generating...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [18:21<00:11, 11.07s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [18:32<00:00, 11.11s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [18:32<00:00, 11.12s/it]
2025-12-22:01:53:20 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-22:01:53:20 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: gsm8k
[Layer 0] Step 52 BUILT MASK: q_len=323, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=328
[Layer 0] Step 52 BUILT MASK: q_len=328, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=323
[Layer 0] Step 52 BUILT MASK: q_len=323, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=405
[Layer 0] Step 52 BUILT MASK: q_len=405, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=345
[Layer 0] Step 52 BUILT MASK: q_len=345, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=365
[Layer 0] Step 52 BUILT MASK: q_len=365, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=324
[Layer 0] Step 52 BUILT MASK: q_len=324, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=321
[Layer 0] Step 52 BUILT MASK: q_len=321, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=327
[Layer 0] Step 52 BUILT MASK: q_len=327, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=314
[Layer 0] Step 52 BUILT MASK: q_len=314, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=338
[Layer 0] Step 52 BUILT MASK: q_len=338, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=309
[Layer 0] Step 52 BUILT MASK: q_len=309, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=311
[Layer 0] Step 52 BUILT MASK: q_len=311, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=303
[Layer 0] Step 52 BUILT MASK: q_len=303, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=362
[Layer 0] Step 52 BUILT MASK: q_len=362, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=367
[Layer 0] Step 52 BUILT MASK: q_len=367, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=358
[Layer 0] Step 52 BUILT MASK: q_len=358, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=317
[Layer 0] Step 52 BUILT MASK: q_len=317, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=328
[Layer 0] Step 52 BUILT MASK: q_len=328, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=365
[Layer 0] Step 52 BUILT MASK: q_len=365, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=315
[Layer 0] Step 52 BUILT MASK: q_len=315, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=330
[Layer 0] Step 52 BUILT MASK: q_len=330, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=363
[Layer 0] Step 52 BUILT MASK: q_len=363, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=330
[Layer 0] Step 52 BUILT MASK: q_len=330, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=321
[Layer 0] Step 52 BUILT MASK: q_len=321, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=311
[Layer 0] Step 52 BUILT MASK: q_len=311, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=333
[Layer 0] Step 52 BUILT MASK: q_len=333, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=375
[Layer 0] Step 52 BUILT MASK: q_len=375, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=361
[Layer 0] Step 52 BUILT MASK: q_len=361, mask_shape=torch.Size([1, 28, 12, 12])
dream_eval (model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=adaptive,max_new_tokens=256,steps=256,temperature=0.8,top_p=0.9,alg=entropy,alg_temp=1.5,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85), gen_kwargs: (None), limit: 100.0, num_fewshot: 0, batch_size: 1
|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|
|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|
|gsm8k|      3|flexible-extract|     0|exact_match|â†‘  |  0.1|Â±  |0.0302|
|     |       |strict-match    |     0|exact_match|â†‘  |  0.0|Â±  |0.0000|

âœ… Completed adaptive on gsm8k
â±ï¸  Running time: 21m 11s (1271s total)


================================================
ğŸ“Š Task: HUMANEVAL
================================================

Progress: [4/6]

========================================
Running: standard on humaneval
========================================
Params: max_new_tokens=256, steps=256, block_size=32, limit=100
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-22:01:53:32 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-22:01:53:32 INFO     [__main__:446] Selected Tasks: ['humaneval']
2025-12-22:01:53:32 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-22:01:53:32 INFO     [evaluator:240] Initializing dream_eval model, with arguments: {'model_path': '/data/qh_models/Dream-v0-Instruct-7B', 'model_type': 'standard',
        'max_new_tokens': 256, 'steps': 256, 'temperature': 0.8, 'top_p': 0.9, 'alg': 'entropy', 'alg_temp': 1.5, 'skip': 0.2, 'select':
        0.3, 'block_size': 32, 'importance_source': 'precomputed', 'min_sparsity': 0.15, 'max_sparsity': 0.85}

Loading Dream model from /data/qh_models/Dream-v0-Instruct-7B...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 108.15it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

======================================================================
Dream Evaluation Setup
======================================================================
Model type: standard
Model path: /data/qh_models/Dream-v0-Instruct-7B
Steps: 256, Max new tokens: 256
Temperature: 0.8, Top-p: 0.9, Top-k: None
======================================================================

2025-12-22:01:53:42 INFO     [evaluator:305] humaneval: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-12-22:01:53:42 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval in its config. Manual configuration will be ignored.
2025-12-22:01:53:42 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-12-22:01:53:42 INFO     [api.task:434] Building contexts for humaneval on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2509.34it/s]
2025-12-22:01:53:42 INFO     [evaluator:574] Running generate_until requests
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 969.73 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 909.60 examples/s]
Generating...:   0%|          | 0/100 [00:00<?, ?it/s]Generating...:   1%|          | 1/100 [00:07<12:02,  7.30s/it]Generating...:   2%|â–         | 2/100 [00:14<11:49,  7.24s/it]Generating...:   3%|â–         | 3/100 [00:21<11:34,  7.16s/it]Generating...:   4%|â–         | 4/100 [00:28<11:17,  7.06s/it]Generating...:   5%|â–Œ         | 5/100 [00:35<11:06,  7.01s/it]Generating...:   6%|â–Œ         | 6/100 [00:42<11:05,  7.07s/it]Generating...:   7%|â–‹         | 7/100 [00:49<11:02,  7.12s/it]Generating...:   8%|â–Š         | 8/100 [00:56<10:56,  7.14s/it]Generating...:   9%|â–‰         | 9/100 [01:04<10:53,  7.18s/it]Generating...:  10%|â–ˆ         | 10/100 [01:11<10:49,  7.22s/it]Generating...:  11%|â–ˆ         | 11/100 [01:18<10:38,  7.18s/it]Generating...:  12%|â–ˆâ–        | 12/100 [01:25<10:30,  7.17s/it]Generating...:  13%|â–ˆâ–        | 13/100 [01:33<10:25,  7.19s/it]Generating...:  14%|â–ˆâ–        | 14/100 [01:40<10:16,  7.16s/it]Generating...:  15%|â–ˆâ–Œ        | 15/100 [01:47<10:06,  7.13s/it]Generating...:  16%|â–ˆâ–Œ        | 16/100 [01:54<09:57,  7.12s/it]Generating...:  17%|â–ˆâ–‹        | 17/100 [02:01<09:51,  7.12s/it]Generating...:  18%|â–ˆâ–Š        | 18/100 [02:08<09:48,  7.17s/it]Generating...:  19%|â–ˆâ–‰        | 19/100 [02:15<09:41,  7.18s/it]Generating...:  20%|â–ˆâ–ˆ        | 20/100 [02:23<09:37,  7.22s/it]Generating...:  21%|â–ˆâ–ˆ        | 21/100 [02:30<09:30,  7.22s/it]Generating...:  22%|â–ˆâ–ˆâ–       | 22/100 [02:37<09:18,  7.16s/it]Generating...:  23%|â–ˆâ–ˆâ–       | 23/100 [02:44<09:14,  7.20s/it]Generating...:  24%|â–ˆâ–ˆâ–       | 24/100 [02:50<08:41,  6.86s/it]Generating...:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [02:57<08:38,  6.92s/it]Generating...:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [03:04<08:35,  6.96s/it]Generating...:  27%|â–ˆâ–ˆâ–‹       | 27/100 [03:12<08:33,  7.04s/it]Generating...:  28%|â–ˆâ–ˆâ–Š       | 28/100 [03:18<08:05,  6.74s/it]Generating...:  29%|â–ˆâ–ˆâ–‰       | 29/100 [03:25<08:08,  6.87s/it]Generating...:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [03:32<08:09,  7.00s/it]Generating...:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [03:39<08:02,  6.99s/it]Generating...:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [03:46<08:01,  7.08s/it]Generating...:  33%|â–ˆâ–ˆâ–ˆâ–      | 33/100 [03:55<08:28,  7.60s/it]Generating...:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [04:02<08:14,  7.49s/it]Generating...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [04:10<08:01,  7.41s/it]Generating...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [04:17<07:50,  7.35s/it]Generating...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [04:24<07:40,  7.31s/it]Generating...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [04:31<07:28,  7.23s/it]Generating...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [04:38<07:21,  7.23s/it]Generating...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [04:46<07:15,  7.25s/it]Generating...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [04:53<07:07,  7.25s/it]Generating...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [05:00<06:59,  7.24s/it]Generating...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/100 [05:08<06:55,  7.29s/it]Generating...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [05:15<06:47,  7.28s/it]Generating...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [05:22<06:40,  7.28s/it]Generating...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [05:28<06:12,  6.91s/it]Generating...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [05:35<06:12,  7.03s/it]Generating...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [05:43<06:08,  7.08s/it]Generating...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [05:50<06:02,  7.11s/it]Generating...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [05:57<05:54,  7.09s/it]Generating...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [06:04<05:50,  7.15s/it]Generating...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [06:11<05:41,  7.12s/it]Generating...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/100 [06:18<05:36,  7.16s/it]Generating...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [06:26<05:28,  7.14s/it]Generating...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [06:33<05:20,  7.13s/it]Generating...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [06:40<05:13,  7.12s/it]Generating...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [06:47<05:07,  7.16s/it]Generating...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [06:54<05:01,  7.18s/it]Generating...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [07:01<04:52,  7.14s/it]Generating...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [07:08<04:45,  7.14s/it]Generating...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [07:16<04:40,  7.20s/it]Generating...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [07:23<04:34,  7.23s/it]Generating...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 63/100 [07:30<04:28,  7.26s/it]Generating...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [07:38<04:21,  7.27s/it]Generating...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [07:45<04:13,  7.24s/it]Generating...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [07:52<04:06,  7.25s/it]Generating...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [07:59<03:57,  7.18s/it]Generating...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [08:07<03:53,  7.30s/it]Generating...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [08:16<04:07,  7.98s/it]Generating...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [08:24<03:53,  7.77s/it]Generating...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [08:31<03:39,  7.56s/it]Generating...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [08:38<03:27,  7.41s/it]Generating...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 73/100 [08:45<03:21,  7.46s/it]Generating...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [08:53<03:12,  7.39s/it]Generating...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [09:00<03:04,  7.39s/it]Generating...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [09:07<02:55,  7.33s/it]Generating...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [09:14<02:48,  7.31s/it]Generating...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [09:22<02:40,  7.32s/it]Generating...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [09:31<02:47,  7.99s/it]Generating...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [09:39<02:35,  7.78s/it]Generating...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [09:46<02:23,  7.56s/it]Generating...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [09:54<02:22,  7.94s/it]Generating...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83/100 [10:02<02:11,  7.72s/it]Generating...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [10:08<01:55,  7.21s/it]Generating...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [10:15<01:47,  7.19s/it]Generating...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [10:22<01:40,  7.15s/it]Generating...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [10:29<01:32,  7.12s/it]Generating...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [10:38<01:31,  7.61s/it]Generating...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [10:45<01:22,  7.54s/it]Generating...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [10:52<01:14,  7.47s/it]Generating...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [10:59<01:06,  7.36s/it]Generating...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [11:07<00:58,  7.34s/it]Generating...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 93/100 [11:14<00:50,  7.26s/it]Generating...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [11:21<00:43,  7.28s/it]Generating...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [11:31<00:39,  7.98s/it]Generating...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [11:38<00:31,  7.78s/it]Generating...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [11:45<00:22,  7.62s/it]Generating...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [11:53<00:15,  7.54s/it]Generating...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [12:00<00:07,  7.42s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [12:07<00:00,  7.37s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [12:07<00:00,  7.28s/it]
2025-12-22:02:07:46 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-22:02:07:46 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: humaneval
dream_eval (model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=standard,max_new_tokens=256,steps=256,temperature=0.8,top_p=0.9,alg=entropy,alg_temp=1.5,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85), gen_kwargs: (None), limit: 100.0, num_fewshot: 0, batch_size: 1
|  Tasks  |Version|  Filter   |n-shot|Metric|   |Value|   |Stderr|
|---------|------:|-----------|-----:|------|---|----:|---|-----:|
|humaneval|      1|create_test|     0|pass@1|   | 0.21|Â±  |0.0409|

âœ… Completed standard on humaneval
â±ï¸  Running time: 14m 25s (865s total)


Progress: [5/6]

========================================
Running: sparse on humaneval
========================================
Params: max_new_tokens=256, steps=256, block_size=32, limit=100
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-22:02:07:57 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-22:02:07:57 INFO     [__main__:446] Selected Tasks: ['humaneval']
2025-12-22:02:07:57 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-22:02:07:57 INFO     [evaluator:240] Initializing dream_eval model, with arguments: {'model_path': '/data/qh_models/Dream-v0-Instruct-7B', 'model_type': 'sparse',
        'max_new_tokens': 256, 'steps': 256, 'temperature': 0.8, 'top_p': 0.9, 'alg': 'entropy', 'alg_temp': 1.5, 'skip': 0.2, 'select':
        0.3, 'block_size': 32, 'importance_source': 'precomputed', 'min_sparsity': 0.15, 'max_sparsity': 0.85}
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]

Loading Dream model from /data/qh_models/Dream-v0-Instruct-7B...
Using the default sparsity block size: 128
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 108.34it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

======================================================================
Dream Evaluation Setup
======================================================================
Model type: sparse
Model path: /data/qh_models/Dream-v0-Instruct-7B
Steps: 256, Max new tokens: 256
Temperature: 0.8, Top-p: 0.9, Top-k: None
Sparse params: skip=0.2, select=0.3, block_size=32
======================================================================

2025-12-22:02:08:09 INFO     [evaluator:305] humaneval: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-12-22:02:08:09 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval in its config. Manual configuration will be ignored.
2025-12-22:02:08:09 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-12-22:02:08:09 INFO     [api.task:434] Building contexts for humaneval on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2329.34it/s]
2025-12-22:02:08:09 INFO     [evaluator:574] Running generate_until requests
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 975.60 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 963.54 examples/s]
Generating...:   0%|          | 0/100 [00:00<?, ?it/s]2025-12-22:02:09:43 WARNING  [models.Dream.core.sparsed_modeling_dream:328] The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
Generating...:   1%|          | 1/100 [00:10<17:12, 10.43s/it]Generating...:   2%|â–         | 2/100 [00:21<17:57, 10.99s/it]Generating...:   3%|â–         | 3/100 [00:30<16:25, 10.16s/it]Generating...:   4%|â–         | 4/100 [00:40<15:36,  9.76s/it]Generating...:   5%|â–Œ         | 5/100 [00:49<15:02,  9.50s/it]Generating...:   6%|â–Œ         | 6/100 [00:58<14:42,  9.39s/it]Generating...:   7%|â–‹         | 7/100 [01:07<14:26,  9.32s/it]Generating...:   8%|â–Š         | 8/100 [01:16<14:13,  9.28s/it]Generating...:   9%|â–‰         | 9/100 [01:25<13:56,  9.19s/it]Generating...:  10%|â–ˆ         | 10/100 [01:35<13:51,  9.24s/it]Generating...:  11%|â–ˆ         | 11/100 [01:44<13:38,  9.20s/it]Generating...:  12%|â–ˆâ–        | 12/100 [01:53<13:28,  9.18s/it]Generating...:  13%|â–ˆâ–        | 13/100 [02:02<13:30,  9.32s/it]Generating...:  14%|â–ˆâ–        | 14/100 [02:12<13:29,  9.41s/it]Generating...:  15%|â–ˆâ–Œ        | 15/100 [02:21<13:15,  9.36s/it]Generating...:  16%|â–ˆâ–Œ        | 16/100 [02:31<13:05,  9.35s/it]Generating...:  17%|â–ˆâ–‹        | 17/100 [02:40<12:54,  9.33s/it]Generating...:  18%|â–ˆâ–Š        | 18/100 [02:49<12:48,  9.37s/it]Generating...:  19%|â–ˆâ–‰        | 19/100 [02:59<12:46,  9.46s/it]Generating...:  20%|â–ˆâ–ˆ        | 20/100 [03:08<12:35,  9.45s/it]Generating...:  21%|â–ˆâ–ˆ        | 21/100 [03:18<12:23,  9.41s/it]Generating...:  22%|â–ˆâ–ˆâ–       | 22/100 [03:27<12:09,  9.35s/it]Generating...:  23%|â–ˆâ–ˆâ–       | 23/100 [03:36<11:55,  9.29s/it]Generating...:  24%|â–ˆâ–ˆâ–       | 24/100 [03:45<11:38,  9.19s/it]Generating...:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [03:54<11:30,  9.20s/it]Generating...:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [04:03<11:18,  9.17s/it]Generating...:  27%|â–ˆâ–ˆâ–‹       | 27/100 [04:12<11:07,  9.14s/it]Generating...:  28%|â–ˆâ–ˆâ–Š       | 28/100 [04:22<11:03,  9.21s/it]Generating...:  29%|â–ˆâ–ˆâ–‰       | 29/100 [04:31<10:49,  9.14s/it]Generating...:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [04:40<10:49,  9.28s/it]Generating...:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [04:50<10:37,  9.24s/it]Generating...:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [04:59<10:29,  9.25s/it]Generating...:  33%|â–ˆâ–ˆâ–ˆâ–      | 33/100 [05:09<10:31,  9.43s/it]Generating...:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [05:18<10:17,  9.36s/it]Generating...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [05:27<10:05,  9.32s/it]Generating...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [05:36<09:52,  9.26s/it]Generating...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [05:45<09:39,  9.20s/it]Generating...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [05:54<09:29,  9.18s/it]Generating...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [06:04<09:25,  9.27s/it]Generating...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [06:14<09:22,  9.37s/it]Generating...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [06:23<09:10,  9.33s/it]Generating...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [06:32<09:03,  9.36s/it]Generating...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/100 [06:42<08:52,  9.34s/it]Generating...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [06:51<08:48,  9.43s/it]Generating...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [07:00<08:36,  9.39s/it]Generating...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [07:10<08:21,  9.29s/it]Generating...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [07:19<08:08,  9.21s/it]Generating...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [07:28<07:58,  9.20s/it]Generating...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [07:37<07:54,  9.30s/it]Generating...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [07:47<07:53,  9.46s/it]Generating...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [07:56<07:40,  9.40s/it]Generating...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [08:05<07:27,  9.32s/it]Generating...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/100 [08:14<07:12,  9.20s/it]Generating...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [08:24<07:01,  9.17s/it]Generating...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [08:33<06:57,  9.27s/it]Generating...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [08:43<06:51,  9.35s/it]Generating...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [08:52<06:37,  9.24s/it]Generating...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [09:01<06:25,  9.17s/it]Generating...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [09:09<06:13,  9.11s/it]Generating...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [09:19<06:06,  9.16s/it]Generating...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [09:28<06:02,  9.29s/it]Generating...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [09:38<05:51,  9.25s/it]Generating...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 63/100 [09:47<05:41,  9.24s/it]Generating...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [09:56<05:29,  9.16s/it]Generating...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [10:05<05:20,  9.16s/it]Generating...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [10:14<05:14,  9.26s/it]Generating...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [10:24<05:04,  9.23s/it]Generating...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [10:33<04:55,  9.23s/it]Generating...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [10:43<04:58,  9.62s/it]Generating...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [10:53<04:46,  9.55s/it]Generating...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [11:01<04:24,  9.12s/it]Generating...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [11:10<04:14,  9.09s/it]Generating...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 73/100 [11:20<04:12,  9.33s/it]Generating...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [11:29<03:59,  9.23s/it]Generating...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [11:38<03:52,  9.30s/it]Generating...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [11:47<03:42,  9.28s/it]Generating...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [11:56<03:31,  9.18s/it]Generating...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [12:05<03:20,  9.13s/it]Generating...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [12:16<03:18,  9.46s/it]Generating...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [12:25<03:07,  9.40s/it]Generating...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [12:34<02:59,  9.47s/it]Generating...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [12:44<02:53,  9.62s/it]Generating...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83/100 [12:53<02:40,  9.41s/it]Generating...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [13:02<02:28,  9.30s/it]Generating...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [13:11<02:17,  9.18s/it]Generating...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [13:20<02:07,  9.11s/it]Generating...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [13:29<01:58,  9.12s/it]Generating...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [13:39<01:51,  9.30s/it]Generating...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [13:48<01:41,  9.27s/it]Generating...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [13:57<01:31,  9.12s/it]Generating...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [14:05<01:19,  8.79s/it]Generating...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [14:14<01:11,  8.88s/it]Generating...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 93/100 [14:24<01:03,  9.01s/it]Generating...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [14:33<00:54,  9.06s/it]Generating...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [14:43<00:47,  9.52s/it]Generating...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [14:52<00:37,  9.43s/it]Generating...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [15:02<00:28,  9.37s/it]Generating...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [15:11<00:18,  9.33s/it]Generating...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [15:20<00:09,  9.24s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [15:29<00:00,  9.23s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [15:29<00:00,  9.30s/it]
2025-12-22:02:25:37 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-22:02:25:37 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: humaneval
dream_eval (model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=sparse,max_new_tokens=256,steps=256,temperature=0.8,top_p=0.9,alg=entropy,alg_temp=1.5,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85), gen_kwargs: (None), limit: 100.0, num_fewshot: 0, batch_size: 1
|  Tasks  |Version|  Filter   |n-shot|Metric|   |Value|   |Stderr|
|---------|------:|-----------|-----:|------|---|----:|---|-----:|
|humaneval|      1|create_test|     0|pass@1|   |  0.2|Â±  |0.0402|

âœ… Completed sparse on humaneval
â±ï¸  Running time: 17m 52s (1072s total)


Progress: [6/6]

========================================
Running: adaptive on humaneval
========================================
Params: max_new_tokens=256, steps=256, block_size=32, limit=100
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-22:02:25:48 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-22:02:25:48 INFO     [__main__:446] Selected Tasks: ['humaneval']
2025-12-22:02:25:48 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-22:02:25:48 INFO     [evaluator:240] Initializing dream_eval model, with arguments: {'model_path': '/data/qh_models/Dream-v0-Instruct-7B', 'model_type': 'adaptive',
        'max_new_tokens': 256, 'steps': 256, 'temperature': 0.8, 'top_p': 0.9, 'alg': 'entropy', 'alg_temp': 1.5, 'skip': 0.2, 'select':
        0.3, 'block_size': 32, 'importance_source': 'precomputed', 'min_sparsity': 0.15, 'max_sparsity': 0.85}
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]

Loading Dream model from /data/qh_models/Dream-v0-Instruct-7B...
Using the default sparsity block size: 128
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 112.55it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
âœ“ Loading pre-computed importance scores from: /home/qiheng/Projects/adaptive-dllm/configs/head_importance_dream/head_importance.pt
âœ“ Created adaptive config using PRE-COMPUTED importance scores

================================================================================
ğŸ“Š Dream Adaptive Sparsity Configuration Statistics
================================================================================
Mode: Relative Weights (mean=1.0, multiply by select at inference)
Target select: 0.300 (30.0%)
Layers: 28, KV Heads per layer: 4
Total heads: 112

--------------------------------------------------------------------------------
Per-Layer Statistics:
--------------------------------------------------------------------------------
Layer  0: weight_mean=2.1101 â†’ keep_ratio=0.6330 (63.3%), range=[0.024, 1.000]
Layer  1: weight_mean=0.3457 â†’ keep_ratio=0.1037 (10.4%), range=[0.000, 0.389]
Layer  2: weight_mean=0.4433 â†’ keep_ratio=0.1330 (13.3%), range=[0.000, 1.000]
Layer  3: weight_mean=1.1076 â†’ keep_ratio=0.3323 (33.2%), range=[0.011, 1.000]
Layer  4: weight_mean=1.2691 â†’ keep_ratio=0.3807 (38.1%), range=[0.002, 1.000]
Layer  5: weight_mean=0.6808 â†’ keep_ratio=0.2042 (20.4%), range=[0.000, 1.000]
Layer  6: weight_mean=1.2413 â†’ keep_ratio=0.3724 (37.2%), range=[0.031, 1.000]
Layer  7: weight_mean=0.5171 â†’ keep_ratio=0.1551 (15.5%), range=[0.000, 1.000]
Layer  8: weight_mean=0.9650 â†’ keep_ratio=0.2895 (29.0%), range=[0.000, 1.000]
Layer  9: weight_mean=0.3463 â†’ keep_ratio=0.1039 (10.4%), range=[0.000, 0.568]
Layer 10: weight_mean=0.4940 â†’ keep_ratio=0.1482 (14.8%), range=[0.000, 1.000]
Layer 11: weight_mean=0.5980 â†’ keep_ratio=0.1794 (17.9%), range=[0.026, 0.633]
Layer 12: weight_mean=0.6318 â†’ keep_ratio=0.1896 (19.0%), range=[0.000, 0.934]
Layer 13: weight_mean=0.5337 â†’ keep_ratio=0.1601 (16.0%), range=[0.000, 1.000]
Layer 14: weight_mean=1.0248 â†’ keep_ratio=0.3074 (30.7%), range=[0.000, 1.000]
Layer 15: weight_mean=1.3154 â†’ keep_ratio=0.3946 (39.5%), range=[0.019, 1.000]
Layer 16: weight_mean=0.7365 â†’ keep_ratio=0.2210 (22.1%), range=[0.000, 1.000]
Layer 17: weight_mean=1.4104 â†’ keep_ratio=0.4231 (42.3%), range=[0.000, 1.000]
Layer 18: weight_mean=0.8546 â†’ keep_ratio=0.2564 (25.6%), range=[0.000, 1.000]
Layer 19: weight_mean=1.7783 â†’ keep_ratio=0.5335 (53.3%), range=[0.000, 1.000]
Layer 20: weight_mean=0.7570 â†’ keep_ratio=0.2271 (22.7%), range=[0.000, 1.000]
Layer 21: weight_mean=0.8359 â†’ keep_ratio=0.2508 (25.1%), range=[0.004, 1.000]
Layer 22: weight_mean=0.7756 â†’ keep_ratio=0.2327 (23.3%), range=[0.000, 1.000]
Layer 23: weight_mean=1.1250 â†’ keep_ratio=0.3375 (33.7%), range=[0.000, 1.000]
Layer 24: weight_mean=1.2877 â†’ keep_ratio=0.3863 (38.6%), range=[0.021, 1.000]
Layer 25: weight_mean=2.0572 â†’ keep_ratio=0.6172 (61.7%), range=[0.000, 1.000]
Layer 26: weight_mean=1.6868 â†’ keep_ratio=0.5060 (50.6%), range=[0.016, 1.000]
Layer 27: weight_mean=1.0709 â†’ keep_ratio=0.3213 (32.1%), range=[0.000, 1.000]
select: 0.3

--------------------------------------------------------------------------------
Global Statistics:
--------------------------------------------------------------------------------
Relative weights:
  Mean:   0.2680 (should be â‰ˆ1.0)
  Std:    0.3087
  Range:  [0.000, 1.000]

Actual keep_ratios (weights Ã— select=0.3):
  Mean:   0.2680 (26.8%)
  Target: 0.3000 (30.0%)
  Deviation: 0.0320 (3.20%)
  Range:  [0.000, 1.000]
  Heads hitting upper limit (>1.0): 0/112 (0.0%)
  âš ï¸  Mean has slight deviation (1-5%)

--------------------------------------------------------------------------------
Layer-wise Variation:
--------------------------------------------------------------------------------
Layer means range: [0.3457, 2.1101]
Layer means std:   0.4884
Actual keep_ratio range across layers: [0.104, 0.633]
Variation: 52.9% spread
================================================================================


!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
ğŸ“Œ é‡è¦è¯´æ˜ï¼šAdaptive Sparsity å·¥ä½œåŸç†
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
adaptive_config ä¸­å­˜å‚¨çš„æ˜¯ç›¸å¯¹æƒé‡ (relative weights)ï¼Œå…¨å±€å¹³å‡=1.0
åœ¨æ¨ç†æ—¶ï¼Œå®é™…çš„ keep_ratio è®¡ç®—æ–¹å¼ä¸ºï¼š
  keep_ratio = relative_weight Ã— select = relative_weight Ã— 0.3

ä¾‹å¦‚ï¼š
  å¦‚æœæŸä¸ªheadçš„ relative_weight = 1.2
  é‚£ä¹ˆå®é™… keep_ratio = 1.2 Ã— 0.3 = 0.360 (36.0%)
  å¦‚æœæŸä¸ªheadçš„ relative_weight = 0.8
  é‚£ä¹ˆå®é™… keep_ratio = 0.8 Ã— 0.3 = 0.240 (24.0%)

æ‰€æœ‰headsçš„å®é™…keep_ratioçš„å¹³å‡å€¼ = 0.300 (30.0%)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


======================================================================
Dream Evaluation Setup
======================================================================
Model type: adaptive
Model path: /data/qh_models/Dream-v0-Instruct-7B
Steps: 256, Max new tokens: 256
Temperature: 0.8, Top-p: 0.9, Top-k: None
Sparse params: skip=0.2, select=0.3, block_size=32
Adaptive params: importance_source=precomputed, min=0.15, max=0.85
======================================================================

2025-12-22:02:27:01 INFO     [evaluator:305] humaneval: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-12-22:02:27:01 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval in its config. Manual configuration will be ignored.
2025-12-22:02:27:01 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-12-22:02:27:01 INFO     [api.task:434] Building contexts for humaneval on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2051.00it/s]
2025-12-22:02:27:01 INFO     [evaluator:574] Running generate_until requests
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 93/100 [00:00<00:00, 912.67 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 865.47 examples/s]
Generating...:   0%|          | 0/100 [00:00<?, ?it/s]Generating...:   1%|          | 1/100 [00:13<22:32, 13.66s/it]Generating...:   2%|â–         | 2/100 [00:27<22:39, 13.87s/it]Generating...:   3%|â–         | 3/100 [00:39<20:35, 12.74s/it]Generating...:   4%|â–         | 4/100 [00:51<19:57, 12.48s/it]Generating...:   5%|â–Œ         | 5/100 [01:02<19:22, 12.24s/it]Generating...:   6%|â–Œ         | 6/100 [01:14<18:47, 12.00s/it]Generating...:   7%|â–‹         | 7/100 [01:26<18:45, 12.10s/it]Generating...:   8%|â–Š         | 8/100 [01:38<18:14, 11.90s/it]Generating...:   9%|â–‰         | 9/100 [01:49<17:43, 11.69s/it]Generating...:  10%|â–ˆ         | 10/100 [02:01<17:32, 11.69s/it]Generating...:  11%|â–ˆ         | 11/100 [02:13<17:27, 11.77s/it]Generating...:  12%|â–ˆâ–        | 12/100 [02:24<17:08, 11.69s/it]Generating...:  13%|â–ˆâ–        | 13/100 [02:36<17:10, 11.84s/it]Generating...:  14%|â–ˆâ–        | 14/100 [02:48<16:41, 11.64s/it]Generating...:  15%|â–ˆâ–Œ        | 15/100 [02:59<16:18, 11.51s/it]Generating...:  16%|â–ˆâ–Œ        | 16/100 [03:10<16:11, 11.57s/it]Generating...:  17%|â–ˆâ–‹        | 17/100 [03:22<15:55, 11.52s/it]Generating...:  18%|â–ˆâ–Š        | 18/100 [03:34<16:06, 11.78s/it]Generating...:  19%|â–ˆâ–‰        | 19/100 [03:46<15:55, 11.80s/it]Generating...:  20%|â–ˆâ–ˆ        | 20/100 [03:58<15:41, 11.77s/it]Generating...:  21%|â–ˆâ–ˆ        | 21/100 [04:10<15:38, 11.88s/it]Generating...:  22%|â–ˆâ–ˆâ–       | 22/100 [04:22<15:24, 11.85s/it]Generating...:  23%|â–ˆâ–ˆâ–       | 23/100 [04:33<15:10, 11.83s/it]Generating...:  24%|â–ˆâ–ˆâ–       | 24/100 [04:44<14:29, 11.44s/it]Generating...:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [04:55<14:09, 11.32s/it]Generating...:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [05:07<14:07, 11.46s/it]Generating...:  27%|â–ˆâ–ˆâ–‹       | 27/100 [05:18<13:55, 11.45s/it]Generating...:  28%|â–ˆâ–ˆâ–Š       | 28/100 [05:29<13:31, 11.27s/it]Generating...:  29%|â–ˆâ–ˆâ–‰       | 29/100 [05:40<13:19, 11.25s/it]Generating...:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [05:52<13:15, 11.37s/it]Generating...:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [06:04<13:20, 11.59s/it]Generating...:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [06:16<13:07, 11.58s/it]Generating...:  33%|â–ˆâ–ˆâ–ˆâ–      | 33/100 [06:30<13:56, 12.49s/it]Generating...:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [06:42<13:39, 12.41s/it]Generating...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [06:54<13:00, 12.01s/it]Generating...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [07:05<12:30, 11.73s/it]Generating...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [07:16<12:08, 11.56s/it]Generating...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [07:28<12:09, 11.77s/it]Generating...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [07:40<12:05, 11.89s/it]Generating...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [07:52<11:50, 11.84s/it]Generating...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [08:04<11:41, 11.90s/it]Generating...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [08:16<11:31, 11.93s/it]Generating...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/100 [08:27<11:12, 11.79s/it]Generating...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [08:40<11:05, 11.89s/it]Generating...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [08:51<10:47, 11.77s/it]Generating...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [09:02<10:22, 11.52s/it]Generating...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [09:14<10:13, 11.58s/it]Generating...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [09:25<09:54, 11.43s/it]Generating...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [09:36<09:38, 11.34s/it]Generating...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [09:48<09:35, 11.52s/it]Generating...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [10:00<09:30, 11.65s/it]Generating...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [10:12<09:22, 11.71s/it]Generating...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/100 [10:23<09:03, 11.56s/it]Generating...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [10:34<08:44, 11.39s/it]Generating...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [10:46<08:38, 11.51s/it]Generating...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [10:57<08:22, 11.42s/it]Generating...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [11:08<08:07, 11.33s/it]Generating...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [11:19<07:56, 11.34s/it]Generating...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [11:31<07:46, 11.38s/it]Generating...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [11:42<07:33, 11.33s/it]Generating...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [11:54<07:27, 11.48s/it]Generating...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [12:05<07:16, 11.48s/it]Generating...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 63/100 [12:17<07:05, 11.51s/it]Generating...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [12:29<06:58, 11.62s/it]Generating...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [12:41<06:48, 11.67s/it]Generating...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [12:52<06:34, 11.61s/it]Generating...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [13:04<06:25, 11.68s/it]Generating...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [13:17<06:27, 12.12s/it]Generating...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [13:34<07:00, 13.56s/it]Generating...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [13:46<06:36, 13.21s/it]Generating...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [13:58<06:05, 12.60s/it][Layer 0] Step 0 RESET: q_len=393
[Layer 0] Step 52 BUILT MASK: q_len=393, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=384
[Layer 0] Step 52 BUILT MASK: q_len=384, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=354
[Layer 0] Step 52 BUILT MASK: q_len=354, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=388
[Layer 0] Step 52 BUILT MASK: q_len=388, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=386
[Layer 0] Step 52 BUILT MASK: q_len=386, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=361
[Layer 0] Step 52 BUILT MASK: q_len=361, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=381
[Layer 0] Step 52 BUILT MASK: q_len=381, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=362
[Layer 0] Step 52 BUILT MASK: q_len=362, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=381
[Layer 0] Step 52 BUILT MASK: q_len=381, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=369
[Layer 0] Step 52 BUILT MASK: q_len=369, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=411
[Layer 0] Step 52 BUILT MASK: q_len=411, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=351
[Layer 0] Step 52 BUILT MASK: q_len=351, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=371
[Layer 0] Step 52 BUILT MASK: q_len=371, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=341
[Layer 0] Step 52 BUILT MASK: q_len=341, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=328
[Layer 0] Step 52 BUILT MASK: q_len=328, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=337
[Layer 0] Step 52 BUILT MASK: q_len=337, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=339
[Layer 0] Step 52 BUILT MASK: q_len=339, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=441
[Layer 0] Step 52 BUILT MASK: q_len=441, mask_shape=torch.Size([1, 28, 14, 14])
[Layer 0] Step 0 RESET: q_len=357
[Layer 0] Step 52 BUILT MASK: q_len=357, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=374
[Layer 0] Step 52 BUILT MASK: q_len=374, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=435
[Layer 0] Step 52 BUILT MASK: q_len=435, mask_shape=torch.Size([1, 28, 14, 14])
[Layer 0] Step 0 RESET: q_len=394
[Layer 0] Step 52 BUILT MASK: q_len=394, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=361
[Layer 0] Step 52 BUILT MASK: q_len=361, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=312
[Layer 0] Step 52 BUILT MASK: q_len=312, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=322
[Layer 0] Step 52 BUILT MASK: q_len=322, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=397
[Layer 0] Step 52 BUILT MASK: q_len=397, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=353
[Layer 0] Step 52 BUILT MASK: q_len=353, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=317
[Layer 0] Step 52 BUILT MASK: q_len=317, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=326
[Layer 0] Step 52 BUILT MASK: q_len=326, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=356
[Layer 0] Step 52 BUILT MASK: q_len=356, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=387
[Layer 0] Step 52 BUILT MASK: q_len=387, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=375
[Layer 0] Step 52 BUILT MASK: q_len=375, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=530
[Layer 0] Step 52 BUILT MASK: q_len=530, mask_shape=torch.Size([1, 28, 17, 17])
[Layer 0] Step 0 RESET: q_len=419
[Layer 0] Step 52 BUILT MASK: q_len=419, mask_shape=torch.Size([1, 28, 14, 14])
[Layer 0] Step 0 RESET: q_len=346
[Layer 0] Step 52 BUILT MASK: q_len=346, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=356
[Layer 0] Step 52 BUILT MASK: q_len=356, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=353
[Layer 0] Step 52 BUILT MASK: q_len=353, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=392
[Layer 0] Step 52 BUILT MASK: q_len=392, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=427
[Layer 0] Step 52 BUILT MASK: q_len=427, mask_shape=torch.Size([1, 28, 14, 14])
[Layer 0] Step 0 RESET: q_len=368
[Layer 0] Step 52 BUILT MASK: q_len=368, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=428
[Layer 0] Step 52 BUILT MASK: q_len=428, mask_shape=torch.Size([1, 28, 14, 14])
[Layer 0] Step 0 RESET: q_len=420
[Layer 0] Step 52 BUILT MASK: q_len=420, mask_shape=torch.Size([1, 28, 14, 14])
[Layer 0] Step 0 RESET: q_len=382
[Layer 0] Step 52 BUILT MASK: q_len=382, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=426
[Layer 0] Step 52 BUILT MASK: q_len=426, mask_shape=torch.Size([1, 28, 14, 14])
[Layer 0] Step 0 RESET: q_len=366
[Layer 0] Step 52 BUILT MASK: q_len=366, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=315
[Layer 0] Step 52 BUILT MASK: q_len=315, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=441
[Layer 0] Step 52 BUILT MASK: q_len=441, mask_shape=torch.Size([1, 28, 14, 14])
[Layer 0] Step 0 RESET: q_len=350
[Layer 0] Step 52 BUILT MASK: q_len=350, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=340
[Layer 0] Step 52 BUILT MASK: q_len=340, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=385
[Layer 0] Step 52 BUILT MASK: q_len=385, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=360
[Layer 0] Step 52 BUILT MASK: q_len=360, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=391
[Layer 0] Step 52 BUILT MASK: q_len=391, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=353
[Layer 0] Step 52 BUILT MASK: q_len=353, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=323
[Layer 0] Step 52 BUILT MASK: q_len=323, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=409
[Layer 0] Step 52 BUILT MASK: q_len=409, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=324
[Layer 0] Step 52 BUILT MASK: q_len=324, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=361
[Layer 0] Step 52 BUILT MASK: q_len=361, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=360
[Layer 0] Step 52 BUILT MASK: q_len=360, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=392
[Layer 0] Step 52 BUILT MASK: q_len=392, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=338
[Layer 0] Step 52 BUILT MASK: q_len=338, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=373
[Layer 0] Step 52 BUILT MASK: q_len=373, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=358
[Layer 0] Step 52 BUILT MASK: q_len=358, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=379
[Layer 0] Step 52 BUILT MASK: q_len=379, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=419
[Layer 0] Step 52 BUILT MASK: q_len=419, mask_shape=torch.Size([1, 28, 14, 14])
[Layer 0] Step 0 RESET: q_len=395
[Layer 0] Step 52 BUILT MASK: q_len=395, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=355
[Layer 0] Step 52 BUILT MASK: q_len=355, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=389
[Layer 0] Step 52 BUILT MASK: q_len=389, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=495
[Layer 0] Step 52 BUILT MASK: q_len=495, mask_shape=torch.Size([1, 28, 16, 16])
[Layer 0] Step 0 RESET: q_len=584
[Layer 0] Step 52 BUILT MASK: q_len=584, mask_shape=torch.Size([1, 28, 19, 19])
[Layer 0] Step 0 RESET: q_len=434
[Layer 0] Step 52 BUILT MASK: q_len=434, mask_shape=torch.Size([1, 28, 14, 14])
[Layer 0] Step 0 RESET: q_len=393
[Layer 0] Step 52 BUILT MASK: q_len=393, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=392
Generating...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [14:10<05:48, 12.43s/it]Generating...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 73/100 [14:23<05:40, 12.61s/it]Generating...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [14:35<05:25, 12.51s/it]Generating...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [14:48<05:18, 12.73s/it]Generating...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [14:59<04:53, 12.21s/it]Generating...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [15:11<04:37, 12.07s/it]Generating...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [15:22<04:19, 11.78s/it]Generating...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [15:38<04:31, 12.94s/it]Generating...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [15:49<04:09, 12.48s/it]Generating...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [16:01<03:53, 12.29s/it]Generating...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [16:15<03:53, 12.98s/it]Generating...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83/100 [16:27<03:30, 12.41s/it]Generating...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [16:37<03:09, 11.84s/it]Generating...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [16:49<02:56, 11.75s/it]Generating...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [16:59<02:40, 11.43s/it]Generating...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [17:11<02:29, 11.53s/it]Generating...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [17:26<02:30, 12.56s/it]Generating...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [17:39<02:19, 12.65s/it]Generating...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [17:50<02:02, 12.22s/it]Generating...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [18:01<01:47, 11.95s/it]Generating...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [18:13<01:34, 11.82s/it]Generating...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 93/100 [18:25<01:22, 11.82s/it]Generating...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [18:36<01:10, 11.70s/it]Generating...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [18:52<01:04, 12.98s/it]Generating...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [19:04<00:51, 12.77s/it]Generating...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [19:17<00:37, 12.61s/it]Generating...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [19:28<00:24, 12.19s/it]Generating...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [19:39<00:11, 11.78s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [19:51<00:00, 12.01s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [19:51<00:00, 11.92s/it]
[Layer 0] Step 52 BUILT MASK: q_len=392, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=499
[Layer 0] Step 52 BUILT MASK: q_len=499, mask_shape=torch.Size([1, 28, 16, 16])
[Layer 0] Step 0 RESET: q_len=426
[Layer 0] Step 52 BUILT MASK: q_len=426, mask_shape=torch.Size([1, 28, 14, 14])
[Layer 0] Step 0 RESET: q_len=462
[Layer 0] Step 52 BUILT MASK: q_len=462, mask_shape=torch.Size([1, 28, 15, 15])
[Layer 0] Step 0 RESET: q_len=352
[Layer 0] Step 52 BUILT MASK: q_len=352, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=411
[Layer 0] Step 52 BUILT MASK: q_len=411, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=382
[Layer 0] Step 52 BUILT MASK: q_len=382, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=584
[Layer 0] Step 52 BUILT MASK: q_len=584, mask_shape=torch.Size([1, 28, 19, 19])
[Layer 0] Step 0 RESET: q_len=416
[Layer 0] Step 52 BUILT MASK: q_len=416, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=390
[Layer 0] Step 52 BUILT MASK: q_len=390, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=539
[Layer 0] Step 52 BUILT MASK: q_len=539, mask_shape=torch.Size([1, 28, 17, 17])
[Layer 0] Step 0 RESET: q_len=350
[Layer 0] Step 52 BUILT MASK: q_len=350, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=313
[Layer 0] Step 52 BUILT MASK: q_len=313, mask_shape=torch.Size([1, 28, 10, 10])
[Layer 0] Step 0 RESET: q_len=416
[Layer 0] Step 52 BUILT MASK: q_len=416, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=324
[Layer 0] Step 52 BUILT MASK: q_len=324, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=399
[Layer 0] Step 52 BUILT MASK: q_len=399, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=528
[Layer 0] Step 52 BUILT MASK: q_len=528, mask_shape=torch.Size([1, 28, 17, 17])
[Layer 0] Step 0 RESET: q_len=472
[Layer 0] Step 52 BUILT MASK: q_len=472, mask_shape=torch.Size([1, 28, 15, 15])
[Layer 0] Step 0 RESET: q_len=379
[Layer 0] Step 52 BUILT MASK: q_len=379, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=393
[Layer 0] Step 52 BUILT MASK: q_len=393, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=376
[Layer 0] Step 52 BUILT MASK: q_len=376, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=404
[Layer 0] Step 52 BUILT MASK: q_len=404, mask_shape=torch.Size([1, 28, 13, 13])
[Layer 0] Step 0 RESET: q_len=378
[Layer 0] Step 52 BUILT MASK: q_len=378, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=596
[Layer 0] Step 52 BUILT MASK: q_len=596, mask_shape=torch.Size([1, 28, 19, 19])
[Layer 0] Step 0 RESET: q_len=442
[Layer 0] Step 52 BUILT MASK: q_len=442, mask_shape=torch.Size([1, 28, 14, 14])
[Layer 0] Step 0 RESET: q_len=424
[Layer 0] Step 52 BUILT MASK: q_len=424, mask_shape=torch.Size([1, 28, 14, 14])
[Layer 0] Step 0 RESET: q_len=382
[Layer 0] Step 52 BUILT MASK: q_len=382, mask_shape=torch.Size([1, 28, 12, 12])
[Layer 0] Step 0 RESET: q_len=341
[Layer 0] Step 52 BUILT MASK: q_len=341, mask_shape=torch.Size([1, 28, 11, 11])
[Layer 0] Step 0 RESET: q_len=433
[Layer 0] Step 52 BUILT MASK: q_len=433, mask_shape=torch.Size([1, 28, 14, 14])
2025-12-22:02:48:29 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-22:02:48:29 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: humaneval
dream_eval (model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=adaptive,max_new_tokens=256,steps=256,temperature=0.8,top_p=0.9,alg=entropy,alg_temp=1.5,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85), gen_kwargs: (None), limit: 100.0, num_fewshot: 0, batch_size: 1
|  Tasks  |Version|  Filter   |n-shot|Metric|   |Value|   |Stderr|
|---------|------:|-----------|-----:|------|---|----:|---|-----:|
|humaneval|      1|create_test|     0|pass@1|   | 0.04|Â±  |0.0197|

âœ… Completed adaptive on humaneval
â±ï¸  Running time: 22m 52s (1372s total)


================================================
âœ¨ All evaluations completed!
Finished at: Monday, December 22, 2025 AM02:48:32 HKT
================================================

ğŸ“ Results saved in: results/
ğŸ“Š Timing log: results/timing_log.txt

ğŸ“ˆ Summary:

Task: gsm8k
  âŒ standard: FAILED
  âŒ sparse: FAILED
  âŒ adaptive: FAILED

Task: humaneval
  âŒ standard: FAILED
  âŒ sparse: FAILED
  âŒ adaptive: FAILED

========================================================
Dream Quick Test Configuration
========================================================
Tasks: GSM8K, HumanEval
Model Types: standard, sparse, adaptive
Max New Tokens: 256
Block Size: 32
Test Samples: 50 per dataset
========================================================

ğŸš€ Starting Dream quick test evaluation...
Started at: Monday, December 22, 2025 PM05:37:42 HKT


================================================
ğŸ“Š Task: GSM8K
================================================

Progress: [1/2]

========================================
Running: standard on gsm8k
========================================
Params: max_new_tokens=256, steps=256, temperature=0.1, alg_temp=0.0, block_size=32, limit=25
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-22:17:37:51 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-22:17:37:51 INFO     [__main__:446] Selected Tasks: ['gsm8k']
2025-12-22:17:37:51 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-22:17:37:51 INFO     [evaluator:240] Initializing dream_eval model, with arguments: {'model_path': '/data/qh_models/Dream-v0-Instruct-7B', 'model_type': 'standard',
        'max_new_tokens': 256, 'steps': 256, 'temperature': 0.1, 'top_p': 0.9, 'alg': 'entropy', 'alg_temp': 0.0, 'skip': 0.2, 'select':
        0.3, 'block_size': 32, 'importance_source': 'precomputed', 'min_sparsity': 0.15, 'max_sparsity': 0.85}

Loading Dream model from /data/qh_models/Dream-v0-Instruct-7B...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 114.56it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
2025-12-22:17:38:02 INFO     [evaluator:305] gsm8k: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}
2025-12-22:17:38:02 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k from 5 to 0
2025-12-22:17:38:02 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-12-22:17:38:02 INFO     [api.task:434] Building contexts for gsm8k on rank 0...

======================================================================
Dream Evaluation Setup
======================================================================
Model type: standard
Model path: /data/qh_models/Dream-v0-Instruct-7B
Steps: 256, Max new tokens: 256
Temperature: 0.1, Top-p: 0.9, Top-k: None
======================================================================

  0%|          | 0/25 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1398.03it/s]
2025-12-22:17:38:02 INFO     [evaluator:574] Running generate_until requests
Map:   0%|          | 0/25 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 835.96 examples/s]
Generating...:   0%|          | 0/25 [00:00<?, ?it/s]Generating...:   4%|â–         | 1/25 [00:10<04:19, 10.81s/it]Generating...:   8%|â–Š         | 2/25 [00:20<03:49,  9.98s/it]Generating...:  12%|â–ˆâ–        | 3/25 [00:30<03:43, 10.18s/it]Generating...:  16%|â–ˆâ–Œ        | 4/25 [00:40<03:27,  9.87s/it]Generating...:  20%|â–ˆâ–ˆ        | 5/25 [00:50<03:21, 10.09s/it]Generating...:  24%|â–ˆâ–ˆâ–       | 6/25 [01:01<03:14, 10.24s/it]Generating...:  28%|â–ˆâ–ˆâ–Š       | 7/25 [01:11<03:05, 10.28s/it]Generating...:  32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [01:21<02:55, 10.34s/it]Generating...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [01:32<02:46, 10.42s/it]Generating...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [01:42<02:36, 10.41s/it]Generating...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [01:53<02:25, 10.40s/it]Generating...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [02:03<02:15, 10.41s/it]Generating...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [02:14<02:05, 10.46s/it]Generating...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [02:24<01:55, 10.49s/it]Generating...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [02:35<01:45, 10.50s/it]Generating...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [02:45<01:34, 10.53s/it]Generating...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [02:56<01:23, 10.49s/it]Generating...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [03:06<01:13, 10.46s/it]Generating...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [03:16<01:00, 10.12s/it]Generating...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [03:26<00:51, 10.24s/it]Generating...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [03:36<00:41, 10.30s/it]Generating...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [03:47<00:31, 10.36s/it]Generating...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [03:58<00:20, 10.41s/it]Generating...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [04:07<00:10, 10.12s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [04:16<00:00,  9.90s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [04:16<00:00, 10.27s/it]
2025-12-22:17:43:28 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-22:17:43:28 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: gsm8k
dream_eval (model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=standard,max_new_tokens=256,steps=256,temperature=0.1,top_p=0.9,alg=entropy,alg_temp=0.0,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85), gen_kwargs: (None), limit: 25.0, num_fewshot: 0, batch_size: 1
|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|
|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|
|gsm8k|      3|flexible-extract|     0|exact_match|â†‘  | 0.72|Â±  |0.0917|
|     |       |strict-match    |     0|exact_match|â†‘  | 0.20|Â±  |0.0816|

âœ… Completed standard on gsm8k
â±ï¸  Running time: 5m 47s (347s total)


================================================
ğŸ“Š Task: HUMANEVAL
================================================

Progress: [2/2]

========================================
Running: standard on humaneval
========================================
Params: max_new_tokens=768, steps=768, temperature=0.1, alg_temp=0.0, block_size=32, limit=25
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-22:17:43:41 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-22:17:43:41 INFO     [__main__:446] Selected Tasks: ['humaneval']
2025-12-22:17:43:41 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-22:17:43:41 INFO     [evaluator:240] Initializing dream_eval model, with arguments: {'model_path': '/data/qh_models/Dream-v0-Instruct-7B', 'model_type': 'standard',
        'max_new_tokens': 768, 'steps': 768, 'temperature': 0.1, 'top_p': 0.9, 'alg': 'entropy', 'alg_temp': 0.0, 'skip': 0.2, 'select':
        0.3, 'block_size': 32, 'importance_source': 'precomputed', 'min_sparsity': 0.15, 'max_sparsity': 0.85}

Loading Dream model from /data/qh_models/Dream-v0-Instruct-7B...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 62.88it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

======================================================================
Dream Evaluation Setup
======================================================================
Model type: standard
Model path: /data/qh_models/Dream-v0-Instruct-7B
Steps: 768, Max new tokens: 768
Temperature: 0.1, Top-p: 0.9, Top-k: None
======================================================================

2025-12-22:17:43:53 INFO     [evaluator:305] humaneval: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-12-22:17:43:53 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval in its config. Manual configuration will be ignored.
2025-12-22:17:43:53 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-12-22:17:43:53 INFO     [api.task:434] Building contexts for humaneval on rank 0...
  0%|          | 0/25 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1440.85it/s]
2025-12-22:17:43:53 INFO     [evaluator:574] Running generate_until requests
Map:   0%|          | 0/25 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 791.07 examples/s]
Generating...:   0%|          | 0/25 [00:00<?, ?it/s]Generating...:   4%|â–         | 1/25 [00:55<22:09, 55.39s/it]Generating...:   8%|â–Š         | 2/25 [01:50<21:05, 55.02s/it]Generating...:  12%|â–ˆâ–        | 3/25 [02:44<20:05, 54.78s/it]Generating...:  16%|â–ˆâ–Œ        | 4/25 [03:39<19:13, 54.95s/it]Generating...:  20%|â–ˆâ–ˆ        | 5/25 [04:34<18:19, 55.00s/it]Generating...:  24%|â–ˆâ–ˆâ–       | 6/25 [05:29<17:21, 54.79s/it]Generating...:  28%|â–ˆâ–ˆâ–Š       | 7/25 [06:24<16:25, 54.76s/it]Generating...:  32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [07:18<15:28, 54.63s/it]Generating...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [08:12<14:33, 54.62s/it]Generating...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [09:07<13:39, 54.62s/it]Generating...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [10:03<12:48, 54.87s/it]Generating...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [10:57<11:51, 54.73s/it]Generating...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [11:52<10:56, 54.70s/it]Generating...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [12:46<09:59, 54.53s/it]Generating...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [13:40<09:03, 54.34s/it]Generating...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [14:34<08:08, 54.24s/it]Generating...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [15:28<07:13, 54.20s/it]Generating...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [16:24<06:23, 54.78s/it]Generating...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [17:18<05:27, 54.63s/it]Generating...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [18:13<04:33, 54.64s/it]Generating...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [19:09<03:40, 55.14s/it]Generating...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [20:04<02:45, 55.19s/it]Generating...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [20:59<01:49, 54.96s/it]Generating...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [21:53<00:54, 54.64s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [22:47<00:00, 54.52s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [22:47<00:00, 54.70s/it]
2025-12-22:18:07:52 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-22:18:07:52 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: humaneval
dream_eval (model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=standard,max_new_tokens=768,steps=768,temperature=0.1,top_p=0.9,alg=entropy,alg_temp=0.0,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85), gen_kwargs: (None), limit: 25.0, num_fewshot: 0, batch_size: 1
|  Tasks  |Version|  Filter   |n-shot|Metric|   |Value|   |Stderr|
|---------|------:|-----------|-----:|------|---|----:|---|-----:|
|humaneval|      1|create_test|     0|pass@1|   | 0.08|Â±  |0.0554|

âœ… Completed standard on humaneval
â±ï¸  Running time: 24m 25s (1465s total)


================================================
âœ¨ All evaluations completed!
Finished at: Monday, December 22, 2025 PM06:07:54 HKT
================================================

ğŸ“ Results saved in: results/
ğŸ“Š Timing log: results/timing_log.txt

ğŸ“ˆ Summary:

Task: gsm8k
  âŒ standard: FAILED

Task: humaneval
  âŒ standard: FAILED

========================================================
Dream Quick Test Configuration
========================================================
Tasks: GSM8K, HumanEval
Model Types: standard, sparse, adaptive
Max New Tokens: 256
Block Size: 32
Test Samples: 50 per dataset
========================================================

ğŸš€ Starting Dream quick test evaluation...
Started at: Monday, December 22, 2025 PM07:41:44 HKT


================================================
ğŸ“Š Task: GSM8K_COT
================================================

Progress: [1/2]

========================================
Running: standard on gsm8k_cot
========================================
Params: max_new_tokens=256, steps=256, temperature=0.1, alg_temp=0.0, block_size=32, limit=20
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-22:19:41:53 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-22:19:41:53 INFO     [__main__:446] Selected Tasks: ['gsm8k_cot']
2025-12-22:19:41:53 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-22:19:41:53 INFO     [evaluator:240] Initializing dream_eval model, with arguments: {'model_path': '/data/qh_models/Dream-v0-Instruct-7B', 'model_type': 'standard',
        'max_new_tokens': 256, 'steps': 256, 'temperature': 0.1, 'top_p': 0.9, 'alg': 'entropy', 'alg_temp': 0.0, 'skip': 0.2, 'select':
        0.3, 'block_size': 32, 'importance_source': 'precomputed', 'min_sparsity': 0.15, 'max_sparsity': 0.85}

Loading Dream model from /data/qh_models/Dream-v0-Instruct-7B...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 103.09it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
2025-12-22:19:42:03 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-12-22:19:42:03 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 0
2025-12-22:19:42:03 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-12-22:19:42:03 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 0...

======================================================================
Dream Evaluation Setup
======================================================================
Model type: standard
Model path: /data/qh_models/Dream-v0-Instruct-7B
Steps: 256, Max new tokens: 256
Temperature: 0.1, Top-p: 0.9, Top-k: None
======================================================================

  0%|          | 0/20 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 1262.22it/s]
2025-12-22:19:42:03 INFO     [evaluator:574] Running generate_until requests
Map:   0%|          | 0/20 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 847.85 examples/s]
Generating...:   0%|          | 0/20 [00:00<?, ?it/s]Generating...:   5%|â–Œ         | 1/20 [00:10<03:24, 10.76s/it]Generating...:  10%|â–ˆ         | 2/20 [00:20<02:58,  9.91s/it]Generating...:  15%|â–ˆâ–Œ        | 3/20 [00:30<02:51, 10.11s/it]Generating...:  20%|â–ˆâ–ˆ        | 4/20 [00:39<02:37,  9.83s/it]Generating...:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:50<02:31, 10.12s/it]Generating...:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [01:00<02:22, 10.21s/it]Generating...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [01:11<02:12, 10.22s/it]Generating...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [01:21<02:03, 10.32s/it]Generating...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [01:32<01:54, 10.44s/it]Generating...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [01:42<01:44, 10.47s/it]Generating...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [01:53<01:34, 10.49s/it]Generating...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [02:03<01:23, 10.45s/it]Generating...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [02:14<01:13, 10.43s/it]Generating...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [02:24<01:02, 10.43s/it]Generating...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [02:34<00:52, 10.40s/it]Generating...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [02:45<00:41, 10.47s/it]Generating...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [02:56<00:31, 10.47s/it]Generating...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [03:06<00:20, 10.49s/it]Generating...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [03:15<00:10, 10.17s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [03:26<00:00, 10.26s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [03:26<00:00, 10.32s/it]
2025-12-22:19:46:39 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-22:19:46:39 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: gsm8k_cot
dream_eval (model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=standard,max_new_tokens=256,steps=256,temperature=0.1,top_p=0.9,alg=entropy,alg_temp=0.0,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85), gen_kwargs: (None), limit: 20.0, num_fewshot: 0, batch_size: 1
|  Tasks  |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|
|---------|------:|----------------|-----:|-----------|---|----:|---|-----:|
|gsm8k_cot|      3|flexible-extract|     0|exact_match|â†‘  |  0.8|Â±  |0.0918|
|         |       |strict-match    |     0|exact_match|â†‘  |  0.0|Â±  |0.0000|

âœ… Completed standard on gsm8k_cot
â±ï¸  Running time: 4m 57s (297s total)


================================================
ğŸ“Š Task: HUMANEVAL_INSTRUCT
================================================

Progress: [2/2]

========================================
Running: standard on humaneval_instruct
========================================
Params: max_new_tokens=768, steps=768, temperature=0.1, alg_temp=0.0, block_size=32, limit=20
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-22:19:46:50 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-22:19:46:50 INFO     [__main__:446] Selected Tasks: ['humaneval_instruct']
2025-12-22:19:46:50 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-22:19:46:50 INFO     [evaluator:240] Initializing dream_eval model, with arguments: {'model_path': '/data/qh_models/Dream-v0-Instruct-7B', 'model_type': 'standard',
        'max_new_tokens': 768, 'steps': 768, 'temperature': 0.1, 'top_p': 0.9, 'alg': 'entropy', 'alg_temp': 0.0, 'skip': 0.2, 'select':
        0.3, 'block_size': 32, 'importance_source': 'precomputed', 'min_sparsity': 0.15, 'max_sparsity': 0.85}

Loading Dream model from /data/qh_models/Dream-v0-Instruct-7B...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 112.94it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

======================================================================
Dream Evaluation Setup
======================================================================
Model type: standard
Model path: /data/qh_models/Dream-v0-Instruct-7B
Steps: 768, Max new tokens: 768
Temperature: 0.1, Top-p: 0.9, Top-k: None
======================================================================

2025-12-22:19:47:04 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-12-22:19:47:04 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-12-22:19:47:04 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-12-22:19:47:04 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 0...
  0%|          | 0/20 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 889.89it/s]
2025-12-22:19:47:04 INFO     [evaluator:574] Running generate_until requests
Map:   0%|          | 0/20 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 504.82 examples/s]
Generating...:   0%|          | 0/20 [00:00<?, ?it/s]Generating...:   5%|â–Œ         | 1/20 [01:04<20:18, 64.12s/it]Generating...:  10%|â–ˆ         | 2/20 [02:07<19:08, 63.82s/it]Generating...:  15%|â–ˆâ–Œ        | 3/20 [03:04<17:07, 60.46s/it]Generating...:  20%|â–ˆâ–ˆ        | 4/20 [04:08<16:29, 61.85s/it]Generating...:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [05:11<15:37, 62.53s/it]Generating...:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [06:08<14:08, 60.63s/it]Generating...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [07:12<13:21, 61.66s/it]Generating...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [08:09<12:00, 60.06s/it]Generating...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [09:13<11:13, 61.26s/it]Generating...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [10:10<09:59, 59.92s/it]Generating...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [11:14<09:12, 61.41s/it]Generating...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [12:11<07:59, 59.93s/it]Generating...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [13:08<06:53, 59.09s/it]Generating...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [14:04<05:49, 58.22s/it]Generating...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [15:00<04:47, 57.46s/it]Generating...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [15:56<03:47, 56.95s/it]Generating...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [16:52<02:49, 56.66s/it]Generating...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [17:58<01:58, 59.49s/it]Generating...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [18:54<00:58, 58.63s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [19:51<00:00, 58.11s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [19:51<00:00, 59.59s/it]
2025-12-22:20:08:06 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-22:20:08:06 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: humaneval_instruct
dream_eval (model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=standard,max_new_tokens=768,steps=768,temperature=0.1,top_p=0.9,alg=entropy,alg_temp=0.0,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85), gen_kwargs: (None), limit: 20.0, num_fewshot: 0, batch_size: 1
|      Tasks       |Version|  Filter   |n-shot|Metric|   |Value|   |Stderr|
|------------------|------:|-----------|-----:|------|---|----:|---|-----:|
|humaneval_instruct|      4|create_test|     0|pass@1|   |    0|Â±  |     0|

âœ… Completed standard on humaneval_instruct
â±ï¸  Running time: 21m 27s (1287s total)


================================================
âœ¨ All evaluations completed!
Finished at: Monday, December 22, 2025 PM08:08:08 HKT
================================================

ğŸ“ Results saved in: results/
ğŸ“Š Timing log: results/timing_log.txt

ğŸ“ˆ Summary:

Task: gsm8k_cot
  âŒ standard: FAILED

Task: humaneval_instruct
  âŒ standard: FAILED

========================================================
Dream Quick Test Configuration
========================================================
Tasks: GSM8K, HumanEval
Model Types: standard, sparse, adaptive
Max New Tokens: 256
Block Size: 32
Test Samples: 50 per dataset
========================================================

ğŸš€ Starting Dream quick test evaluation...
Started at: Monday, December 22, 2025 PM08:18:14 HKT


================================================
ğŸ“Š Task: HUMANEVAL_INSTRUCT
================================================

Progress: [1/1]

========================================
Running: standard on humaneval_instruct
========================================
Params: max_new_tokens=768, steps=768, temperature=0.1, alg_temp=0.0, block_size=32, limit=10
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-22:20:18:23 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-22:20:18:23 INFO     [__main__:446] Selected Tasks: ['humaneval_instruct']
2025-12-22:20:18:23 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-22:20:18:23 INFO     [evaluator:240] Initializing dream_eval model, with arguments: {'model_path': '/data/qh_models/Dream-v0-Instruct-7B', 'model_type': 'standard',
        'max_new_tokens': 768, 'steps': 768, 'temperature': 0.1, 'top_p': 0.9, 'alg': 'entropy', 'alg_temp': 0.0, 'skip': 0.2, 'select':
        0.3, 'block_size': 32, 'importance_source': 'precomputed', 'min_sparsity': 0.15, 'max_sparsity': 0.85}

Loading Dream model from /data/qh_models/Dream-v0-Instruct-7B...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 101.63it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

======================================================================
Dream Evaluation Setup
======================================================================
Model type: standard
Model path: /data/qh_models/Dream-v0-Instruct-7B
Steps: 768, Max new tokens: 768
Temperature: 0.1, Top-p: 0.9, Top-k: None
======================================================================

2025-12-22:20:18:36 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-12-22:20:18:36 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-12-22:20:18:36 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-12-22:20:18:36 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 0...
  0%|          | 0/10 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 628.84it/s]
2025-12-22:20:18:36 INFO     [evaluator:574] Running generate_until requests
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:  10%|â–ˆ         | 1/10 [01:04<09:40, 64.51s/it]Generating...:  20%|â–ˆâ–ˆ        | 2/10 [02:08<08:31, 63.94s/it]Generating...:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [03:04<07:04, 60.58s/it]Generating...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [04:08<06:11, 61.97s/it]Generating...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [05:12<05:13, 62.72s/it]Generating...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [06:09<04:02, 60.69s/it]Generating...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [07:13<03:05, 61.76s/it]Generating...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [08:10<02:00, 60.20s/it]Generating...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [09:14<01:01, 61.37s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [10:11<00:00, 59.99s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [10:11<00:00, 61.12s/it]
2025-12-22:20:28:53 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-22:20:28:53 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: humaneval_instruct
dream_eval (model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=standard,max_new_tokens=768,steps=768,temperature=0.1,top_p=0.9,alg=entropy,alg_temp=0.0,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85), gen_kwargs: (None), limit: 10.0, num_fewshot: 0, batch_size: 1
|      Tasks       |Version|  Filter   |n-shot|Metric|   |Value|   |Stderr|
|------------------|------:|-----------|-----:|------|---|----:|---|-----:|
|humaneval_instruct|      4|create_test|     0|pass@1|   |    0|Â±  |     0|

âœ… Completed standard on humaneval_instruct
â±ï¸  Running time: 10m 41s (641s total)


================================================
âœ¨ All evaluations completed!
Finished at: Monday, December 22, 2025 PM08:28:55 HKT
================================================

ğŸ“ Results saved in: results/
ğŸ“Š Timing log: results/timing_log.txt

ğŸ“ˆ Summary:

Task: humaneval_instruct
  âŒ standard: FAILED

========================================================
Dream Quick Test Configuration
========================================================
Tasks: GSM8K, HumanEval
Model Types: standard, sparse, adaptive
Max New Tokens: 256
Block Size: 32
Test Samples: 50 per dataset
========================================================

ğŸš€ Starting Dream quick test evaluation...
Started at: Monday, December 22, 2025 PM09:03:04 HKT


================================================
ğŸ“Š Task: HUMANEVAL
================================================

Progress: [1/2]

========================================
Running: standard on humaneval
========================================
Params: max_new_tokens=768, steps=768, temperature=0.1, alg_temp=0.0, block_size=32, limit=5
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-22:21:03:13 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-22:21:03:13 INFO     [__main__:446] Selected Tasks: ['humaneval']
2025-12-22:21:03:13 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-22:21:03:13 INFO     [evaluator:240] Initializing dream_eval model, with arguments: {'model_path': '/data/qh_models/Dream-v0-Instruct-7B', 'model_type': 'standard',
        'max_new_tokens': 768, 'steps': 768, 'temperature': 0.1, 'top_p': 0.9, 'alg': 'entropy', 'alg_temp': 0.0, 'skip': 0.2, 'select':
        0.3, 'block_size': 32, 'importance_source': 'precomputed', 'min_sparsity': 0.15, 'max_sparsity': 0.85}

Loading Dream model from /data/qh_models/Dream-v0-Instruct-7B...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 107.61it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

======================================================================
Dream Evaluation Setup
======================================================================
Model type: standard
Model path: /data/qh_models/Dream-v0-Instruct-7B
Steps: 768, Max new tokens: 768
Temperature: 0.1, Top-p: 0.9, Top-k: None
======================================================================

2025-12-22:21:03:24 INFO     [evaluator:305] humaneval: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-12-22:21:03:24 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval in its config. Manual configuration will be ignored.
2025-12-22:21:03:24 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-12-22:21:03:24 INFO     [api.task:434] Building contexts for humaneval on rank 0...
  0%|          | 0/5 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 432.67it/s]
2025-12-22:21:03:24 INFO     [evaluator:574] Running generate_until requests
Generating...:   0%|          | 0/5 [00:00<?, ?it/s]Generating...:  20%|â–ˆâ–ˆ        | 1/5 [00:56<03:46, 56.70s/it]Generating...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:52<02:48, 56.12s/it]Generating...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:47<01:51, 55.77s/it]Generating...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [03:43<00:55, 55.76s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [04:38<00:00, 55.61s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [04:38<00:00, 55.77s/it]

[DEBUG] First request context:
  Context length: 456 chars
  Context (first 300 chars): '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nfrom typing import List\n\n\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    """ Check if in given list of numbers, are any two numbers closer to each other than\n    given threshold.\n    >>> has_close'
  Context (last 200 chars): ' than\n    given threshold.\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    True\n    """\n<|im_end|>\n<|im_start|>assistant\n'
  gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}

[DEBUG] First generation:
  Prompt length: 137
  Generated sequence length: 905
  Generated tokens shape: torch.Size([905])
  First 10 tokens: [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645]
  Last 10 tokens: [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]
  Mask token ID: 151666
  Generated answer length: 94
  Generated answer (first 200 chars): 'Here is a Python solution using a sliding window approach:\n\n```python\nfrom typing import List\n'

2025-12-22:21:08:08 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-22:21:08:08 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: humaneval
dream_eval (model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=standard,max_new_tokens=768,steps=768,temperature=0.1,top_p=0.9,alg=entropy,alg_temp=0.0,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85), gen_kwargs: (None), limit: 5.0, num_fewshot: 0, batch_size: 1
|  Tasks  |Version|  Filter   |n-shot|Metric|   |Value|   |Stderr|
|---------|------:|-----------|-----:|------|---|----:|---|-----:|
|humaneval|      1|create_test|     0|pass@1|   |    0|Â±  |     0|

âœ… Completed standard on humaneval
â±ï¸  Running time: 5m 6s (306s total)


================================================
ğŸ“Š Task: GSM8K
================================================

Progress: [2/2]

========================================
Running: standard on gsm8k
========================================
Params: max_new_tokens=256, steps=256, temperature=0.1, alg_temp=0.0, block_size=32, limit=5
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-22:21:08:19 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-22:21:08:19 INFO     [__main__:446] Selected Tasks: ['gsm8k']
2025-12-22:21:08:19 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-22:21:08:19 INFO     [evaluator:240] Initializing dream_eval model, with arguments: {'model_path': '/data/qh_models/Dream-v0-Instruct-7B', 'model_type': 'standard',
        'max_new_tokens': 256, 'steps': 256, 'temperature': 0.1, 'top_p': 0.9, 'alg': 'entropy', 'alg_temp': 0.0, 'skip': 0.2, 'select':
        0.3, 'block_size': 32, 'importance_source': 'precomputed', 'min_sparsity': 0.15, 'max_sparsity': 0.85}

Loading Dream model from /data/qh_models/Dream-v0-Instruct-7B...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 108.85it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
2025-12-22:21:08:29 INFO     [evaluator:305] gsm8k: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}
2025-12-22:21:08:29 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k from 5 to 0
2025-12-22:21:08:29 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-12-22:21:08:29 INFO     [api.task:434] Building contexts for gsm8k on rank 0...

======================================================================
Dream Evaluation Setup
======================================================================
Model type: standard
Model path: /data/qh_models/Dream-v0-Instruct-7B
Steps: 256, Max new tokens: 256
Temperature: 0.1, Top-p: 0.9, Top-k: None
======================================================================

  0%|          | 0/5 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 433.22it/s]
2025-12-22:21:08:29 INFO     [evaluator:574] Running generate_until requests
Generating...:   0%|          | 0/5 [00:00<?, ?it/s]Generating...:  20%|â–ˆâ–ˆ        | 1/5 [00:11<00:44, 11.10s/it]Generating...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:20<00:30, 10.06s/it]Generating...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:30<00:20, 10.23s/it]Generating...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:40<00:09,  9.93s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:50<00:00, 10.07s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:50<00:00, 10.13s/it]
2025-12-22:21:09:24 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-22:21:09:24 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: gsm8k

[DEBUG] First request context:
  Context length: 406 chars
  Context (first 300 chars): "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. Ho"
  Context (last 200 chars): " with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\nAnswer:<|im_end|>\n<|im_start|>assistant\n"
  gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}

[DEBUG] First generation:
  Prompt length: 87
  Generated sequence length: 343
  Generated tokens shape: torch.Size([343])
  First 10 tokens: [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645]
  Last 10 tokens: [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]
  Mask token ID: 151666
  Generated answer length: 268
  Generated answer (first 200 chars): 'Janet eats 3 eggs for breakfast and bakes 4 eggs for muffins, so she uses 3 + 4 = 7 eggs per day. Her ducks lay 16 eggs per day, so she sells 16 - 7 = 9 eggs per day. She sells each egg for $2, so she'

dream_eval (model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=standard,max_new_tokens=256,steps=256,temperature=0.1,top_p=0.9,alg=entropy,alg_temp=0.0,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85), gen_kwargs: (None), limit: 5.0, num_fewshot: 0, batch_size: 1
|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|
|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|
|gsm8k|      3|flexible-extract|     0|exact_match|â†‘  |  0.8|Â±  |0.2000|
|     |       |strict-match    |     0|exact_match|â†‘  |  0.4|Â±  |0.2449|

âœ… Completed standard on gsm8k
â±ï¸  Running time: 1m 16s (76s total)


================================================
âœ¨ All evaluations completed!
Finished at: Monday, December 22, 2025 PM09:09:26 HKT
================================================

ğŸ“ Results saved in: results/
ğŸ“Š Timing log: results/timing_log.txt

ğŸ“ˆ Summary:

Task: humaneval
  âŒ standard: FAILED

Task: gsm8k
  âŒ standard: FAILED

========================================================
Dream Quick Test Configuration
========================================================
Tasks: GSM8K, HumanEval
Model Types: standard, sparse, adaptive
Max New Tokens: 256
Block Size: 32
Test Samples: 50 per dataset
========================================================

ğŸš€ Starting Dream quick test evaluation...
Started at: Monday, December 22, 2025 PM09:19:56 HKT


================================================
ğŸ“Š Task: HUMANEVAL
================================================

Progress: [1/2]

========================================
Running: standard on humaneval
========================================
Params: max_new_tokens=768, steps=768, temperature=0.1, alg_temp=0.0, block_size=32, limit=3
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-22:21:20:05 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-22:21:20:05 INFO     [__main__:446] Selected Tasks: ['humaneval']
2025-12-22:21:20:05 WARNING  [evaluator:172] pretrained=model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=standard,max_new_tokens=768,steps=768,temperature=0.1,top_p=0.9,alg=entropy,alg_temp=0.0,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85
        appears to be an instruct or chat variant but chat template is not applied. Recommend setting `apply_chat_template` (optionally
        `fewshot_as_multiturn`).
2025-12-22:21:20:05 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-22:21:20:05 INFO     [evaluator:240] Initializing dream_eval model, with arguments: {'model_path': '/data/qh_models/Dream-v0-Instruct-7B', 'model_type': 'standard',
        'max_new_tokens': 768, 'steps': 768, 'temperature': 0.1, 'top_p': 0.9, 'alg': 'entropy', 'alg_temp': 0.0, 'skip': 0.2, 'select':
        0.3, 'block_size': 32, 'importance_source': 'precomputed', 'min_sparsity': 0.15, 'max_sparsity': 0.85}

Loading Dream model from /data/qh_models/Dream-v0-Instruct-7B...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 109.28it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

======================================================================
Dream Evaluation Setup
======================================================================
Model type: standard
Model path: /data/qh_models/Dream-v0-Instruct-7B
Steps: 768, Max new tokens: 768
Temperature: 0.1, Top-p: 0.9, Top-k: None
======================================================================

2025-12-22:21:20:16 INFO     [evaluator:305] humaneval: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-12-22:21:20:16 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval in its config. Manual configuration will be ignored.
2025-12-22:21:20:16 INFO     [api.task:434] Building contexts for humaneval on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 2839.10it/s]
2025-12-22:21:20:16 INFO     [evaluator:574] Running generate_until requests
Generating...:   0%|          | 0/3 [00:00<?, ?it/s]Generating...:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:56<01:52, 56.18s/it]Generating...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:51<00:55, 55.43s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [02:45<00:00, 54.90s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [02:45<00:00, 55.11s/it]

[DEBUG] First request context:
  Context length: 348 chars
  Context (first 300 chars): 'from typing import List\n\n\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    """ Check if in given list of numbers, are any two numbers closer to each other than\n    given threshold.\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    >>> has_close_elements([1.0, '
  Context (last 200 chars): ' two numbers closer to each other than\n    given threshold.\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    True\n    """\n'
  gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}

[DEBUG] First generation:
  Prompt length: 118
  Generated sequence length: 886
  Generated tokens shape: torch.Size([886])
  First 10 tokens: [1499, 19496, 1159, 1759, 1406, 750, 702, 12704, 22801, 47207]
  Last 10 tokens: [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]
  Mask token ID: 151666
  Generated answer length: 150
  Generated answer (first 200 chars): '    numbers.sort()\n    for i in range(len(numbers) - 1):\n        if numbers[i + 1] - numbers[i] < threshold:\n            return True\n    return False\n'

2025-12-22:21:23:06 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-22:21:23:06 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: humaneval
dream_eval (model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=standard,max_new_tokens=768,steps=768,temperature=0.1,top_p=0.9,alg=entropy,alg_temp=0.0,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85), gen_kwargs: (None), limit: 3.0, num_fewshot: 0, batch_size: 1
|  Tasks  |Version|  Filter   |n-shot|Metric|   |Value |   |Stderr|
|---------|------:|-----------|-----:|------|---|-----:|---|-----:|
|humaneval|      1|create_test|     0|pass@1|   |0.3333|Â±  |0.3333|

âœ… Completed standard on humaneval
â±ï¸  Running time: 3m 12s (192s total)


================================================
ğŸ“Š Task: GSM8K
================================================

Progress: [2/2]

========================================
Running: standard on gsm8k
========================================
Params: max_new_tokens=256, steps=256, temperature=0.1, alg_temp=0.0, block_size=32, limit=3
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-22:21:23:17 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-22:21:23:17 INFO     [__main__:446] Selected Tasks: ['gsm8k']
2025-12-22:21:23:17 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-22:21:23:17 INFO     [evaluator:240] Initializing dream_eval model, with arguments: {'model_path': '/data/qh_models/Dream-v0-Instruct-7B', 'model_type': 'standard',
        'max_new_tokens': 256, 'steps': 256, 'temperature': 0.1, 'top_p': 0.9, 'alg': 'entropy', 'alg_temp': 0.0, 'skip': 0.2, 'select':
        0.3, 'block_size': 32, 'importance_source': 'precomputed', 'min_sparsity': 0.15, 'max_sparsity': 0.85}

Loading Dream model from /data/qh_models/Dream-v0-Instruct-7B...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 93.08it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
2025-12-22:21:23:27 INFO     [evaluator:305] gsm8k: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}
2025-12-22:21:23:27 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k from 5 to 0
2025-12-22:21:23:27 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-12-22:21:23:27 INFO     [api.task:434] Building contexts for gsm8k on rank 0...

======================================================================
Dream Evaluation Setup
======================================================================
Model type: standard
Model path: /data/qh_models/Dream-v0-Instruct-7B
Steps: 256, Max new tokens: 256
Temperature: 0.1, Top-p: 0.9, Top-k: None
======================================================================

  0%|          | 0/3 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 273.35it/s]
2025-12-22:21:23:27 INFO     [evaluator:574] Running generate_until requests
Generating...:   0%|          | 0/3 [00:00<?, ?it/s]Generating...:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:11<00:22, 11.37s/it]Generating...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:20<00:10, 10.22s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:31<00:00, 10.34s/it]Generating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:31<00:00, 10.42s/it]
2025-12-22:21:24:02 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-12-22:21:24:02 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: gsm8k

[DEBUG] First request context:
  Context length: 406 chars
  Context (first 300 chars): "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. Ho"
  Context (last 200 chars): " with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\nAnswer:<|im_end|>\n<|im_start|>assistant\n"
  gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}

[DEBUG] First generation:
  Prompt length: 87
  Generated sequence length: 343
  Generated tokens shape: torch.Size([343])
  First 10 tokens: [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645]
  Last 10 tokens: [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]
  Mask token ID: 151666
  Generated answer length: 268
  Generated answer (first 200 chars): 'Janet eats 3 eggs for breakfast and bakes 4 eggs for muffins, so she uses 3 + 4 = 7 eggs per day. Her ducks lay 16 eggs per day, so she sells 16 - 7 = 9 eggs per day. She sells each egg for $2, so she'

dream_eval (model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=standard,max_new_tokens=256,steps=256,temperature=0.1,top_p=0.9,alg=entropy,alg_temp=0.0,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85), gen_kwargs: (None), limit: 3.0, num_fewshot: 0, batch_size: 1
|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|
|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|
|gsm8k|      3|flexible-extract|     0|exact_match|â†‘  |0.6667|Â±  |0.3333|
|     |       |strict-match    |     0|exact_match|â†‘  |0.0000|Â±  |0.0000|

âœ… Completed standard on gsm8k
â±ï¸  Running time: 0m 56s (56s total)

./run_eval_task.sh: line 203: unexpected EOF while looking for matching `"'
./run_eval_task.sh: line 206: syntax error: unexpected end of file
========================================================
Dream Quick Test Configuration
========================================================
Tasks: GSM8K, HumanEval
Model Types: standard, sparse, adaptive
Max New Tokens: 256
Block Size: 32
Test Samples: 50 per dataset
========================================================

ğŸš€ Starting Dream quick test evaluation...
Started at: Monday, December 22, 2025 PM09:24:34 HKT


================================================
ğŸ“Š Task: HUMANEVAL
================================================

Progress: [1/6]

========================================
Running: standard on humaneval
========================================
Params: max_new_tokens=768, steps=768, temperature=0.1, alg_temp=0.0, block_size=32, limit=50
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/qiheng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-12-22:21:24:42 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-12-22:21:24:42 INFO     [__main__:446] Selected Tasks: ['humaneval']
2025-12-22:21:24:42 WARNING  [evaluator:172] pretrained=model_path=/data/qh_models/Dream-v0-Instruct-7B,model_type=standard,max_new_tokens=768,steps=768,temperature=0.1,top_p=0.9,alg=entropy,alg_temp=0.0,skip=0.2,select=0.3,block_size=32,importance_source=precomputed,min_sparsity=0.15,max_sparsity=0.85
        appears to be an instruct or chat variant but chat template is not applied. Recommend setting `apply_chat_template` (optionally
        `fewshot_as_multiturn`).
2025-12-22:21:24:42 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-12-22:21:24:42 INFO     [evaluator:240] Initializing dream_eval model, with arguments: {'model_path': '/data/qh_models/Dream-v0-Instruct-7B', 'model_type': 'standard',
        'max_new_tokens': 768, 'steps': 768, 'temperature': 0.1, 'top_p': 0.9, 'alg': 'entropy', 'alg_temp': 0.0, 'skip': 0.2, 'select':
        0.3, 'block_size': 32, 'importance_source': 'precomputed', 'min_sparsity': 0.15, 'max_sparsity': 0.85}

Loading Dream model from /data/qh_models/Dream-v0-Instruct-7B...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 108.28it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

======================================================================
Dream Evaluation Setup
======================================================================
Model type: standard
Model path: /data/qh_models/Dream-v0-Instruct-7B
Steps: 768, Max new tokens: 768
Temperature: 0.1, Top-p: 0.9, Top-k: None
======================================================================

2025-12-22:21:24:55 INFO     [evaluator:305] humaneval: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-12-22:21:24:55 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval in its config. Manual configuration will be ignored.
2025-12-22:21:24:55 INFO     [api.task:434] Building contexts for humaneval on rank 0...
  0%|          | 0/50 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 3934.62it/s]
2025-12-22:21:24:55 INFO     [evaluator:574] Running generate_until requests
Generating...:   0%|          | 0/50 [00:00<?, ?it/s]