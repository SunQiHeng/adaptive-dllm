"""
æµ‹è¯• head_attribution.py ä¸­çš„è‡ªå®šä¹‰ forward å‡½æ•°æ˜¯å¦ä¸å®˜æ–¹å®ç°ä¸€è‡´
"""
import torch
import sys
sys.path.insert(0, '/home/qiheng/Projects/adaptive-dllm')

from transformers import AutoTokenizer, AutoModel
from models.LLaDA.attribution.head_attribution import IntegratedGradientsHeadAttribution


def test_compute_layer_head_att():
    """
    æµ‹è¯• _compute_layer_head_att æ˜¯å¦ä¸å®˜æ–¹æ¨¡å‹çš„è¾“å‡ºä¸€è‡´
    
    ç­–ç•¥ï¼š
    1. ä½¿ç”¨å®˜æ–¹æ¨¡å‹å‰å‘ä¼ æ’­åˆ°ç›®æ ‡å±‚ï¼Œæå– attention è¾“å‡º
    2. ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°è®¡ç®—åŒä¸€å±‚çš„ attention è¾“å‡º
    3. å¯¹æ¯”ä¸¤è€…æ˜¯å¦ä¸€è‡´
    """
    print("=" * 80)
    print("Test 1: _compute_layer_head_att")
    print("=" * 80)
    
    device = 'cuda'
    model_path = "/home/qiheng/Projects/models/LLaDA-8B-Instruct"
    
    # åŠ è½½æ¨¡å‹å’Œ tokenizer
    model = AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.bfloat16).to(device).eval()
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    # å‡†å¤‡è¾“å…¥
    prompt = "Hello, how are you?"
    messages = [{"role": "user", "content": prompt}]
    prompt_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
    
    tokenizer.padding_side = 'left'
    encoded = tokenizer([prompt_text], add_special_tokens=False, padding=True, return_tensors="pt")
    input_ids = encoded['input_ids'].to(device)
    attention_mask = encoded['attention_mask'].to(device)
    
    print(f"Input shape: {input_ids.shape}")
    print(f"Input text: {prompt_text[:50]}...")
    
    # åˆ›å»ºå½’å› å¯¹è±¡
    ig_attribution = IntegratedGradientsHeadAttribution(model.model, n_steps=10)
    
    # æµ‹è¯•å¤šä¸ªå±‚
    test_layers = [0, 15, 31]  # ç¬¬ä¸€å±‚ã€ä¸­é—´å±‚ã€æœ€åä¸€å±‚
    
    for target_layer in test_layers:
        print(f"\n--- Testing Layer {target_layer} ---")
        
        # æ–¹æ³• 1: ä½¿ç”¨å®˜æ–¹æ¨¡å‹ + hook æå– attention è¾“å‡º
        import torch.nn.functional as F
        captured_att_official = [None]
        
        original_sdpa = F.scaled_dot_product_attention
        
        def sdpa_with_capture(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False):
            result = original_sdpa(q, k, v, attn_mask=attn_mask, dropout_p=dropout_p, is_causal=is_causal)
            captured_att_official[0] = result.clone()
            return result
        
        with torch.no_grad():
            # è·å– embeddings
            x = model.model.transformer.wte(input_ids)
            if model.model.config.input_emb_norm:
                x = x * (model.model.config.d_model ** 0.5)
            
            if not (model.model.config.alibi or model.model.config.rope):
                seq_len = input_ids.shape[1]
                pos = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0)
                pos_emb = model.model.transformer.wpe(pos)
                x = pos_emb + x
            
            x = model.model.transformer.emb_drop(x)
            
            # å¤„ç† attention mask and bias (matching official logic)
            attention_mask_input = attention_mask
            if attention_mask_input is not None and 0.0 in attention_mask_input:
                attention_mask_processed = attention_mask_input.to(dtype=torch.float).view(-1, seq_len)[:, None, None, :]
                attention_mask_processed = (1.0 - attention_mask_processed) * torch.finfo(attention_mask_processed.dtype).min
            else:
                attention_mask_processed = None
            
            attention_bias = None
            if attention_mask_processed is not None or model.model.config.alibi:
                if attention_bias is None and not model.model.config.alibi:
                    attention_bias = model.model.get_bidirectional_attention_bias(seq_len, x.device)
                    mask_len = seq_len
                    if attention_mask_processed is not None:
                        mask_len = attention_mask_processed.shape[-1]
                    attention_bias = attention_bias[:, :, :mask_len, :mask_len].to(dtype=torch.float)
                    if attention_mask_processed is not None:
                        attention_bias = attention_bias + attention_mask_processed
            
            # é€å±‚å‰å‘ä¼ æ’­ï¼Œä½¿ç”¨å®˜æ–¹ block
            blocks = model.model.transformer.blocks
            for layer_idx, block in enumerate(blocks):
                if layer_idx == target_layer:
                    # åœ¨ç›®æ ‡å±‚ï¼Œä½¿ç”¨ hook æ•è· attention
                    F.scaled_dot_product_attention = sdpa_with_capture
                    try:
                        x, _ = block(x, attention_bias=attention_bias)
                    finally:
                        F.scaled_dot_product_attention = original_sdpa
                    break
                else:
                    # æ­£å¸¸ forward
                    x, _ = block(x, attention_bias=attention_bias)
            
            att_official = captured_att_official[0]
            
            # ç»§ç»­åé¢çš„æ— ç”¨ä»£ç ï¼ˆä¸ºäº†ä¿æŒæµ‹è¯•ç»“æ„ï¼‰
            if False:
                # Feed-forward
                og_x = x
                x = block.ff_norm(x)
                
                if hasattr(block, 'att_proj'):
                    x = block.ff_proj(x)
                    x = block.act(x)
                    x = block.ff_out(x)
                else:
                    x_gate = block.ff_proj(x)
                    x_up = block.up_proj(x)
                    x = block.act(x_gate) * x_up
                    x = block.ff_out(x)
                
                x = block.dropout(x)
                x = og_x + x
        
        # æ–¹æ³• 2: ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°
        att_custom = ig_attribution._compute_layer_head_att(
            input_ids=input_ids,
            attention_mask=attention_mask,
            target_layer_idx=target_layer
        )
        
        # å¯¹æ¯”ç»“æœ
        print(f"Official att shape: {att_official.shape}")
        print(f"Custom att shape:   {att_custom.shape}")
        
        # è®¡ç®—å·®å¼‚
        diff = (att_official - att_custom).abs()
        max_diff = diff.max().item()
        mean_diff = diff.mean().item()
        relative_diff = (diff / (att_official.abs() + 1e-8)).mean().item()
        
        print(f"Max absolute difference:  {max_diff:.2e}")
        print(f"Mean absolute difference: {mean_diff:.2e}")
        print(f"Mean relative difference: {relative_diff:.2%}")
        
        # åˆ¤æ–­æ˜¯å¦é€šè¿‡
        if max_diff < 1e-5 or relative_diff < 0.01:
            print("âœ… PASSED: Attention outputs match!")
        else:
            print("âŒ FAILED: Attention outputs differ significantly!")
            print(f"Sample official values: {att_official[0, 0, 0, :5]}")
            print(f"Sample custom values:   {att_custom[0, 0, 0, :5]}")


def test_forward_with_layer_head_cache():
    """
    æµ‹è¯• _forward_with_layer_head_cache æ˜¯å¦ä¸å®˜æ–¹æ¨¡å‹è¾“å‡ºä¸€è‡´
    
    ç­–ç•¥ï¼š
    1. ä½¿ç”¨å®˜æ–¹æ¨¡å‹å®Œæ•´å‰å‘ä¼ æ’­ï¼Œå¾—åˆ° logits
    2. æå–æŸä¸€å±‚çš„ attention è¾“å‡º
    3. ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°ï¼Œå°†è¯¥å±‚çš„ attention æ›¿æ¢ä¸ºæå–çš„å€¼
    4. å¯¹æ¯”ä¸¤ä¸ª logits æ˜¯å¦ä¸€è‡´
    """
    print("\n" + "=" * 80)
    print("Test 2: _forward_with_layer_head_cache")
    print("=" * 80)
    
    device = 'cuda'
    model_path = "/home/qiheng/Projects/models/LLaDA-8B-Instruct"
    
    # åŠ è½½æ¨¡å‹å’Œ tokenizer
    model = AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.bfloat16).to(device).eval()
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    # å‡†å¤‡è¾“å…¥
    prompt = "What is the capital of France?"
    messages = [{"role": "user", "content": prompt}]
    prompt_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
    
    tokenizer.padding_side = 'left'
    encoded = tokenizer([prompt_text], add_special_tokens=False, padding=True, return_tensors="pt")
    input_ids = encoded['input_ids'].to(device)
    attention_mask = encoded['attention_mask'].to(device)
    
    print(f"Input shape: {input_ids.shape}")
    print(f"Input text: {prompt_text[:50]}...")
    
    # åˆ›å»ºå½’å› å¯¹è±¡
    ig_attribution = IntegratedGradientsHeadAttribution(model.model, n_steps=10)
    
    # æµ‹è¯•å¤šä¸ªå±‚
    test_layers = [0, 15, 31]
    
    for target_layer in test_layers:
        print(f"\n--- Testing Layer {target_layer} ---")
        
        # æ–¹æ³• 1: å®˜æ–¹å®Œæ•´å‰å‘ä¼ æ’­
        with torch.no_grad():
            output_official = model(input_ids, attention_mask=attention_mask)
            logits_official = output_official.logits
        
        # æå–è¯¥å±‚çš„ attention è¾“å‡º
        with torch.no_grad():
            att_actual = ig_attribution._compute_layer_head_att(
                input_ids=input_ids,
                attention_mask=attention_mask,
                target_layer_idx=target_layer
            )
        
        print(f"  Extracted att mean: {att_actual.mean().item():.6f}")
        
        # æ–¹æ³• 2: ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°ï¼Œå°†è¯¥å±‚çš„ att æ›¿æ¢ä¸ºæå–çš„å€¼
        logits_custom = ig_attribution._forward_with_layer_head_cache(
            input_ids=input_ids,
            attention_mask=attention_mask,
            target_layer_idx=target_layer,
            head_att_values=att_actual
        )
        
        print(f"  Target layer: {target_layer}")
        
        # å¯¹æ¯”ç»“æœ
        print(f"Official logits shape: {logits_official.shape}")
        print(f"Custom logits shape:   {logits_custom.shape}")
    
        # è®¡ç®—å·®å¼‚
        diff = (logits_official - logits_custom).abs()
        max_diff = diff.max().item()
        mean_diff = diff.mean().item()
        relative_diff = (diff / (logits_official.abs() + 1e-8)).mean().item()
        
        print(f"Max absolute difference:  {max_diff:.2e}")
        print(f"Mean absolute difference: {mean_diff:.2e}")
        print(f"Mean relative difference: {relative_diff:.2%}")
        
        # åˆ¤æ–­æ˜¯å¦é€šè¿‡
        if max_diff < 1e-3 or relative_diff < 0.01:
            print("âœ… PASSED: Logits match!")
        else:
            print("âŒ FAILED: Logits differ significantly!")
            print(f"Sample official logits: {logits_official[0, 0, :5]}")
            print(f"Sample custom logits:   {logits_custom[0, 0, :5]}")


def test_modified_att_values():
    """
    æµ‹è¯•ä½¿ç”¨ä¿®æ”¹è¿‡çš„ att å€¼æ˜¯å¦èƒ½æ­£ç¡®å½±å“è¾“å‡º
    
    ç­–ç•¥ï¼š
    1. æå–æŸä¸€å±‚çš„ attention è¾“å‡º
    2. ä¿®æ”¹è¯¥ attention è¾“å‡ºï¼ˆä¾‹å¦‚ä¹˜ä»¥ 0.5ï¼‰
    3. ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°å‰å‘ä¼ æ’­
    4. éªŒè¯è¾“å‡ºç¡®å®å‘ç”Ÿäº†å˜åŒ–
    """
    print("\n" + "=" * 80)
    print("Test 3: Modified att values")
    print("=" * 80)
    
    device = 'cuda'
    model_path = "/home/qiheng/Projects/models/LLaDA-8B-Instruct"
    
    # åŠ è½½æ¨¡å‹å’Œ tokenizer
    model = AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.bfloat16).to(device).eval()
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    # å‡†å¤‡è¾“å…¥
    prompt = "The answer is"
    messages = [{"role": "user", "content": prompt}]
    prompt_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
    
    tokenizer.padding_side = 'left'
    encoded = tokenizer([prompt_text], add_special_tokens=False, padding=True, return_tensors="pt")
    input_ids = encoded['input_ids'].to(device)
    attention_mask = encoded['attention_mask'].to(device)
    
    print(f"Input shape: {input_ids.shape}")
    
    # åˆ›å»ºå½’å› å¯¹è±¡
    ig_attribution = IntegratedGradientsHeadAttribution(model.model, n_steps=10)
    
    target_layer = 15
    
    # è·å–åŸå§‹ att
    with torch.no_grad():
        att_original = ig_attribution._compute_layer_head_att(
            input_ids=input_ids,
            attention_mask=attention_mask,
            target_layer_idx=target_layer
        )
    
    # åŸå§‹ logits
    logits_original = ig_attribution._forward_with_layer_head_cache(
        input_ids=input_ids,
        attention_mask=attention_mask,
        target_layer_idx=target_layer,
        head_att_values=att_original
    )
    
    # ä¿®æ”¹ attï¼ˆä¹˜ä»¥ä¸åŒçš„ç³»æ•°ï¼‰
    test_scales = [0.0, 0.5, 2.0]
    
    for scale in test_scales:
        att_modified = att_original * scale
        
        logits_modified = ig_attribution._forward_with_layer_head_cache(
            input_ids=input_ids,
            attention_mask=attention_mask,
            target_layer_idx=target_layer,
            head_att_values=att_modified
        )
        
        diff = (logits_original - logits_modified).abs()
        max_diff = diff.max().item()
        mean_diff = diff.mean().item()
        
        print(f"\nScale: {scale:.1f}")
        print(f"  Max difference:  {max_diff:.2e}")
        print(f"  Mean difference: {mean_diff:.2e}")
        
        if scale == 1.0:
            if max_diff < 1e-5:
                print("  âœ… Scale=1.0 produces identical output")
            else:
                print("  âŒ Scale=1.0 should produce identical output!")
        else:
            if max_diff > 1e-3:
                print(f"  âœ… Scale={scale} correctly modifies output")
            else:
                print(f"  âŒ Scale={scale} should modify output more!")


if __name__ == "__main__":
    print("\n" + "ğŸ§ª" * 40)
    print("Testing head_attribution.py custom forward functions")
    print("ğŸ§ª" * 40 + "\n")
    
    try:
        # Test 1: _compute_layer_head_att
        test_compute_layer_head_att()
        
        # Test 2: _forward_with_layer_head_cache
        test_forward_with_layer_head_cache()
        
        # Test 3: Modified att values
        test_modified_att_values()
        
        print("\n" + "=" * 80)
        print("âœ… All tests completed!")
        print("=" * 80)
        
    except Exception as e:
        print(f"\nâŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()

