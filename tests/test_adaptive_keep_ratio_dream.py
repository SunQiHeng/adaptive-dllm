#!/usr/bin/env python3
"""
æµ‹è¯•Dream adaptiveæ¨¡å¼ä¸‹å®é™…çš„attention keep ratio

è¿è¡Œæ­¤è„šæœ¬æŸ¥çœ‹åœ¨ä¸åŒselectå€¼ä¸‹ï¼Œæ¯å±‚å’Œå…¨å±€çš„å¹³å‡keep_ratio
"""

import sys
import os
sys.path.insert(0, '/home/qiheng/Projects/adaptive-dllm')

import torch
from models.Dream.sparse.adaptive_utils_dream import create_adaptive_sparsity_config

def test_dream_keep_ratios():
    """æµ‹è¯•Dreamæ¨¡å‹åœ¨ä¸åŒselectå€¼ä¸‹çš„keep ratio"""
    
    print("=" * 80)
    print("Dream Adaptive Keep Ratio æµ‹è¯•")
    print("=" * 80)
    
    # Dreamæ¨¡å‹é…ç½® (éœ€è¦æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´)
    # Dream-v0-Instruct-7B ä½¿ç”¨çš„é…ç½®
    n_layers = 28
    n_heads = 4  # KV heads (Dream uses GQA: 32 query heads, 4 KV heads)
    
    # åŠ è½½é¢„è®¡ç®—çš„importance scores
    importance_path = '/home/qiheng/Projects/adaptive-dllm/configs/head_importance_dream/head_importance.pt'
    
    if os.path.exists(importance_path):
        print(f"\nâœ“ åŠ è½½é¢„è®¡ç®—çš„importance scores: {importance_path}")
        importance_data = torch.load(importance_path, weights_only=False)
        importance_scores = importance_data['importance_scores']
    else:
        print(f"\nâš  æœªæ‰¾åˆ°é¢„è®¡ç®—æ–‡ä»¶ï¼Œä½¿ç”¨éšæœºimportance scores")
        importance_scores = None
    
    # åˆ›å»ºadaptiveé…ç½®
    print(f"\nåˆ›å»ºadaptiveé…ç½®...")
    print(f"  Layers: {n_layers}")
    print(f"  KV Heads: {n_heads}")
    print(f"  Output mode: ç›¸å¯¹æƒé‡ (mean=1.0)")
    
    adaptive_config = create_adaptive_sparsity_config(
        n_layers=n_layers,
        n_heads=n_heads,
        importance_scores=importance_scores,
        min_sparsity=0.1,
        max_sparsity=0.9,
        normalize_strategy='global_percentile',
        output_relative_weights=True,
        seed=42
    )
    
    sparsity_levels = adaptive_config['sparsity_levels']
    
    # æ‰“å°ç›¸å¯¹æƒé‡ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("ç›¸å¯¹æƒé‡ç»Ÿè®¡ (è¿™äº›æ˜¯å½’ä¸€åŒ–çš„é‡è¦æ€§æƒé‡ï¼Œmeanâ‰ˆ1.0)")
    print("=" * 80)
    
    all_weights = []
    for layer_idx in range(n_layers):
        weights = sparsity_levels[layer_idx]
        all_weights.append(weights)
        print(f"Layer {layer_idx:2d}: mean={weights.mean():.4f}, "
              f"min={weights.min():.4f}, max={weights.max():.4f}, "
              f"range=[{weights.min():.3f}, {weights.max():.3f}]")
    
    # å…¨å±€ç»Ÿè®¡
    all_weights_tensor = torch.cat(all_weights)
    print("\n" + "-" * 80)
    print(f"å…¨å±€æƒé‡ç»Ÿè®¡: mean={all_weights_tensor.mean():.4f}, "
          f"min={all_weights_tensor.min():.4f}, max={all_weights_tensor.max():.4f}")
    
    # æµ‹è¯•ä¸åŒçš„selectå€¼
    select_values = [0.2, 0.3, 0.5, 0.8]
    
    print("\n" + "=" * 80)
    print("åœ¨ä¸åŒselectå€¼ä¸‹çš„å®é™…keep_ratioï¼ˆæ¨ç†æ—¶ï¼‰")
    print("=" * 80)
    
    for select in select_values:
        print(f"\n{'=' * 80}")
        print(f"SELECT = {select:.1f} (ç›®æ ‡: å¹³å‡ä¿ç•™{select*100:.0f}%çš„attentionå—)")
        print(f"{'=' * 80}")
        
        layer_keep_ratios = []
        
        for layer_idx in range(n_layers):
            weights = sparsity_levels[layer_idx]
            
            # æ¨¡æ‹Ÿæ¨ç†æ—¶çš„è®¡ç®—: keep_ratio = weight * select
            keep_ratios = torch.clamp(weights * select, 0.0, 1.0)
            
            mean_keep = keep_ratios.mean().item()
            min_keep = keep_ratios.min().item()
            max_keep = keep_ratios.max().item()
            
            layer_keep_ratios.append(keep_ratios)
            
            # è®¡ç®—æœ‰å¤šå°‘headsè§¦åŠä¸Šé™
            clamped_heads = (weights * select > 1.0).sum().item()
            
            print(f"Layer {layer_idx:2d}: "
                  f"å¹³å‡={mean_keep:.4f} ({mean_keep*100:.1f}%), "
                  f"èŒƒå›´=[{min_keep:.3f}, {max_keep:.3f}], "
                  f"è§¦åŠä¸Šé™: {clamped_heads}/{n_heads} heads")
        
        # å…¨å±€ç»Ÿè®¡
        all_keep_ratios = torch.cat(layer_keep_ratios)
        global_mean = all_keep_ratios.mean().item()
        global_min = all_keep_ratios.min().item()
        global_max = all_keep_ratios.max().item()
        
        # è®¡ç®—æ€»çš„è§¦åŠä¸Šé™çš„headsæ•°é‡
        total_clamped = sum((layer_keep_ratios[i] >= 1.0).sum().item() 
                           for i in range(n_layers))
        total_heads = n_layers * n_heads
        
        print(f"\n{'â”€' * 80}")
        print(f"ğŸ“Š å…¨å±€ç»Ÿè®¡:")
        print(f"   å¹³å‡keep_ratio: {global_mean:.4f} ({global_mean*100:.1f}%)")
        print(f"   ç›®æ ‡select:     {select:.4f} ({select*100:.1f}%)")
        print(f"   åå·®:          {abs(global_mean - select):.4f} ({abs(global_mean - select)*100:.1f}%)")
        print(f"   èŒƒå›´:          [{global_min:.3f}, {global_max:.3f}]")
        print(f"   è§¦åŠä¸Šé™:       {total_clamped}/{total_heads} heads ({total_clamped/total_heads*100:.1f}%)")
        
        # åˆ†æ
        if abs(global_mean - select) < 0.01:
            print(f"   âœ… å®é™…å¹³å‡å€¼ä¸ç›®æ ‡éå¸¸æ¥è¿‘ï¼")
        elif abs(global_mean - select) < 0.05:
            print(f"   âš ï¸  å®é™…å¹³å‡å€¼ä¸ç›®æ ‡æœ‰è½»å¾®åå·®")
        else:
            print(f"   âŒ å®é™…å¹³å‡å€¼ä¸ç›®æ ‡åå·®è¾ƒå¤§ï¼ˆå¯èƒ½ç”±äºå¤§é‡headsè§¦åŠä¸Šé™ï¼‰")

if __name__ == "__main__":
    test_dream_keep_ratios()

