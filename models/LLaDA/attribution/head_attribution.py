"""
Head Attribution using Integrated Gradients for Diffusion Language Models (LLaDA).

é’ˆå¯¹æ¯ä¸€å±‚çš„ attention heads è¿›è¡Œå½’å› ï¼Œbaseline æ˜¯è¯¥å±‚ att å®Œå…¨ mask æ‰ã€‚
"""
import torch
import torch.nn.functional as F
from typing import List, Dict, Optional, Tuple
import numpy as np


def ensure_finite_(x: torch.Tensor, check_neg_inf: bool = True, check_pos_inf: bool = False):
    """
    Modify ``x`` in place to replace ``float("-inf")`` with the minimum value of the dtype when ``check_neg_inf``
    is ``True`` and to replace ``float("inf")`` with the maximum value of the dtype when ``check_pos_inf`` is ``True``.
    """
    if check_neg_inf:
        x.masked_fill_(x == float("-inf"), torch.finfo(x.dtype).min)
    if check_pos_inf:
        x.masked_fill_(x == float("inf"), torch.finfo(x.dtype).max)


class IntegratedGradientsHeadAttribution:
    """
    å¯¹ Diffusion LM çš„æ¯ä¸€å±‚ attention heads ä½¿ç”¨ Integrated Gradients å½’å› ã€‚
    
    æ ¸å¿ƒæ€è·¯ï¼š
    1. é€å±‚å½’å› ï¼šå¯¹æ¯ä¸€å±‚å•ç‹¬è®¡ç®— head importance
    2. Baseline: è¯¥å±‚æ‰€æœ‰ heads çš„ att è¾“å‡ºéƒ½ä¸º 0
    3. Actual: è¯¥å±‚ heads çš„æ­£å¸¸ att è¾“å‡º
    4. åœ¨ baseline å’Œ actual ä¹‹é—´æ’å€¼å¹¶ç§¯åˆ†
    """
    
    def __init__(
        self,
        model,
        n_steps: int = 20,
    ):
        """
        Args:
            model: LLaDAModel instance
            n_steps: Integrated Gradients çš„ç§¯åˆ†æ­¥æ•°
        """
        self.model = model
        self.n_steps = n_steps
        self.device = next(model.parameters()).device
    
    def _forward_with_layer_head_cache(
        self,
        input_ids: torch.LongTensor,
        attention_mask: Optional[torch.Tensor],
        target_layer_idx: int,
        head_att_values: torch.Tensor,  # (B, nh, T, hs) - è¯¥å±‚å„ head çš„ att å€¼
    ) -> torch.Tensor:
        """
        Forward passï¼Œä½¿ç”¨ç»™å®šçš„ head attention outputs æ›¿æ¢ç›®æ ‡å±‚çš„æ­£å¸¸è®¡ç®—ã€‚
        
        Args:
            input_ids: Input token ids (B, L)
            attention_mask: Attention mask (B, L)
            target_layer_idx: è¦æ›¿æ¢ att çš„å±‚ç´¢å¼•
            head_att_values: è¦ä½¿ç”¨çš„ att å€¼ (B, nh, T, hs)
        
        Returns:
            logits: (B, L, vocab_size)
        """
        # ç¡®ä¿æ¨¡å‹å¤„äº eval æ¨¡å¼ï¼ˆç¦ç”¨ dropoutï¼‰
        was_training = self.model.training
        self.model.eval()
        
        batch_size, seq_len = input_ids.shape
        d_model = self.model.config.d_model
        n_heads = self.model.config.n_heads
        head_dim = d_model // n_heads
        
        # Get embeddings
        x = self.model.transformer.wte(input_ids)
        
        # Input embedding processing
        if self.model.config.input_emb_norm:
            x = x * (d_model ** 0.5)
        
        if not (self.model.config.alibi or self.model.config.rope):
            pos = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0)
            pos_emb = self.model.transformer.wpe(pos)
            x = pos_emb + x
        
        x = self.model.transformer.emb_drop(x)
        
        # Process attention mask and bias (following official implementation EXACTLY - line 1251-1292)
        # Transform the attention mask into what the blocks expect
        if attention_mask is not None and 0.0 in attention_mask:
            # shape: (batch_size, 1, 1, seq_len)
            attention_mask = attention_mask.to(dtype=torch.float).view(batch_size, -1)[:, None, None, :]
            attention_mask = (1.0 - attention_mask) * torch.finfo(attention_mask.dtype).min
        else:
            attention_mask = None
        
        # Merge attention mask with attention bias
        # ğŸ”‘ KEY: Only prepare attention_bias if needed (matching official logic in modeling_llada.py:1259-1273)
        attention_bias = None
        if (
            attention_mask is not None
            or self.model.config.alibi
            # NOTE: We don't have past_key_values in our use case
        ):
            if attention_bias is None and self.model.config.alibi:
                # Not implemented: would need get_causal_attention_bias + get_alibi_attention_bias
                raise NotImplementedError("ALiBi is not supported in this implementation")
            elif attention_bias is None:
                attention_bias = self.model.get_bidirectional_attention_bias(seq_len, x.device)
            
            # Transform to the right shape
            mask_len = seq_len
            if attention_mask is not None:
                mask_len = attention_mask.shape[-1]
            attention_bias = attention_bias[:, :, :mask_len, :mask_len].to(dtype=torch.float)
            
            # Add in the masking bias
            if attention_mask is not None:
                attention_bias = attention_bias + attention_mask
                # Handle -inf values
                ensure_finite_(attention_bias, check_neg_inf=True, check_pos_inf=False)
        
        # Get blocks
        blocks = self.model.transformer.blocks if self.model.config.block_group_size == 1 else [
            block for group in self.model.transformer.block_groups for block in group
        ]
        
        # Forward through blocks
        for curr_layer_idx, block in enumerate(blocks):
            # Attention
            x_normed = block.attn_norm(x)
            
            if curr_layer_idx == target_layer_idx:
                # ä½¿ç”¨ç»™å®šçš„ att å€¼ï¼Œä¸è®¡ç®— attention
                # head_att_values å·²ç»æ˜¯ (B, nh, T, hs) å½¢çŠ¶
                # éœ€è¦ merge heads: (B, nh, T, hs) -> (B, T, d_model)
                att = head_att_values.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)
            else:
                # æ­£å¸¸è®¡ç®— attention
                # Get Q, K, V
                if hasattr(block, 'att_proj'):
                    qkv = block.att_proj(x_normed)
                    head_dim_size = d_model // n_heads
                    fused_dims = (d_model, block.config.effective_n_kv_heads * head_dim_size, 
                                 block.config.effective_n_kv_heads * head_dim_size)
                    q, k, v = qkv.split(fused_dims, dim=-1)
                else:
                    q = block.q_proj(x_normed)
                    k = block.k_proj(x_normed)
                    v = block.v_proj(x_normed)
                
                # Reshape
                q = q.view(batch_size, seq_len, n_heads, head_dim).transpose(1, 2)
                k = k.view(batch_size, seq_len, block.config.effective_n_kv_heads, head_dim).transpose(1, 2)
                v = v.view(batch_size, seq_len, block.config.effective_n_kv_heads, head_dim).transpose(1, 2)
                
                # RoPE
                if block.config.rope:
                    q, k = block.rotary_emb(q, k)
                
                # GQA
                if k.size(1) != q.size(1):
                    k = k.repeat_interleave(n_heads // k.size(1), dim=1, output_size=n_heads)
                    v = v.repeat_interleave(n_heads // v.size(1), dim=1, output_size=n_heads)
                
                # Attention
                # Apply attention bias (slice for current sequence length)
                attn_bias_slice = attention_bias[:, :, :seq_len, :seq_len] if attention_bias is not None else None
                att = F.scaled_dot_product_attention(
                    q, k, v,
                    attn_mask=attn_bias_slice,
                    dropout_p=0.0,
                    is_causal=False
                )
                
                # Merge heads: (B, nh, T, hs) -> (B, T, d_model)
                att = att.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)
            
            # Apply output projection
            att = block.attn_out(att)
            x = x + block.dropout(att)
            
            # Feed-forward
            og_x = x
            x = block.ff_norm(x)
            
            if hasattr(block, 'att_proj'):
                x = block.ff_proj(x)
                x = block.act(x)
                x = block.ff_out(x)
            else:
                x_gate = block.ff_proj(x)
                x_up = block.up_proj(x)
                x = block.act(x_gate) * x_up
                x = block.ff_out(x)
            
            x = block.dropout(x)
            x = og_x + x
        
        # Final layer norm
        x = self.model.transformer.ln_f(x)
        
        # Get logits
        if self.model.config.weight_tying:
            logits = F.linear(x, self.model.transformer.wte.weight, None)
        else:
            logits = self.model.transformer.ff_out(x)
        
        # æ¢å¤åŸå§‹è®­ç»ƒçŠ¶æ€
        if was_training:
            self.model.train()
        
        return logits
    
    def _compute_layer_head_att(
        self,
        input_ids: torch.LongTensor,
        attention_mask: Optional[torch.Tensor],
        target_layer_idx: int,
    ) -> torch.Tensor:
        """
        è®¡ç®—æŸä¸€å±‚çš„æ­£å¸¸ attention outputsï¼ˆåœ¨ scaled_dot_product_attention ä¹‹åï¼‰ã€‚
        
        ç­–ç•¥ï¼šä½¿ç”¨ hook æ•è·å®˜æ–¹ block çš„ attention è¾“å‡ºï¼Œç¡®ä¿ä¸å®˜æ–¹å®ç°å®Œå…¨ä¸€è‡´ã€‚
        
        Returns:
            att: shape (B, nh, T, hs)
        """
        was_training = self.model.training
        self.model.eval()
        
        batch_size, seq_len = input_ids.shape
        d_model = self.model.config.d_model
        
        # Get embeddings and process
        x = self.model.transformer.wte(input_ids)
        
        if self.model.config.input_emb_norm:
            x = x * (d_model ** 0.5)
        
        if not (self.model.config.alibi or self.model.config.rope):
            pos = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0)
            pos_emb = self.model.transformer.wpe(pos)
            x = pos_emb + x
        
        x = self.model.transformer.emb_drop(x)
        
        # Process attention mask and bias (matching official logic)
        if attention_mask is not None and 0.0 in attention_mask:
            attention_mask = attention_mask.to(dtype=torch.float).view(batch_size, -1)[:, None, None, :]
            attention_mask = (1.0 - attention_mask) * torch.finfo(attention_mask.dtype).min
        else:
            attention_mask = None
        
        attention_bias = None
        if (
            attention_mask is not None
            or self.model.config.alibi
        ):
            if attention_bias is None and self.model.config.alibi:
                raise NotImplementedError("ALiBi is not supported in this implementation")
            elif attention_bias is None:
                attention_bias = self.model.get_bidirectional_attention_bias(seq_len, x.device)
            
            mask_len = seq_len
            if attention_mask is not None:
                mask_len = attention_mask.shape[-1]
            attention_bias = attention_bias[:, :, :mask_len, :mask_len].to(dtype=torch.float)
            
            if attention_mask is not None:
                attention_bias = attention_bias + attention_mask
                ensure_finite_(attention_bias, check_neg_inf=True, check_pos_inf=False)
        
        blocks = self.model.transformer.blocks if self.model.config.block_group_size == 1 else [
            block for group in self.model.transformer.block_groups for block in group
        ]
        
        # ä½¿ç”¨ hook æ•è·ç›®æ ‡å±‚çš„ attention è¾“å‡º
        captured_att = [None]
        
        # æ›¿æ¢ F.scaled_dot_product_attention æ¥æ•è·è¾“å‡º
        original_sdpa = F.scaled_dot_product_attention
        
        def sdpa_with_capture(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False):
            result = original_sdpa(q, k, v, attn_mask=attn_mask, dropout_p=dropout_p, is_causal=is_causal)
            captured_att[0] = result.clone()
            return result
        
        # Forward through blocks
        for curr_layer_idx, block in enumerate(blocks):
            if curr_layer_idx == target_layer_idx:
                # åœ¨ç›®æ ‡å±‚ï¼Œæ›¿æ¢ SDPA æ¥æ•è·è¾“å‡º
                F.scaled_dot_product_attention = sdpa_with_capture
                try:
                    x, _ = block(x, attention_bias=attention_bias)
                finally:
                    F.scaled_dot_product_attention = original_sdpa
                break
            else:
                # æ­£å¸¸ forward
                x, _ = block(x, attention_bias=attention_bias)
        
        if was_training:
            self.model.train()
        
        if captured_att[0] is None:
            raise ValueError(f"Failed to capture attention at layer {target_layer_idx}")
        
        return captured_att[0]  # (B, nh, T, hs)
    
    def compute_head_attribution_for_layer(
        self,
        input_ids: torch.LongTensor,
        update_positions: List[int],
        target_layer_idx: int,
        attention_mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        å¯¹æŸä¸€å±‚çš„æ‰€æœ‰ heads ä½¿ç”¨ Integrated Gradients å½’å› ã€‚
        
        Args:
            input_ids: (1, L)
            update_positions: è¢«æ›´æ–°çš„ä½ç½®åˆ—è¡¨ï¼Œå¦‚ [10, 12, 13]
            target_layer_idx: è¦å½’å› çš„å±‚
            attention_mask: (1, L)
        
        Returns:
            attributions: shape (n_heads,) - è¯¥å±‚æ¯ä¸ª head çš„é‡è¦æ€§åˆ†æ•°
        """
        self.model.eval()
        n_heads = self.model.config.n_heads
        batch_size = input_ids.shape[0]
        n_positions = len(update_positions)
        
        # Step 1: è®¡ç®—è¯¥å±‚æ­£å¸¸çš„ att è¾“å‡º (actual)
        with torch.no_grad():
            att_actual = self._compute_layer_head_att(
                input_ids, attention_mask, target_layer_idx
            )  # (B, nh, T, hs)
        
        # Step 2: Baseline - æ‰€æœ‰ heads çš„ att éƒ½ä¸º 0
        att_baseline = torch.zeros_like(att_actual)
        
        # Step 3: Integrated Gradients - æ²¿æ’å€¼è·¯å¾„ç§¯åˆ†
        # å­˜å‚¨æ¯ä¸ª head åœ¨æ¯ä¸ªæ’å€¼ç‚¹çš„æ¢¯åº¦ï¼ˆä¿æŒå®Œæ•´å½¢çŠ¶ç”¨äºé€å…ƒç´ ä¹˜æ³•ï¼‰
        accumulated_grads = torch.zeros_like(att_actual, device=self.device)  # (B, nh, T, hs)
        
        for step in range(self.n_steps + 1):
            alpha = step / self.n_steps
            
            # æ’å€¼ï¼šæ‰€æœ‰ heads ä¸€èµ·ä» baseline åˆ° actual
            # att_Î± = Î± * att_actual (å› ä¸º baseline = 0)
            att_interpolated = alpha * att_actual  # (B, nh, T, hs)
            att_interpolated = att_interpolated.detach().requires_grad_(True)
            
            # Forward pass å¾—åˆ° logits
            logits = self._forward_with_layer_head_cache(
                input_ids=input_ids,
                attention_mask=attention_mask,
                target_layer_idx=target_layer_idx,
                head_att_values=att_interpolated
            )
            
            # æå–æ›´æ–°ä½ç½®çš„ logitsï¼Œå¹¶å¯¹ç›®æ ‡ä½ç½®æ±‚å’Œä½œä¸ºæ ‡é‡è¾“å‡º
            logits_at_pos = logits[0, update_positions, :]  # (n_pos, vocab_size)
            
            # å¯¹ logits æ±‚å’Œå¾—åˆ°æ ‡é‡ï¼ˆç”¨äºè®¡ç®—æ¢¯åº¦ï¼‰
            # è¿™é‡Œæˆ‘ä»¬å¯¹æ¯ä¸ªä½ç½®çš„ logits çš„ L1 èŒƒæ•°æ±‚å’Œ
            output_scalar = logits_at_pos.abs().sum()
            
            # è®¡ç®—æ¢¯åº¦ï¼šâˆ‚output/âˆ‚att_interpolated
            if att_interpolated.requires_grad:
                output_scalar.backward()
                
                # att_interpolated.grad shape: (B, nh, T, hs)
                grads = att_interpolated.grad
                
                # æ¢¯å½¢æ³•åˆ™æƒé‡
                if step == 0 or step == self.n_steps:
                    weight = 0.5
                else:
                    weight = 1.0
                
                # ç´¯ç§¯æ¢¯åº¦ï¼ˆä¿æŒå®Œæ•´å½¢çŠ¶ï¼‰
                accumulated_grads += grads * weight
        
        # Step 4: å½’å›  = (att_actual - att_baseline) âŠ™ å¹³å‡æ¢¯åº¦
        # å› ä¸º baseline = 0ï¼Œæ‰€ä»¥å½’å›  = att_actual âŠ™ å¹³å‡æ¢¯åº¦
        # âŠ™ è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•ï¼ˆelement-wise multiplicationï¼‰
        
        # è®¡ç®—å¹³å‡æ¢¯åº¦ï¼ˆä¿æŒå®Œæ•´å½¢çŠ¶ï¼‰
        avg_grads = accumulated_grads / self.n_steps  # (B, nh, T, hs)
        
        # é€å…ƒç´ ä¹˜æ³•ï¼Œç„¶åå¯¹æ¯ä¸ª head æ±‚å’Œ
        # è¿™æ˜¯æ ‡å‡†çš„ Integrated Gradients å…¬å¼
        elementwise_attribution = att_actual * avg_grads  # (B, nh, T, hs)
        
        # å¯¹æ¯ä¸ª head åœ¨ batch, seq_len, head_dim ç»´åº¦ä¸Šæ±‚å’Œ
        attributions = elementwise_attribution.sum(dim=[0, 2, 3])  # (nh,)
        
        return attributions
    
    def get_head_ranking(
        self,
        importance_scores: Dict[int, torch.Tensor]
    ) -> Dict[int, List[int]]:
        """
        è·å–æ¯å±‚ heads çš„é‡è¦æ€§æ’åºã€‚
        
        Returns:
            {layer_idx: [head_indices sorted by importance (descending)]}
        """
        rankings = {}
        for layer_idx, scores in importance_scores.items():
            sorted_indices = torch.argsort(scores, descending=True).cpu().tolist()
            rankings[layer_idx] = sorted_indices
        
        return rankings
