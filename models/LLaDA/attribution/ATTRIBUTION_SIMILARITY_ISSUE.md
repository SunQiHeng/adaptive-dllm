# Head Attributionå½’å› ç»“æœç›¸ä¼¼æ€§é—®é¢˜åˆ†ææŠ¥å‘Š

## ğŸ“‹ é—®é¢˜æè¿°

åœ¨ä½¿ç”¨è”åˆIG (Joint Integrated Gradients) æ–¹æ³•è®¡ç®—head importanceæ—¶ï¼Œå‘ç°ï¼š
- **1æ¡æ•°æ®**å’Œ**50æ¡æ•°æ®**çš„å½’å› ç»“æœé«˜åº¦ç›¸ä¼¼
- Pearsonç›¸å…³ç³»æ•°è¾¾åˆ° **0.917**
- ä½™å¼¦ç›¸ä¼¼åº¦è¾¾åˆ° **0.928**
- ç»Ÿè®¡åˆ†å¸ƒå‡ ä¹ä¸€è‡´ï¼ˆmean, std, min, maxéƒ½éå¸¸æ¥è¿‘ï¼‰

è¿™ç§ç°è±¡ä¸åˆç†â€”â€”ä¸åŒæ•°æ®åº”è¯¥æ¿€æ´»ä¸åŒçš„attention headsï¼Œå½’å› ç»“æœåº”è¯¥æœ‰æ›´å¤§å·®å¼‚ã€‚

## ğŸ” æ ¹æœ¬åŸå› åˆ†æ

### 1. **è”åˆIGæ–¹æ³•çš„è®¾è®¡ç¼ºé™·**

å½“å‰å®ç°ï¼ˆ`compute_loss_attribution_all_heads.py`ï¼‰ä¸­ï¼Œæ‰€æœ‰1024ä¸ªheads **åŒæ—¶ä»baselineæ’å€¼åˆ°1.0**ï¼š

```python
# ç¬¬293-299è¡Œ
alpha_flat = torch.full(
    (total_heads,),  # æ‰€æœ‰heads
    fill_value=alpha_val,  # ä½¿ç”¨ç›¸åŒçš„æ’å€¼æ­¥é•¿ t
    device=device,
    dtype=torch.float32,
    requires_grad=True,
)
```

**é—®é¢˜æ‰€åœ¨ï¼š**
- IGè·¯å¾„ä¸Šï¼Œæ‰€æœ‰headsä»¥**ç›¸åŒé€Ÿåº¦**ä»baselineæ¿€æ´»
- æ¢¯åº¦ `âˆ‚L/âˆ‚Î±_i` åæ˜ çš„æ˜¯"å½“æ‰€æœ‰headsä¸€èµ·å˜åŒ–æ—¶ï¼Œhead içš„è¾¹é™…è´¡çŒ®"
- **æ— æ³•æ•æ‰å•ä¸ªheadçš„ç‹¬ç«‹ä½œç”¨**ï¼Œåªèƒ½æ•æ‰æ•´ä½“æ¿€æ´»æ¨¡å¼
- æ¨¡å‹çš„æ•´ä½“è¡Œä¸ºæ¨¡å¼ç›¸å¯¹å›ºå®šï¼Œå¯¼è‡´ä¸åŒæ ·æœ¬ç»“æœç›¸ä¼¼

### 2. **Per-Sample IG å‘é‡é«˜åº¦ç›¸å…³**

åˆ†æ10ä¸ªä¸åŒæ ·æœ¬çš„IGå‘é‡å‘ç°ï¼š
- æ ·æœ¬é—´å¹³å‡Pearsonç›¸å…³ç³»æ•°ï¼š**0.78**
- ç›¸å…³ç³»æ•°èŒƒå›´ï¼š[0.61, 0.90]
- Headè·¨æ ·æœ¬æ–¹å·®ï¼šmean = **0.000141** (æå°)
- åªæœ‰15/1024ä¸ªheadsçš„è·¨æ ·æœ¬æ–¹å·® > 0.001

**å«ä¹‰ï¼š**
- ä¸åŒæ ·æœ¬æ¿€æ´»çš„headæ¨¡å¼æƒŠäººç›¸ä¼¼
- æ¨¡å‹ä¾èµ–çš„attention headså¯¹ä¸åŒä»»åŠ¡/æ ·æœ¬å…·æœ‰**é€šç”¨æ€§**
- è”åˆIGæ–¹æ³•ä¸»è¦æ•æ‰è¿™ç§"é€šç”¨é‡è¦æ€§"ï¼Œè€Œéæ ·æœ¬ç‰¹å¼‚æ€§

### 3. **ä¸é€å±‚IGæ–¹æ³•çš„å¯¹æ¯”**

| ç»´åº¦ | é€å±‚IG (Per-layer) | è”åˆIG (Joint) |
|-----|-------------------|---------------|
| **æ’å€¼æ–¹å¼** | å•å±‚æ’å€¼ï¼Œå…¶ä»–å±‚å›ºå®šåœ¨Î±=1 | æ‰€æœ‰å±‚åŒæ—¶æ’å€¼ |
| **æ¢¯åº¦å«ä¹‰** | åœ¨å…¶ä»–å±‚æ­£å¸¸å·¥ä½œæ—¶ï¼Œå½“å‰å±‚headçš„ç‹¬ç«‹ä½œç”¨ | æ‰€æœ‰headsä¸€èµ·æ¿€æ´»æ—¶çš„è¾¹é™…è´¡çŒ® |
| **äº¤äº’æ•ˆåº”** | æ— æ³•æ•æ‰è·¨å±‚äº¤äº’ | ç†è®ºä¸Šèƒ½æ•æ‰ï¼Œä½†å®é™…è¢«"æ•´ä½“æ¨¡å¼"ä¸»å¯¼ |
| **æ ·æœ¬ç‰¹å¼‚æ€§** | è¾ƒå¥½ï¼ˆéœ€éªŒè¯ï¼‰ | **å·®**ï¼ˆæœ¬é—®é¢˜æ‰€åœ¨ï¼‰ |

## ğŸ“Š å®éªŒæ•°æ®

### å¯¹æ¯”å®éªŒç»“æœ

| æŒ‡æ ‡ | 50æ¡æ•°æ® | 1æ¡æ•°æ® | å·®å¼‚ |
|-----|---------|--------|-----|
| Mean | -0.009304 | -0.009097 | 0.000207 |
| Std | 0.022929 | 0.023633 | 0.000704 |
| Min | -0.259404 | -0.313883 | 0.054479 |
| Max | 0.138141 | 0.138763 | 0.000622 |
| **Pearsonç›¸å…³** | - | - | **0.917** |
| **ä½™å¼¦ç›¸ä¼¼åº¦** | - | - | **0.928** |

### Per-Sample IGç›¸å…³æ€§ï¼ˆ10ä¸ªæ ·æœ¬ï¼‰

```
å¹³å‡ç›¸å…³ç³»æ•°: 0.78
ç›¸å…³ç³»æ•°çŸ©é˜µï¼ˆéƒ¨åˆ†ï¼‰:
     s0    s1    s2    s3    s4
s0  1.00  0.80  0.83  0.81  0.78
s1  0.80  1.00  0.76  0.80  0.83
s2  0.83  0.76  1.00  0.82  0.77
s3  0.81  0.80  0.82  1.00  0.76
s4  0.78  0.83  0.77  0.76  1.00
```

### Headè·¨æ ·æœ¬æ–¹å·®åˆ†æ

- å¹³å‡æ–¹å·®ï¼š0.000141 (æå°)
- ä¸­ä½æ•°æ–¹å·®ï¼š0.000044
- æ–¹å·®>0.001çš„headsï¼š15 / 1024 (ä»…1.5%)
- æ–¹å·®>0.005çš„headsï¼š3 / 1024 (0.3%)

## ğŸ’¡ è§£å†³æ–¹æ¡ˆ

### âœ… æ–¹æ¡ˆ1ï¼šåˆ‡æ¢åˆ°é€å±‚IGæ–¹æ³•ã€æ¨èã€‘

**å®ç°ï¼š** ä½¿ç”¨ `compute_loss_attribution.py` è€Œé `compute_loss_attribution_all_heads.py`

**ä¼˜ç‚¹ï¼š**
- æ¯å±‚å•ç‹¬è®¡ç®—ï¼Œå…¶ä»–å±‚ä¿æŒå®Œå…¨æ¿€æ´»
- æ›´èƒ½æ•æ‰æ¯å±‚headsçš„**ç‹¬ç«‹ä½œç”¨**
- è®¡ç®—æˆæœ¬ç•¥é«˜ä½†å¯æ¥å—ï¼ˆ32å±‚ Ã— æ•°æ®é›†å¤§å°ï¼‰

**ç¼ºç‚¹ï¼š**
- æ— æ³•æ•æ‰è·¨å±‚çš„äºŒé˜¶äº¤äº’æ•ˆåº”ï¼ˆä½†å®è·µä¸­å½±å“è¾ƒå°ï¼‰

**ä½¿ç”¨æ–¹æ³•ï¼š**
```bash
# ä¿®æ”¹ run.shï¼Œä½¿ç”¨é€å±‚è„šæœ¬
python .../compute_loss_attribution.py \
  --model_path /data/qh_models/LLaDA-1.5 \
  --dataset nemotron \
  --max_samples 50 \
  --layer_start 0 \
  --layer_end 31 \
  ...
```

### æ–¹æ¡ˆ2ï¼šä¿®æ”¹è”åˆIG - åˆ†å±‚æ’å€¼

**æ€è·¯ï¼š** ä¸åŒå±‚ä½¿ç”¨ä¸åŒçš„æ’å€¼é€Ÿåº¦æˆ–éšæœºåŒ–è·¯å¾„

```python
# ç¤ºä¾‹ä¿®æ”¹ï¼šç¬¬lå±‚çš„æ’å€¼æ­¥é•¿ä¸º t^(1+Î»_l)
for k in range(1, ig_steps + 1):
    t = float(k) / float(ig_steps)
    alpha_vals = []
    for layer_idx in layer_indices:
        # æ¯å±‚ä½¿ç”¨ä¸åŒçš„æ’å€¼æ›²çº¿
        lambda_l = layer_idx / len(layer_indices)  # 0åˆ°1
        alpha_val = baseline + t**(1 + lambda_l) * (1 - baseline)
        alpha_vals.extend([alpha_val] * n_heads_per_layer[layer_idx])
    alpha_flat = torch.tensor(alpha_vals, requires_grad=True)
    ...
```

**ä¼˜ç‚¹ï¼š**
- ä¿æŒè”åˆè®¡ç®—çš„æ•ˆç‡
- å¢åŠ æ ·æœ¬é—´çš„åŒºåˆ†åº¦

**ç¼ºç‚¹ï¼š**
- å¼•å…¥æ–°è¶…å‚æ•°ï¼ˆå¦‚ä½•é€‰æ‹©Î»_lï¼Ÿï¼‰
- ç†è®ºåŸºç¡€ä¸å¦‚æ ‡å‡†IGä¸¥æ ¼

### æ–¹æ¡ˆ3ï¼šä½¿ç”¨Shapleyå€¼æ–¹æ³•

**æ€è·¯ï¼š** è®¡ç®—æ¯ä¸ªheadåœ¨æ‰€æœ‰å¯èƒ½å­é›†ä¸­çš„å¹³å‡è¾¹é™…è´¡çŒ®

**ä¼˜ç‚¹ï¼š**
- ç†è®ºä¸Šæœ€ä¸¥æ ¼çš„å½’å› æ–¹æ³•
- èƒ½å®Œå…¨æ•æ‰äº¤äº’æ•ˆåº”

**ç¼ºç‚¹ï¼š**
- è®¡ç®—å¤æ‚åº¦ O(2^1024) - **å®é™…ä¸å¯è¡Œ**
- éœ€è¦é‡‡æ ·è¿‘ä¼¼ï¼ˆå¦‚KernelSHAPï¼‰ï¼Œå¼•å…¥æ–°çš„è¯¯å·®

### æ–¹æ¡ˆ4ï¼šå¢åŠ ä»»åŠ¡å¤šæ ·æ€§

**æ€è·¯ï¼š** ä¸ä»…ä½¿ç”¨masked LMï¼Œè¿˜åŒ…æ‹¬ï¼š
- ç”Ÿæˆä»»åŠ¡ï¼ˆcontinuationï¼‰
- åˆ†ç±»ä»»åŠ¡ï¼ˆsentiment, NLIï¼‰
- é—®ç­”ä»»åŠ¡ï¼ˆextractive QAï¼‰

**ä¼˜ç‚¹ï¼š**
- ä¸åŒä»»åŠ¡å¯èƒ½æ¿€æ´»ä¸åŒçš„heads
- æ›´å…¨é¢çš„é‡è¦æ€§è¯„ä¼°

**ç¼ºç‚¹ï¼š**
- éœ€è¦å¤§å¹…ä¿®æ”¹è¯„ä¼°æµç¨‹
- ä¸åŒä»»åŠ¡çš„å½’å› ç»“æœå¦‚ä½•èåˆï¼Ÿ

## ğŸ¯ æ¨èè¡ŒåŠ¨æ–¹æ¡ˆ

### çŸ­æœŸï¼ˆç«‹å³å®æ–½ï¼‰
1. **åˆ‡æ¢åˆ°é€å±‚IGæ–¹æ³•**
   - ä¿®æ”¹è¿è¡Œè„šæœ¬ï¼Œä½¿ç”¨ `compute_loss_attribution.py`
   - é‡æ–°è·‘1æ¡å’Œ50æ¡æ•°æ®çš„å¯¹æ¯”å®éªŒ
   - éªŒè¯æ˜¯å¦èƒ½å¾—åˆ°æ›´æœ‰åŒºåˆ†åº¦çš„ç»“æœ

2. **ä¿å­˜å¹¶åˆ†æper-sample IG**
   ```bash
   --debug_save_per_sample 50  # ä¿å­˜å‰50ä¸ªæ ·æœ¬çš„IGå‘é‡
   ```
   - åˆ†ææ ·æœ¬é—´çš„ç›¸å…³æ€§æ˜¯å¦é™ä½
   - è¯†åˆ«å“ªäº›headsçš„é‡è¦æ€§åœ¨ä¸åŒæ ·æœ¬é—´å˜åŒ–æœ€å¤§

### ä¸­æœŸï¼ˆåç»­ç ”ç©¶ï¼‰
3. **å¯è§†åŒ–åˆ†æ**
   - å¯¹æ¯”é€å±‚IG vs è”åˆIGçš„ç»“æœå·®å¼‚
   - ç»˜åˆ¶ä¸åŒæ•°æ®ç±»å‹ï¼ˆcode/math/science/chat/safetyï¼‰çš„headä½¿ç”¨æ¨¡å¼

4. **æ¶ˆèå®éªŒ**
   - æµ‹è¯•pruningæ‰"ä¸é‡è¦"çš„headsåï¼Œæ¨¡å‹æ€§èƒ½æ˜¯å¦ä¸‹é™
   - éªŒè¯å½’å› æ–¹æ³•çš„æœ‰æ•ˆæ€§

### é•¿æœŸï¼ˆæ–¹æ³•æ”¹è¿›ï¼‰
5. **æ¢ç´¢æ–°çš„å½’å› æ–¹æ³•**
   - åˆ†å±‚æ’å€¼IG
   - Attention rollout
   - Gradient Ã— Activation

## ğŸ“ ç›¸å…³æ–‡ä»¶

- é—®é¢˜è„šæœ¬ï¼š`models/LLaDA/attribution/loss_attribution/compute_loss_attribution_all_heads.py`
- æ¨èæ›¿ä»£ï¼š`models/LLaDA/attribution/loss_attribution/compute_loss_attribution.py`
- è¯Šæ–­è„šæœ¬ï¼š`/tmp/diagnose_attribution_problem.py`
- å¯è§†åŒ–ï¼š`/tmp/attribution_similarity_analysis.png`

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. Sundararajan et al. (2017). "Axiomatic Attribution for Deep Networks" (Integrated GradientsåŸè®ºæ–‡)
2. Michel et al. (2019). "Are Sixteen Heads Really Better than One?" (BERT head pruning)
3. Voita et al. (2019). "Analyzing Multi-Head Self-Attention" (é€å±‚åˆ†ææ–¹æ³•)
4. Lundberg & Lee (2017). "A Unified Approach to Interpreting Model Predictions" (SHAP)

---

**ç”Ÿæˆæ—¶é—´ï¼š** 2026-01-09  
**åˆ†æè€…ï¼š** AI Assistant  
**å®éªŒç¯å¢ƒï¼š** LLaDA-1.5, Nemotronæ•°æ®é›†

