\section{Introduction}


Large language models (LLMs) have revolutionized natural language processing, yet their dominant autoregressive generation paradigm—producing tokens sequentially—imposes inherent latency and computational bottlenecks during inference. This limitation hinders real-time deployment in resource-constrained environments and escalates energy costs at scale. Diffusion language models (DLMs), also referred to as diffusion-based large language models (DLLMs), have recently emerged as a compelling alternative by replacing next-token decoding with an iterative denoising process over a partially masked sequence. By updating multiple positions in parallel across denoising steps, DLLMs offer a different accuracy--latency trade-off via the number of refinement steps and the amount of computation per step. However, as this nascent generation paradigm diverges fundamentally from standard autoregressive decoding, its operational mechanisms—particularly the functional roles of internal components—remain underexplored. Understanding such mechanisms is critical not only for improving DLLM efficiency but also for advancing their interpretability and reliability.

Within DLLMs, multi-head attention mechanisms serve as the fundamental building blocks that govern information integration across token positions. However, unlike autoregressive LLMs where head functionality has been extensively studied (e.g., syntactic parsing, positional bias, or induction behavior), the role of attention heads in DLLMs remains critically underexplored. This gap is non-trivial: DLLMs’ parallel denoising fundamentally reshapes attention dynamics, causing head contributions to vary across denoising steps and masking schedules. Ignoring this heterogeneity impedes both efficiency and transparency. On one hand, redundant or conflicting heads inflate computational costs during iterative refinement—a key bottleneck for real-world deployment. On the other, opaque head behaviors limit interpretability and hinder trust in safety-critical applications.

Existing attribution methods face fundamental challenges in DLLMs because their assumptions are closely tied to next-token prediction. In autoregressive decoding, a natural target exists at each step (the next token), and gradients can often be interpreted locally with respect to that single prediction. In contrast, DLLMs update \emph{multiple} masked positions in parallel at each denoising step, and the learning signal is typically a masked reconstruction objective. This creates three practical obstacles for head attribution. First, the \emph{target is non-local}: a single forward pass jointly affects a set of positions, making it difficult to disentangle which heads matter for which part of the loss. Second, the denoising trajectory exhibits strong \emph{path dependence}: intermediate states depend on earlier updates, so naïvely attributing only to the final output can conflate qualitatively different head roles across the refinement process. Third, the process is \emph{discrete} (masking/unmasking, stepwise updates), making raw gradients sensitive to the particular trajectory and masking rate. These challenges motivate an attribution framework that (i) aligns with the masked reconstruction loss used by DLLMs and (ii) produces stable head-importance estimates under trajectory variability.

To address these issues, we propose a loss-based head attribution framework, \emph{Head Integrated Gradients} (HeadIG), tailored to diffusion-style masking and denoising. HeadIG introduces a differentiable scaling gate for each attention head and measures its contribution by integrating the gradient of the masked reconstruction loss as the gates transition from a baseline configuration to the full model. We consider practical baselines such as \emph{zero} (all gated off) or a small \emph{scalar} baseline, and aggregate attribution over multiple masking probabilities to reflect different denoising difficulty regimes. A key design choice is the \emph{integration path} in the high-dimensional space of head gates. We provide two complementary paths: a deterministic \emph{Diagonal Path} that turns on all heads synchronously, and a more robust \emph{Stochastic Threshold Path} (STP) that samples random activation thresholds for heads along the path, reducing sensitivity to any single ordering and better capturing the trajectory variability inherent to DLLMs.

We validate the resulting head-importance scores in two causal ways that connect attribution to both efficiency and behavior. First, we use the scores to drive \emph{adaptive sparse attention}, reallocating sparse budgets toward important heads during inference. Second, we conduct \emph{head masking/pruning} studies, comparing pruning the most important versus least important heads (with random pruning as a control), to test whether the attribution ranking predicts functional necessity. Overall, our contributions are:
\begin{itemize}
    \item a DLLM-aligned, loss-based head attribution framework based on integrated gradients (HeadIG);
    \item a robust integration path for head gating, the Stochastic Threshold Path (STP), alongside a deterministic diagonal path;
    \item two complementary validations of head importance via adaptive sparse attention and importance-based head masking/pruning;
    \item empirical evidence across two representative DLLM families and multiple tasks.
\end{itemize}